{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"User Documentation for NCAR High Performance Computing \u00b6 This is the home of user documentation for the NCAR high-performance computing (HPC) and storage resources that CISL manages. The knowledge base includes searchable information specific to HPC resources, storage systems, authentication procedures and others, as well as additional how-to articles and troubleshooting articles. \u00b6 Selected Links \u00b6 Getting Started Using Derecho Using Casper Using JupyterHub Getting Help Don't find what you need? Log in here to submit a help request: NCAR Research Computing . You need a CIT password to submit a request. Call 303-497-2400 if you don't have one. Tip The NCAR HPC Users Group (NHUG) is a reouce group for all users of NCAR HPC resources. All users are welcome to join the NHUG Slack workspace . CISL welcomes your contributions This project is hosted on GitHub and your contributions are welcome!","title":"Home"},{"location":"#user-documentation-for-ncar-high-performance-computing","text":"This is the home of user documentation for the NCAR high-performance computing (HPC) and storage resources that CISL manages. The knowledge base includes searchable information specific to HPC resources, storage systems, authentication procedures and others, as well as additional how-to articles and troubleshooting articles.","title":"User Documentation for NCAR High Performance Computing"},{"location":"#_1","text":"","title":""},{"location":"#selected-links","text":"Getting Started Using Derecho Using Casper Using JupyterHub Getting Help Don't find what you need? Log in here to submit a help request: NCAR Research Computing . You need a CIT password to submit a request. Call 303-497-2400 if you don't have one. Tip The NCAR HPC Users Group (NHUG) is a reouce group for all users of NCAR HPC resources. All users are welcome to join the NHUG Slack workspace . CISL welcomes your contributions This project is hosted on GitHub and your contributions are welcome!","title":"Selected Links"},{"location":"contributing/","text":"Contribution Guide \u00b6 Welcome to the NCAR HPC Resources GitHub repository! This guide provides an overview of how to contribute to this documentations and the standards to follow when adding content to this repository. Our goal is to create a comprehensive and user-friendly documentation resource for NCAR's HPC resources. Repository Overview \u00b6 This repository contains technical documentation for NCAR HPC resources. The documentation is written in Markdown, which is then converted to HTML/CSS/JS using the mkdocs static site generator. We have customized the mkdocs-material theme to align with NCAR branding and colors. Note Here is the reference to the mkdocs-material documentation features . Making Contributions \u00b6 For modifications, such as a comprehensive revision of a section in the documentation, we recommend fork this repository, and clone the repository to your local machine and working from there. Steps to Contribute \u00b6 Fork the repository: Go to the repository page and click the \"Fork\" button to create a copy of the repository under your GitHub account. Clone the forked repository to your local machine: This can be done by running the following command in your terminal: git clone https://github.com/<YOUR_USERNAME>/hpc-docs-demo.git Replace <YOUR_USERNAME> with your GitHub username. Create a new branch: It's a good practice to create a new branch before you start making changes. This can be done by running: git checkout -b <BRANCH_NAME> Replace <BRANCH_NAME> with a name that gives a hint about the changes you're about to make. Make your changes: With your new branch checked out, you can start making changes to the documentation. Remember to save your work regularly. Tip You can live preview your changes locally by running mkdocs serve in your terminal. For more on this, see Building the Documentation Locally section. Commit your changes: Once you have made and tested your changes, stage the files you have modified using git add <file> or git add . to stage all changes. Then, commit your changes with a descriptive message using git commit -m \"<YOUR_COMMIT_MESSAGE>\" . Push your changes: You can push your changes to your forked repository by running git push origin <BRANCH_NAME> . Submit a Pull Request (PR): After pushing your changes, go to HPC-Docs github repository, and click on \"New pull request\". Fill in the necessary details and submit the PR. Once your have submitted the PR, a bunch of automatic workflows will be triggered. readthedocs will build a preview of your document and add it to the PR. This allows you to preview your changes before they are merged into the main branch. Please note, for larger changes, it's often a good idea to discuss your plans in an issue before investing a lot of time in implementation. Building the Documentation Locally \u00b6 If you want to build the documentation locally to see the changes you've made, you can do so by following these steps: Create an Environment \u00b6 To build the documentation locally, you'll need to install certain dependencies. Although this step is optional, we strongly recommend it. The example provided here utilizes a conda environment, but feel free to use any Python 3 environment of your preference. conda env create -f conda.yaml conda activate mkdocs Preview Documentation Locally \u00b6 You can preview your documentation locally to make sure that your changes do not introduce any errors. With MkDocs, you can preview your changes by running mkdocs serve in your terminal. This starts a local server where you can preview your work. mkdocs serve --strict Note --strict flag will enable strict mode and treat warnings as errors. This is useful to ensure that your changes do not introduce any issues such as new pages that does not exist. Simple Contributions \u00b6 Warning At this stage, please avoid making any changes using this tool, since it will make changes directly to main branch. If you're looking to make a minor adjustment to our documentation, such as fixing a typo or adding minor enhancements to a few documents, GitHub's built-in web editor and web IDE make it easy. As you browse our documentation, you'll notice a pencil icon next to the header on each page. This icon is a shortcut to edit the current page. Here's how you can use this feature: Click on the pencil icon to open the editor. Make your desired changes. After you've made your changes, be sure to update the commit message. A clear and concise commit message helps us understand your contribution better. While not mandatory, we recommend renaming the branch for better organization and tracking of changes. Submit your changes. Feedback and Support \u00b6 If you have any questions or need assistance while contributing to this repository, please reach out to the repository maintainers or open an issue on the GitHub repository page. Thank you for your contributions and helping us create a valuable resource for NCAR's HPC community!","title":"Contributing to the Documentation"},{"location":"contributing/#contribution-guide","text":"Welcome to the NCAR HPC Resources GitHub repository! This guide provides an overview of how to contribute to this documentations and the standards to follow when adding content to this repository. Our goal is to create a comprehensive and user-friendly documentation resource for NCAR's HPC resources.","title":"Contribution Guide"},{"location":"contributing/#repository-overview","text":"This repository contains technical documentation for NCAR HPC resources. The documentation is written in Markdown, which is then converted to HTML/CSS/JS using the mkdocs static site generator. We have customized the mkdocs-material theme to align with NCAR branding and colors. Note Here is the reference to the mkdocs-material documentation features .","title":"Repository Overview"},{"location":"contributing/#making-contributions","text":"For modifications, such as a comprehensive revision of a section in the documentation, we recommend fork this repository, and clone the repository to your local machine and working from there.","title":"Making Contributions"},{"location":"contributing/#steps-to-contribute","text":"Fork the repository: Go to the repository page and click the \"Fork\" button to create a copy of the repository under your GitHub account. Clone the forked repository to your local machine: This can be done by running the following command in your terminal: git clone https://github.com/<YOUR_USERNAME>/hpc-docs-demo.git Replace <YOUR_USERNAME> with your GitHub username. Create a new branch: It's a good practice to create a new branch before you start making changes. This can be done by running: git checkout -b <BRANCH_NAME> Replace <BRANCH_NAME> with a name that gives a hint about the changes you're about to make. Make your changes: With your new branch checked out, you can start making changes to the documentation. Remember to save your work regularly. Tip You can live preview your changes locally by running mkdocs serve in your terminal. For more on this, see Building the Documentation Locally section. Commit your changes: Once you have made and tested your changes, stage the files you have modified using git add <file> or git add . to stage all changes. Then, commit your changes with a descriptive message using git commit -m \"<YOUR_COMMIT_MESSAGE>\" . Push your changes: You can push your changes to your forked repository by running git push origin <BRANCH_NAME> . Submit a Pull Request (PR): After pushing your changes, go to HPC-Docs github repository, and click on \"New pull request\". Fill in the necessary details and submit the PR. Once your have submitted the PR, a bunch of automatic workflows will be triggered. readthedocs will build a preview of your document and add it to the PR. This allows you to preview your changes before they are merged into the main branch. Please note, for larger changes, it's often a good idea to discuss your plans in an issue before investing a lot of time in implementation.","title":"Steps to Contribute"},{"location":"contributing/#building-the-documentation-locally","text":"If you want to build the documentation locally to see the changes you've made, you can do so by following these steps:","title":"Building the Documentation Locally"},{"location":"contributing/#create-an-environment","text":"To build the documentation locally, you'll need to install certain dependencies. Although this step is optional, we strongly recommend it. The example provided here utilizes a conda environment, but feel free to use any Python 3 environment of your preference. conda env create -f conda.yaml conda activate mkdocs","title":"Create an Environment"},{"location":"contributing/#preview-documentation-locally","text":"You can preview your documentation locally to make sure that your changes do not introduce any errors. With MkDocs, you can preview your changes by running mkdocs serve in your terminal. This starts a local server where you can preview your work. mkdocs serve --strict Note --strict flag will enable strict mode and treat warnings as errors. This is useful to ensure that your changes do not introduce any issues such as new pages that does not exist.","title":"Preview Documentation Locally"},{"location":"contributing/#simple-contributions","text":"Warning At this stage, please avoid making any changes using this tool, since it will make changes directly to main branch. If you're looking to make a minor adjustment to our documentation, such as fixing a typo or adding minor enhancements to a few documents, GitHub's built-in web editor and web IDE make it easy. As you browse our documentation, you'll notice a pencil icon next to the header on each page. This icon is a shortcut to edit the current page. Here's how you can use this feature: Click on the pencil icon to open the editor. Make your desired changes. After you've made your changes, be sure to update the commit message. A clear and concise commit message helps us understand your contribution better. While not mandatory, we recommend renaming the branch for better organization and tracking of changes. Submit your changes.","title":"Simple Contributions"},{"location":"contributing/#feedback-and-support","text":"If you have any questions or need assistance while contributing to this repository, please reach out to the repository maintainers or open an issue on the GitHub repository page. Thank you for your contributions and helping us create a valuable resource for NCAR's HPC community!","title":"Feedback and Support"},{"location":"allocations/","text":"Allocations \u00b6 The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community. Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu . Info The next university deadline for submitting Large Allocation Requests is March 13, 2023. University allocation opportunities \u00b6 NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September. See University allocations for details. Info Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards. Wyoming-NCAR Alliance \u00b6 The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process. Details of the Wyoming process are available at the University of Wyoming web site . NCAR lab allocation opportunities \u00b6 NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects. See NCAR allocations for more details.","title":"Allocations"},{"location":"allocations/#allocations","text":"The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community. Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu . Info The next university deadline for submitting Large Allocation Requests is March 13, 2023.","title":"Allocations"},{"location":"allocations/#university-allocation-opportunities","text":"NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September. See University allocations for details. Info Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards.","title":"University allocation opportunities"},{"location":"allocations/#wyoming-ncar-alliance","text":"The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process. Details of the Wyoming process are available at the University of Wyoming web site .","title":"Wyoming-NCAR Alliance"},{"location":"allocations/#ncar-lab-allocation-opportunities","text":"NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects. See NCAR allocations for more details.","title":"NCAR lab allocation opportunities"},{"location":"allocations/determining-computational-resource-needs/","text":"Determining computational resource needs \u00b6 Documenting your code\u2019s performance and scalability to demonstrate that your stated resource needs are reasonable is an important aspect of preparing an allocation request. These guidelines are intended to help you gather and present the data you need to support your request. Ultimately, you will complete a table like the one shown here to include in your application. With accompanying narrative that describes each experiment or experimental configuration, it will communicate clearly to reviewers what computing resources you need and how you will use those resources. Estimating Derecho allocation needs \u00b6 Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems. You can use a chart like this as a starting point. Review the documentation for the relevant allocation opportunity to learn what else is required. Experiment (Experimental configuration) Core-hours per simulation Number of simulations Total core-hours Totals CESM and WRF \u00b6 Community Earth System Model \u00b6 In the case of CESM , see the timing tables provided by the CESM team at NCAR for information that will help you develop a statement of resource requirements for your project. The computational cost of CESM experiments typically is expressed in core-hours (also known as processor element-hours or \"pe-hrs\") per simulated year, so the \u201cCost pe-hrs/yr\u201d column (the cost in core-hours for each simulated year) provides the necessary value needed to calculate the cost of a simulation; you provide the number of years. Your allocation request should show that the number of years proposed and the model resolution chosen are sufficient and necessary to answer your scientific question(s). Weather Research and Forecasting Model \u00b6 For WRF projects, review the guidance on our Optimizing WRF performance page. Follow those recommendations as you do some benchmark runs to estimate the number of core-hours you will need for each planned simulation. Cite that page in your core-hour request and describe for the review panel how you estimated your resource requirements. Other codes and models \u00b6 Proposed experiments may be different enough from the documented CESM and WRF simulations that you need to run your code several times to determine how many core-hours you\u2019ll need. If you are using other codes or models that do not have well-known or published performance information, you will need to document the performance and scalability of your codes to complete your resource request. (A reference to a web site or paper with performance and scalability details for the code or model is acceptable for purposes of an allocation request.) Presumably you\u2019ve run your code on a multi-core system and have at least a general idea of what resources you will need in order to run at the same scale or larger on the Cheyenne or Casper systems. To begin fine-tuning your general idea into a specific request for resources, consider these questions: How large is any data set that you need to load? How much memory needs to be available for you to complete a run? (The peak_memusage tool can tell you how much memory your program uses.) Your answers will help you calculate the minimum number of nodes you can use. Memory needed / memory per node = minimum nodes That minimum number of nodes can serve as your starting point unless you already know that a larger number will work. You may need anything from a few dozen to hundreds or thousands of cores. Document your code\u2019s performance with test runs on the Cheyenne system if possible, or on a reasonably similar platform and software stack. You will want to illustrate in a graph that your code performs well at increasingly higher scale, showing at least four points indicating the results of your runs (as in Figure 1). If your application\u2019s performance is not already well documented, you may need to establish a baseline by running it on a single node before scaling. On Derecho, your parallel code would be executed on a total of 128 cores on a single node. From that baseline, you can generate scaling data by making a series of runs on progressively greater numbers of nodes to determine the optimum number to use. Start by documenting the smallest run that you know will work. For example, say you\u2019ve run your parallel code on a similar system using 4,000 cores, or you\u2019ve had an opportunity to do some similar-size test runs on Cheyenne. Do additional larger runs to demonstrate that the code scales as you expect it to scale. To ensure accuracy, it can help to do several runs at each point on different days to detect variations that might result from changes in the machine\u2019s workload. Based on the hypothetical results shown in Figure 1, a run could be done efficiently using 1,000 cores. The job would finish more quickly, using less wall-clock time, than it would using 400 cores. To convert that into total core-hours needed for one type of simulation, multiply core-hours per simulation (the time needed to complete one simulation times the number of cores used) by the number of times you are proposing to run that type of simulation. Core-hours per simulation x total simulations = total core-hours That gives you the information you need to fill out one row of your table. Repeat the process for each type of simulation, then add the figures in the last column to get the overall core-hour total. Strong vs. weak scaling \u00b6 As you do your test runs, keep in mind that strong scaling \u2013 the degree to which performance improves as more processors are applied to a fixed problem size \u2013 is more useful for these purposes than weak scaling, which reflects the ability to solve larger problem sizes with more processors. Strong scaling will reflect the improvement in speed (resulting in reduced overall run time) as more processors are used. At some point, using more processors is likely to have little additional benefit, because the potential for increasing speed is limited by the ratio of serial to parallel code in the application. Figure 2 reflects such a scenario, which shows little or no performance benefit from using more than 5,000 cores. If you identify such a point in your test runs, it would make little sense to base your allocation request on higher numbers of cores. Reporting on performance \u00b6 When submitting your allocation request, provide documentation on how you generated your scaling data. Include a graph similar to those shown here to illustrate the results. Describe how flexible your code is regarding the number of processors it can use and why you chose a certain number on which to base your request. It also is helpful to include information on the portability of the code to other platforms and on your team\u2019s knowledge and experience with systems that are similar to Cheyenne. Use these links to download sample proposals: Example proposal 1 Example proposal 2 They are specific to large university allocations but are good examples of documenting performance that you can follow for other types of requests.","title":"Determining computational resource needs"},{"location":"allocations/determining-computational-resource-needs/#determining-computational-resource-needs","text":"Documenting your code\u2019s performance and scalability to demonstrate that your stated resource needs are reasonable is an important aspect of preparing an allocation request. These guidelines are intended to help you gather and present the data you need to support your request. Ultimately, you will complete a table like the one shown here to include in your application. With accompanying narrative that describes each experiment or experimental configuration, it will communicate clearly to reviewers what computing resources you need and how you will use those resources.","title":"Determining computational resource needs"},{"location":"allocations/determining-computational-resource-needs/#estimating-derecho-allocation-needs","text":"Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems. You can use a chart like this as a starting point. Review the documentation for the relevant allocation opportunity to learn what else is required. Experiment (Experimental configuration) Core-hours per simulation Number of simulations Total core-hours Totals","title":"Estimating Derecho allocation needs"},{"location":"allocations/determining-computational-resource-needs/#cesm-and-wrf","text":"","title":"CESM and WRF"},{"location":"allocations/determining-computational-resource-needs/#community-earth-system-model","text":"In the case of CESM , see the timing tables provided by the CESM team at NCAR for information that will help you develop a statement of resource requirements for your project. The computational cost of CESM experiments typically is expressed in core-hours (also known as processor element-hours or \"pe-hrs\") per simulated year, so the \u201cCost pe-hrs/yr\u201d column (the cost in core-hours for each simulated year) provides the necessary value needed to calculate the cost of a simulation; you provide the number of years. Your allocation request should show that the number of years proposed and the model resolution chosen are sufficient and necessary to answer your scientific question(s).","title":"Community Earth System Model"},{"location":"allocations/determining-computational-resource-needs/#weather-research-and-forecasting-model","text":"For WRF projects, review the guidance on our Optimizing WRF performance page. Follow those recommendations as you do some benchmark runs to estimate the number of core-hours you will need for each planned simulation. Cite that page in your core-hour request and describe for the review panel how you estimated your resource requirements.","title":"Weather Research and Forecasting Model"},{"location":"allocations/determining-computational-resource-needs/#other-codes-and-models","text":"Proposed experiments may be different enough from the documented CESM and WRF simulations that you need to run your code several times to determine how many core-hours you\u2019ll need. If you are using other codes or models that do not have well-known or published performance information, you will need to document the performance and scalability of your codes to complete your resource request. (A reference to a web site or paper with performance and scalability details for the code or model is acceptable for purposes of an allocation request.) Presumably you\u2019ve run your code on a multi-core system and have at least a general idea of what resources you will need in order to run at the same scale or larger on the Cheyenne or Casper systems. To begin fine-tuning your general idea into a specific request for resources, consider these questions: How large is any data set that you need to load? How much memory needs to be available for you to complete a run? (The peak_memusage tool can tell you how much memory your program uses.) Your answers will help you calculate the minimum number of nodes you can use. Memory needed / memory per node = minimum nodes That minimum number of nodes can serve as your starting point unless you already know that a larger number will work. You may need anything from a few dozen to hundreds or thousands of cores. Document your code\u2019s performance with test runs on the Cheyenne system if possible, or on a reasonably similar platform and software stack. You will want to illustrate in a graph that your code performs well at increasingly higher scale, showing at least four points indicating the results of your runs (as in Figure 1). If your application\u2019s performance is not already well documented, you may need to establish a baseline by running it on a single node before scaling. On Derecho, your parallel code would be executed on a total of 128 cores on a single node. From that baseline, you can generate scaling data by making a series of runs on progressively greater numbers of nodes to determine the optimum number to use. Start by documenting the smallest run that you know will work. For example, say you\u2019ve run your parallel code on a similar system using 4,000 cores, or you\u2019ve had an opportunity to do some similar-size test runs on Cheyenne. Do additional larger runs to demonstrate that the code scales as you expect it to scale. To ensure accuracy, it can help to do several runs at each point on different days to detect variations that might result from changes in the machine\u2019s workload. Based on the hypothetical results shown in Figure 1, a run could be done efficiently using 1,000 cores. The job would finish more quickly, using less wall-clock time, than it would using 400 cores. To convert that into total core-hours needed for one type of simulation, multiply core-hours per simulation (the time needed to complete one simulation times the number of cores used) by the number of times you are proposing to run that type of simulation. Core-hours per simulation x total simulations = total core-hours That gives you the information you need to fill out one row of your table. Repeat the process for each type of simulation, then add the figures in the last column to get the overall core-hour total.","title":"Other codes and models"},{"location":"allocations/determining-computational-resource-needs/#strong-vs-weak-scaling","text":"As you do your test runs, keep in mind that strong scaling \u2013 the degree to which performance improves as more processors are applied to a fixed problem size \u2013 is more useful for these purposes than weak scaling, which reflects the ability to solve larger problem sizes with more processors. Strong scaling will reflect the improvement in speed (resulting in reduced overall run time) as more processors are used. At some point, using more processors is likely to have little additional benefit, because the potential for increasing speed is limited by the ratio of serial to parallel code in the application. Figure 2 reflects such a scenario, which shows little or no performance benefit from using more than 5,000 cores. If you identify such a point in your test runs, it would make little sense to base your allocation request on higher numbers of cores.","title":"Strong vs. weak scaling"},{"location":"allocations/determining-computational-resource-needs/#reporting-on-performance","text":"When submitting your allocation request, provide documentation on how you generated your scaling data. Include a graph similar to those shown here to illustrate the results. Describe how flexible your code is regarding the number of processors it can use and why you chose a certain number on which to base your request. It also is helpful to include information on the portability of the code to other platforms and on your team\u2019s knowledge and experience with systems that are similar to Cheyenne. Use these links to download sample proposals: Example proposal 1 Example proposal 2 They are specific to large university allocations but are good examples of documenting performance that you can follow for other types of requests.","title":"Reporting on performance"},{"location":"allocations/chap/","text":"CISL HPC Allocations Panel \u00b6 Overview \u00b6 CISL's advisory panel on high-performance computing and services is the CISL HPC Allocations Panel (CHAP). The CHAP's primary responsibility is to assess the merit of large computing requests for CISL supercomputers and related resources. The panel accepts computing proposals from U.S. university researchers in the atmospheric and closely related sciences who are supported by the National Science Foundation (NSF). The CHAP recommends action with respect to a prospective user's request on the basis of the computational experimental design, computational effectiveness, and availability of computing resources. Note The next university deadline for submitting Large Allocation Requests is March 12, 2024. CHAP members, most of whom are from the university community, are appointed to three-year terms by the CISL Director. Meetings are scheduled twice a year, usually in May and October. NSF's program coordinator for the NCAR and Facilities Section attends the semi-annual meetings and provides guidance from the NSF. Core-hours Allocated and Used by University Projects \u00b6","title":"CISL HPC Allocations Panel"},{"location":"allocations/chap/#cisl-hpc-allocations-panel","text":"","title":"CISL HPC Allocations Panel"},{"location":"allocations/chap/#overview","text":"CISL's advisory panel on high-performance computing and services is the CISL HPC Allocations Panel (CHAP). The CHAP's primary responsibility is to assess the merit of large computing requests for CISL supercomputers and related resources. The panel accepts computing proposals from U.S. university researchers in the atmospheric and closely related sciences who are supported by the National Science Foundation (NSF). The CHAP recommends action with respect to a prospective user's request on the basis of the computational experimental design, computational effectiveness, and availability of computing resources. Note The next university deadline for submitting Large Allocation Requests is March 12, 2024. CHAP members, most of whom are from the university community, are appointed to three-year terms by the CISL Director. Meetings are scheduled twice a year, usually in May and October. NSF's program coordinator for the NCAR and Facilities Section attends the semi-annual meetings and provides guidance from the NSF.","title":"Overview"},{"location":"allocations/chap/#core-hours-allocated-and-used-by-university-projects","text":"","title":"Core-hours Allocated and Used by University Projects"},{"location":"allocations/chap/chap-allocation-review-criteria/","text":"CHAP: Allocation Review Criteria \u00b6 The CHAP assesses the merits of large computing requests from U.S. university researchers for use of CISL supercomputing resources. Computing requests are accepted in the atmospheric and closely related sciences for projects that are supported by the National Science Foundation. The panel recommends action with respect to a prospective user's request on the basis of the computational experimental design, computational effectiveness, and availability of computing resources as described below. Overall context \u00b6 As part of reviewing the merits of requests for CISL resource allocations, the CHAP and CISL allocations staff will ensure that: All requests for resources that exceed a threshold level determined by CISL and the CHAP shall be peer-reviewed. Written reviews of the resource requests shall be completed in a timely way and made available to the requestors. Recommendations to CISL management for the allocation of resources based on the requests, reviews, and available resources shall be documented. The allocations process shall be consistent with the conflict-of-interest policy and shall maintain confidentiality of requestors and their reviews. Purpose and scope of reviews \u00b6 In its review of allocation proposals, the CHAP verifies the suitability of the work for CISL resources, considers the ability of the research team to complete the work, and most significantly, reviews the merit of the proposed computational plan. Scientific eligibility \u00b6 A request for resources will succinctly state the scientific impact of the research to be conducted and the existing merit-reviewed support for the research as demonstrated by current financial support from NSF. The scientific merit and approach will not be subject to further review by the CHAP. Since NCAR computing resources are provided specifically for the atmospheric and closely related sciences, the request must fall within these areas. Merit review criteria \u00b6 The justification of the resource request will be reviewed against three criteria, which apply to both computational and storage resources, with the level of detail of the review rising with the size of the requested resources: Effectiveness of methodology \u00b6 For computational resource requests, the choice of applications, methods, algorithms, and techniques to be employed to accomplish the stated scientific objectives should be reasonably described and motivated. For data storage resource requests, the data to be stored should be reasonably described and motivated with respect to the stated research objectives. Appropriateness of research plan \u00b6 The steps in the research plan should explain how the scientific objectives will be achieved. For computational experiments, the proposed computations should encompass simulation parameters (grid size, time scale, ensemble parameters, and so on) that are needed to obtain accurate and meaningful results, as well as the human resources that can be devoted to the task. For storage resources, the justification should describe the rationale for determining which data will be stored. The amount of resources (of all types) requested should be derived from the methodology and research plan. If there are serious concerns about the research plan, reviewers should share their concerns with the proposer, and may decide to proceed only after the concerns have been addressed. Efficiency of resource use \u00b6 The resources requested should be used as efficiently as is reasonably possible and in accordance with the recommended use guidelines of those resources. In exceptional cases, where the reviewers conclude that the proposed methods are so inefficient that they amount to a waste of public resources, they should not approve the request until the proposer has addressed their concerns. For computational resources, performance and parallel scaling data should be provided along with a discussion of optimization or other work done to improve the applications. For storage resources, the CHAP will consider the choices made in managing the project's data, the value of that data both within the proposing research team and among the wider community, approaches for data access and dissemination, and long-term retention plans. Prior accomplishments \u00b6 For ongoing computational activities, the CHAP will consider the progress made using prior allocations, including the publication of peer-reviewed manuscripts, other communications within the community, and the effective estimation and use of CISL resources from prior requests.","title":"Allocation Review Criteria"},{"location":"allocations/chap/chap-allocation-review-criteria/#chap-allocation-review-criteria","text":"The CHAP assesses the merits of large computing requests from U.S. university researchers for use of CISL supercomputing resources. Computing requests are accepted in the atmospheric and closely related sciences for projects that are supported by the National Science Foundation. The panel recommends action with respect to a prospective user's request on the basis of the computational experimental design, computational effectiveness, and availability of computing resources as described below.","title":"CHAP: Allocation Review Criteria"},{"location":"allocations/chap/chap-allocation-review-criteria/#overall-context","text":"As part of reviewing the merits of requests for CISL resource allocations, the CHAP and CISL allocations staff will ensure that: All requests for resources that exceed a threshold level determined by CISL and the CHAP shall be peer-reviewed. Written reviews of the resource requests shall be completed in a timely way and made available to the requestors. Recommendations to CISL management for the allocation of resources based on the requests, reviews, and available resources shall be documented. The allocations process shall be consistent with the conflict-of-interest policy and shall maintain confidentiality of requestors and their reviews.","title":"Overall context"},{"location":"allocations/chap/chap-allocation-review-criteria/#purpose-and-scope-of-reviews","text":"In its review of allocation proposals, the CHAP verifies the suitability of the work for CISL resources, considers the ability of the research team to complete the work, and most significantly, reviews the merit of the proposed computational plan.","title":"Purpose and scope of reviews"},{"location":"allocations/chap/chap-allocation-review-criteria/#scientific-eligibility","text":"A request for resources will succinctly state the scientific impact of the research to be conducted and the existing merit-reviewed support for the research as demonstrated by current financial support from NSF. The scientific merit and approach will not be subject to further review by the CHAP. Since NCAR computing resources are provided specifically for the atmospheric and closely related sciences, the request must fall within these areas.","title":"Scientific eligibility"},{"location":"allocations/chap/chap-allocation-review-criteria/#merit-review-criteria","text":"The justification of the resource request will be reviewed against three criteria, which apply to both computational and storage resources, with the level of detail of the review rising with the size of the requested resources:","title":"Merit review criteria"},{"location":"allocations/chap/chap-allocation-review-criteria/#effectiveness-of-methodology","text":"For computational resource requests, the choice of applications, methods, algorithms, and techniques to be employed to accomplish the stated scientific objectives should be reasonably described and motivated. For data storage resource requests, the data to be stored should be reasonably described and motivated with respect to the stated research objectives.","title":"Effectiveness of methodology"},{"location":"allocations/chap/chap-allocation-review-criteria/#appropriateness-of-research-plan","text":"The steps in the research plan should explain how the scientific objectives will be achieved. For computational experiments, the proposed computations should encompass simulation parameters (grid size, time scale, ensemble parameters, and so on) that are needed to obtain accurate and meaningful results, as well as the human resources that can be devoted to the task. For storage resources, the justification should describe the rationale for determining which data will be stored. The amount of resources (of all types) requested should be derived from the methodology and research plan. If there are serious concerns about the research plan, reviewers should share their concerns with the proposer, and may decide to proceed only after the concerns have been addressed.","title":"Appropriateness of research plan"},{"location":"allocations/chap/chap-allocation-review-criteria/#efficiency-of-resource-use","text":"The resources requested should be used as efficiently as is reasonably possible and in accordance with the recommended use guidelines of those resources. In exceptional cases, where the reviewers conclude that the proposed methods are so inefficient that they amount to a waste of public resources, they should not approve the request until the proposer has addressed their concerns. For computational resources, performance and parallel scaling data should be provided along with a discussion of optimization or other work done to improve the applications. For storage resources, the CHAP will consider the choices made in managing the project's data, the value of that data both within the proposing research team and among the wider community, approaches for data access and dissemination, and long-term retention plans.","title":"Efficiency of resource use"},{"location":"allocations/chap/chap-allocation-review-criteria/#prior-accomplishments","text":"For ongoing computational activities, the CHAP will consider the progress made using prior allocations, including the publication of peer-reviewed manuscripts, other communications within the community, and the effective estimation and use of CISL resources from prior requests.","title":"Prior accomplishments"},{"location":"allocations/chap/chap-conflict-of-interest-policy/","text":"CHAP: Conflict of Interest Policy \u00b6 It is NCAR policy that the Computational and Information Systems Laboratory (CISL) HPC Allocations Panel (CHAP) procedures for evaluating National Science Foundation (NSF) computational resource requests are fair and equitable to all requestors and protect the integrity of the research, science, the NSF and NCAR. Recommendations are to be based on objective judgments of merit without regard to subjective personal biases. The guidelines and ethical standards presented here provide a framework by which conflict of interest (COI) situations can be identified and resolved, thus minimizing the level of personal bias in the provision of high performance computing resources to the NSF community. A conflict of interest is a clash between an individual\u2019s concern for the public interest or the best interest of NCAR and his or her private interests or allegiances. Conflicts of interest, actual or perceived, may compromise NCAR's integrity and standing in the research community, its sponsors, and the professional reputations of individuals. They also compromise the effectiveness of the decision-making process by warping and biasing such effectiveness. As such, conflicts of interest must be scrupulously avoided. Individuals involved with CHAP activities shall act impartially and not give preferential treatment to any individual or organization, and they may not use their position on CHAP or knowledge gained through CHAP activities to obtain a personal advantage either for themselves or for any other person or entity in whom or in which they have a financial or other vested interest. Potential and actual conflicts of interest, or the appearance of such, must be managed so that NCAR\u2019s assessment process is not compromised, research conducted through NCAR is free from bias, the investment of the public is protected, and confidence in the integrity of NCAR\u2019s activities is maintained. Conflicts of interest are common and even inevitable, so that a disqualification to review should be understood to be a positive solution and in no way is a reproach. Whether particular circumstances create an appearance that the ethical standards outlined in this document have been violated shall be determined from the perspective of a reasonable person with knowledge of the relevant facts. Responsibilities of members \u00b6 Appointment as a CHAP member requires awareness of COI situations that may arise during the evaluation of resource requests. Conflicts of interest may arise, for example, in the following situations: professional and personal relationship with a requestor or requestor\u2019s department; use of inside information or access to such information; financial, investment, or other ownership interests; use of confidential information; subcontracts with employees, their immediate families, and their business associates; work with UCAR/NCAR contractors; involvement in legal actions against the Federal government and other sponsors; improper use of the UCAR or NCAR name or affiliation; and improper use of NCAR facilities and resources. The procedures followed with regard to COIs and CHAP activities are those of Disclosure , Avoidance , and Removal . Disclosure \u00b6 Prior to the assignment of reviewers, CISL will summarize all known COI\u2019s for the CHAP chair (Chair). In some instances, a COI is known only to the individual panel member. Each panel member is responsible to declare immediately each COI and to bring the matter promptly to the attention of CISL and the Chair. The Chair, acting as an objective, disinterested third party, determines how the matter should be handled and additional steps, if any, to take. Simply stating and documenting the existence of a conflict of interest does not suffice to eliminate it. A written record of how each conflict was resolved for each CHAP panelist shall be compiled by the Chair as part of the official record of the meeting. Avoidance \u00b6 Members should avoid all conflicts of interest or the appearance of such. In the course of their duties with CHAP, members should avoid situations in which they can influence or appear to influence a decision or course of action, as well as any actions that may give monetary gain or personal benefit to themselves or to those with whom they are associated professionally and personally, as covered under the relationships discussed infra. Removal \u00b6 In those instances in which a CHAP member is a Requesting Scientist on a resource request for the current panel meeting, or a Principal Investigator (PI), or Co-Principal Investigator (CO-PI) on the NSF award supporting the resource request, the resource request will be evaluated and a resource allocation recommendation will be made to the CISL Director of Operations and Services prior to the panel meeting. The resource request will be discussed via email or via a teleconference involving those panelists without a COI on any of the requests being discussed. In instances where the Chair has judged that a COI exists for a CHAP member who is not a PI, CO-PI, or Requesting Scientist for a proposal being considered, or if there is a need for further discussion of a request that was reviewed in advance to balance the resources allocated, the conflicted panel member shall physically leave the room during discussion of the proposal. In other instances in which a COI is disclosed, and the Chair has judged it to be minor, the panelist may continue to participate in the discussions. Examples of conflict of interest situations \u00b6 Conflicts of interest exist for any of the relationships below: Affiliations with a requestor\u2019s institution: Current employment (formal or informal) within the same department or institution. In case of UCAR/NCAR employees, conflicts exist within the same division, institute or laboratory. Any affiliation with the requestor institution including, but not limited to, current membership on a visiting committee or similar body at the requestor\u2019s department, holder of any office, governing board membership, or relevant committee chairpersonship in the requestor\u2019s department. Currently seeking employment with the institution. Relationships with an Investigator or other person who has a personal, academic and/or financial interest in the request and/or proposal: Known family or marriage relationship. Business or commercial partnership. Present association as primary thesis advisor or thesis student or past association in such a capacity over the last ten years. Professional collaboration involving research and publication over the past four years or as Principal Investigator and Co-Principal Investigator on a current resource request. Other relationships with the requestor or the request. The interests of the following persons are to be treated as if they were the panel member\u2019s own: Any relationship, such as close personal friendship, that might affect the member\u2019s judgments or be seen as doing so by a reasonable person familiar with the relationship. Any other conflicts known to the panel member that would prevent him/her from reviewing a project in a non-biased, fair and objective way. CHAP participants are encouraged to seek guidance on these conflict of interest guidelines at any time. For UCAR/NCAR employees other policies may apply, including: Conflict of Interest (1-1-4), Ethical Conduct (1-1-23), and Investigator Financial Disclosure (1-1-27). Download the Conflict of Interest policy (pdf) .","title":"Conflict of Interest Policy"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#chap-conflict-of-interest-policy","text":"It is NCAR policy that the Computational and Information Systems Laboratory (CISL) HPC Allocations Panel (CHAP) procedures for evaluating National Science Foundation (NSF) computational resource requests are fair and equitable to all requestors and protect the integrity of the research, science, the NSF and NCAR. Recommendations are to be based on objective judgments of merit without regard to subjective personal biases. The guidelines and ethical standards presented here provide a framework by which conflict of interest (COI) situations can be identified and resolved, thus minimizing the level of personal bias in the provision of high performance computing resources to the NSF community. A conflict of interest is a clash between an individual\u2019s concern for the public interest or the best interest of NCAR and his or her private interests or allegiances. Conflicts of interest, actual or perceived, may compromise NCAR's integrity and standing in the research community, its sponsors, and the professional reputations of individuals. They also compromise the effectiveness of the decision-making process by warping and biasing such effectiveness. As such, conflicts of interest must be scrupulously avoided. Individuals involved with CHAP activities shall act impartially and not give preferential treatment to any individual or organization, and they may not use their position on CHAP or knowledge gained through CHAP activities to obtain a personal advantage either for themselves or for any other person or entity in whom or in which they have a financial or other vested interest. Potential and actual conflicts of interest, or the appearance of such, must be managed so that NCAR\u2019s assessment process is not compromised, research conducted through NCAR is free from bias, the investment of the public is protected, and confidence in the integrity of NCAR\u2019s activities is maintained. Conflicts of interest are common and even inevitable, so that a disqualification to review should be understood to be a positive solution and in no way is a reproach. Whether particular circumstances create an appearance that the ethical standards outlined in this document have been violated shall be determined from the perspective of a reasonable person with knowledge of the relevant facts.","title":"CHAP: Conflict of Interest Policy"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#responsibilities-of-members","text":"Appointment as a CHAP member requires awareness of COI situations that may arise during the evaluation of resource requests. Conflicts of interest may arise, for example, in the following situations: professional and personal relationship with a requestor or requestor\u2019s department; use of inside information or access to such information; financial, investment, or other ownership interests; use of confidential information; subcontracts with employees, their immediate families, and their business associates; work with UCAR/NCAR contractors; involvement in legal actions against the Federal government and other sponsors; improper use of the UCAR or NCAR name or affiliation; and improper use of NCAR facilities and resources. The procedures followed with regard to COIs and CHAP activities are those of Disclosure , Avoidance , and Removal .","title":"Responsibilities of members"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#disclosure","text":"Prior to the assignment of reviewers, CISL will summarize all known COI\u2019s for the CHAP chair (Chair). In some instances, a COI is known only to the individual panel member. Each panel member is responsible to declare immediately each COI and to bring the matter promptly to the attention of CISL and the Chair. The Chair, acting as an objective, disinterested third party, determines how the matter should be handled and additional steps, if any, to take. Simply stating and documenting the existence of a conflict of interest does not suffice to eliminate it. A written record of how each conflict was resolved for each CHAP panelist shall be compiled by the Chair as part of the official record of the meeting.","title":"Disclosure"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#avoidance","text":"Members should avoid all conflicts of interest or the appearance of such. In the course of their duties with CHAP, members should avoid situations in which they can influence or appear to influence a decision or course of action, as well as any actions that may give monetary gain or personal benefit to themselves or to those with whom they are associated professionally and personally, as covered under the relationships discussed infra.","title":"Avoidance"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#removal","text":"In those instances in which a CHAP member is a Requesting Scientist on a resource request for the current panel meeting, or a Principal Investigator (PI), or Co-Principal Investigator (CO-PI) on the NSF award supporting the resource request, the resource request will be evaluated and a resource allocation recommendation will be made to the CISL Director of Operations and Services prior to the panel meeting. The resource request will be discussed via email or via a teleconference involving those panelists without a COI on any of the requests being discussed. In instances where the Chair has judged that a COI exists for a CHAP member who is not a PI, CO-PI, or Requesting Scientist for a proposal being considered, or if there is a need for further discussion of a request that was reviewed in advance to balance the resources allocated, the conflicted panel member shall physically leave the room during discussion of the proposal. In other instances in which a COI is disclosed, and the Chair has judged it to be minor, the panelist may continue to participate in the discussions.","title":"Removal"},{"location":"allocations/chap/chap-conflict-of-interest-policy/#examples-of-conflict-of-interest-situations","text":"Conflicts of interest exist for any of the relationships below: Affiliations with a requestor\u2019s institution: Current employment (formal or informal) within the same department or institution. In case of UCAR/NCAR employees, conflicts exist within the same division, institute or laboratory. Any affiliation with the requestor institution including, but not limited to, current membership on a visiting committee or similar body at the requestor\u2019s department, holder of any office, governing board membership, or relevant committee chairpersonship in the requestor\u2019s department. Currently seeking employment with the institution. Relationships with an Investigator or other person who has a personal, academic and/or financial interest in the request and/or proposal: Known family or marriage relationship. Business or commercial partnership. Present association as primary thesis advisor or thesis student or past association in such a capacity over the last ten years. Professional collaboration involving research and publication over the past four years or as Principal Investigator and Co-Principal Investigator on a current resource request. Other relationships with the requestor or the request. The interests of the following persons are to be treated as if they were the panel member\u2019s own: Any relationship, such as close personal friendship, that might affect the member\u2019s judgments or be seen as doing so by a reasonable person familiar with the relationship. Any other conflicts known to the panel member that would prevent him/her from reviewing a project in a non-biased, fair and objective way. CHAP participants are encouraged to seek guidance on these conflict of interest guidelines at any time. For UCAR/NCAR employees other policies may apply, including: Conflict of Interest (1-1-4), Ethical Conduct (1-1-23), and Investigator Financial Disclosure (1-1-27). Download the Conflict of Interest policy (pdf) .","title":"Examples of conflict of interest situations"},{"location":"allocations/ncar-allocations/","text":"NCAR allocations \u00b6 A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the NCAR-Wyoming Supercomputing Center (NWSC) in 2012. With the subsequent introduction of the Cheyenne HPC system in 2017 and now Derecho in 2023, the capabilities and capacity of this resource environment expanded significantly. NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users. Allocation categories \u00b6 Three categories of allocations are available to NCAR users: NCAR Strategic Capability (NSC) Projects NCAR Director\u2019s Reserve NCAR Lab Grants Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below. NCAR Strategic Capability (NSC) projects \u00b6 After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR. See the NSC Projects page for details, including the next deadline for submitting requests. NCAR Director's Reserve \u00b6 The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR). Director\u2019s Reserve requests must meet the following two criteria: The project should have clear relevance to the NCAR mission or strategic priorities. The project will be completed in less than one year and should not be an ongoing or recurring activity. Reserve requests also should also meet at least some of the following six criteria: The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs). The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm). The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines. The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political). The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant. The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.) Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation. To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu . Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days. Reporting \u00b6 Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority. For all projects, the project lead should prepare a short write-up, usually less than one page. Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award. NCAR Lab Grants \u00b6 NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request. Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.) The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL. In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL. NCAR and university use \u00b6 The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance. To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools. NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations. UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies. NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects. For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool. A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration. NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&P office has approved rates for use in the proposal process.","title":"NCAR allocations"},{"location":"allocations/ncar-allocations/#ncar-allocations","text":"A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the NCAR-Wyoming Supercomputing Center (NWSC) in 2012. With the subsequent introduction of the Cheyenne HPC system in 2017 and now Derecho in 2023, the capabilities and capacity of this resource environment expanded significantly. NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users.","title":"NCAR allocations"},{"location":"allocations/ncar-allocations/#allocation-categories","text":"Three categories of allocations are available to NCAR users: NCAR Strategic Capability (NSC) Projects NCAR Director\u2019s Reserve NCAR Lab Grants Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below.","title":"Allocation categories"},{"location":"allocations/ncar-allocations/#ncar-strategic-capability-nsc-projects","text":"After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR. See the NSC Projects page for details, including the next deadline for submitting requests.","title":"NCAR Strategic Capability (NSC) projects"},{"location":"allocations/ncar-allocations/#ncar-directors-reserve","text":"The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR). Director\u2019s Reserve requests must meet the following two criteria: The project should have clear relevance to the NCAR mission or strategic priorities. The project will be completed in less than one year and should not be an ongoing or recurring activity. Reserve requests also should also meet at least some of the following six criteria: The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs). The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm). The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines. The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political). The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant. The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.) Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation. To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu . Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days.","title":"NCAR Director's Reserve"},{"location":"allocations/ncar-allocations/#reporting","text":"Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority. For all projects, the project lead should prepare a short write-up, usually less than one page. Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award.","title":"Reporting"},{"location":"allocations/ncar-allocations/#ncar-lab-grants","text":"NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request. Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.) The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL. In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL.","title":"NCAR Lab Grants"},{"location":"allocations/ncar-allocations/#ncar-and-university-use","text":"The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance. To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools. NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations. UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies. NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects. For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool. A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration. NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&P office has approved rates for use in the proposal process.","title":"NCAR and university use"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/","text":"NCAR Strategic Capability (NSC) projects \u00b6 The next NSC submission deadline is September 19, 2023. NCAR researchers and computational scientists are encouraged to submit requests for the first NCAR Strategic Capability (NSC) projects to be run on the new Derecho system. Requests will be accepted through March 6. These allocations should target large-scale projects lasting one year and align with NCAR\u2019s scientific priorities and strategic plans. Requests for Cheyenne core-hours will be accepted but they must indicate how the additional allocation is necessary to complete work that is already underway on Cheyenne or be limited to work that can be completed by the end of 2023. Due to the rapidly growing scale of the data generated by many projects and the constraints on storage available within the CISL environment, the NSC projects review panel is scrutinizing requests for disk and tape storage closely. As with poorly justified requests for computing time, poorly justified requests for storage resources will result in reduced storage allocation awards. See additional guidance in these instructions for preparing proposal documents , which apply to NSC project requests as well as to large university requests. Long-term storage plans for NSC project data in Campaign Storage should be coordinated with the requester's lab(s). The data management plan section in your NSC request document should describe the arrangements made with your lab. NCAR computational scientists are encouraged to submit requests for the next round of NCAR Strategic Capability (NSC) projects. These opportunities occur approximately every six months. Potential submitters are encouraged to consider applying to the opportunity that best aligns with their project's anticipated timetable and readiness. In most cases, a project should consider the opportunity that starts shortly after their planned start, so that preliminary and benchmarking results can be submitted as part of the NSC proposal. A project for the same or similar work can receive an allocation only once a year. Because of the competitive nature of these allocations, labs may have chosen to coordinate submissions from each lab. Potential applicants are encouraged to contact their lab's or division's allocation representative before submitting a request. NSC allocations \u00b6 NCAR Strategic Capability (NSC) project allocations target large-scale projects lasting one year to a few years (but not indefinitely long) that align with NCAR\u2019s scientific priorities and strategic plans. Twice a year, the NCAR Executive Committee (EC) reviews and approves a set of NSC projects for the coming 12-month period. NSC projects requiring more than one year\u2019s allocation must submit continuation requests each year to report progress toward the project\u2019s objectives. To be considered for an NSC allocation, a proposed project: should directly relate to one or more specific priorities in the 2020-2024 NCAR Strategic Plan . must be technically ready to begin production runs from the start of the allocation period. Requests should provide sufficient details to convince reviewers of the project's readiness; projects may be penalized for delayed progress (see below). must require significant computational resources, above and beyond the minimum level defined by the NCAR Executive Committee. should have a well-defined scope and completion timeline. may be linked to an agency funding award or awards separate from the NCAR Base funding. The progress of NSC projects will be monitored quarterly to ensure that projects are making sufficient progress and that all projects can complete their work in the remaining allocation period. After each quarter, projects may be subject to losing a portion of their unspent, prorated allocation. Exceptions can be made for projects that identify in their submissions that they have compressed or alternative timetables due to external factors, such as the need to align computations with a planned field campaign or calendar season. NSC eligibility \u00b6 All NSC project requests must have a full or part-time regular NCAR staff member with an R or T appointment as project lead. In order for term employees to be eligible as project leads, the period of performance of the project should not extend beyond the employee's term date. Labs may choose to implement policies to coordinate the submissions from the lab in each request period. Joint work with university collaborators is eligible. Projects that span labs are encouraged, though a single project lead should be identified. NSC allocations have a minimum request size of 10 million core-hours; the minimum size may be revised by the NCAR EC, and the EC may choose to permit exceptions for cause. There is no maximum size limit, though in practice the review process will attempt to accommodate approximately a dozen large-scale projects each year. Consistent with the NSC objectives, NSC requests should not aggregate many smaller projects out of the same lab to meet the minimum request limit. Request format \u00b6 NSC requests must prepare Proposal Documents, which should follow the guidance and structure for large allocation requests for universities . Notably, NSC requests must include a five-page summary along with relevant supporting documentation. Review process \u00b6 NSC requests are subject to two-phase review, with final approval by the NCAR EC. The first phase is a scientific and strategic evaluation of the project. The panel conducting this review is composed of representatives of each NCAR lab and program who are appointed by the lab and program directors. This panel evaluates proposals according to several criteria: The three criteria used by the CHAP in its evaluation of large university requests \u2013 the effectiveness of the methodology, the appropriateness of the research plan, and the efficiency of resource use. The technical readiness assessment provides input into this aspect of the evaluation. The scientific appropriateness of the project and its relationship to NCAR strategic priorities. Whether the large-scale computational resources needed are commensurate with the project\u2019s strategic priority. The panel recommends allocation levels and identifies priority ranking for submitted requests. A written review summary is made available to requestors following the final decision on awards. All panel representatives from labs and programs not involved in the proposed project review each request. In the second phase, the EC approves or agrees to modify the recommendations of the review panel. CISL then establishes the final allocation awards. Should any awarded project encounter issues that require it to stop work or be unable to complete its proposed work, additional allocation awards may be made to unawarded requests in order of priority rank and as resource availability permits. Review schedule \u00b6 NSC requests are reviewed twice per year. Projects arising too late for NSC consideration will either need to wait, identify bridging allocations from NCAR labs, or apply for startup allocations via the NCAR Director\u2019s Reserve. Such NSC pre-awards must satisfy the criteria for a Director\u2019s Reserve award. Continuation and reporting \u00b6 NSC allocations come with commensurate reporting requirements. For those projects requiring more than one year\u2019s allocation, a continuation request will need to be submitted as part of the next year\u2019s NSC request and review process. The continuation request should include a short write-up (approximately one page) for CISL\u2019s portion of the NCAR Annual Report. At the completion of each project, the project lead documents the work conducted, resulting outcomes, and contributions toward the strategic priority by preparing both a short write-up (approximately one page) for CISL\u2019s portion of the NCAR Annual Report and a brief, 15-minute presentation to the Executive Committee.","title":"NCAR Strategic Capability (NSC) projects"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#ncar-strategic-capability-nsc-projects","text":"The next NSC submission deadline is September 19, 2023. NCAR researchers and computational scientists are encouraged to submit requests for the first NCAR Strategic Capability (NSC) projects to be run on the new Derecho system. Requests will be accepted through March 6. These allocations should target large-scale projects lasting one year and align with NCAR\u2019s scientific priorities and strategic plans. Requests for Cheyenne core-hours will be accepted but they must indicate how the additional allocation is necessary to complete work that is already underway on Cheyenne or be limited to work that can be completed by the end of 2023. Due to the rapidly growing scale of the data generated by many projects and the constraints on storage available within the CISL environment, the NSC projects review panel is scrutinizing requests for disk and tape storage closely. As with poorly justified requests for computing time, poorly justified requests for storage resources will result in reduced storage allocation awards. See additional guidance in these instructions for preparing proposal documents , which apply to NSC project requests as well as to large university requests. Long-term storage plans for NSC project data in Campaign Storage should be coordinated with the requester's lab(s). The data management plan section in your NSC request document should describe the arrangements made with your lab. NCAR computational scientists are encouraged to submit requests for the next round of NCAR Strategic Capability (NSC) projects. These opportunities occur approximately every six months. Potential submitters are encouraged to consider applying to the opportunity that best aligns with their project's anticipated timetable and readiness. In most cases, a project should consider the opportunity that starts shortly after their planned start, so that preliminary and benchmarking results can be submitted as part of the NSC proposal. A project for the same or similar work can receive an allocation only once a year. Because of the competitive nature of these allocations, labs may have chosen to coordinate submissions from each lab. Potential applicants are encouraged to contact their lab's or division's allocation representative before submitting a request.","title":"NCAR Strategic Capability (NSC) projects"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#nsc-allocations","text":"NCAR Strategic Capability (NSC) project allocations target large-scale projects lasting one year to a few years (but not indefinitely long) that align with NCAR\u2019s scientific priorities and strategic plans. Twice a year, the NCAR Executive Committee (EC) reviews and approves a set of NSC projects for the coming 12-month period. NSC projects requiring more than one year\u2019s allocation must submit continuation requests each year to report progress toward the project\u2019s objectives. To be considered for an NSC allocation, a proposed project: should directly relate to one or more specific priorities in the 2020-2024 NCAR Strategic Plan . must be technically ready to begin production runs from the start of the allocation period. Requests should provide sufficient details to convince reviewers of the project's readiness; projects may be penalized for delayed progress (see below). must require significant computational resources, above and beyond the minimum level defined by the NCAR Executive Committee. should have a well-defined scope and completion timeline. may be linked to an agency funding award or awards separate from the NCAR Base funding. The progress of NSC projects will be monitored quarterly to ensure that projects are making sufficient progress and that all projects can complete their work in the remaining allocation period. After each quarter, projects may be subject to losing a portion of their unspent, prorated allocation. Exceptions can be made for projects that identify in their submissions that they have compressed or alternative timetables due to external factors, such as the need to align computations with a planned field campaign or calendar season.","title":"NSC allocations"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#nsc-eligibility","text":"All NSC project requests must have a full or part-time regular NCAR staff member with an R or T appointment as project lead. In order for term employees to be eligible as project leads, the period of performance of the project should not extend beyond the employee's term date. Labs may choose to implement policies to coordinate the submissions from the lab in each request period. Joint work with university collaborators is eligible. Projects that span labs are encouraged, though a single project lead should be identified. NSC allocations have a minimum request size of 10 million core-hours; the minimum size may be revised by the NCAR EC, and the EC may choose to permit exceptions for cause. There is no maximum size limit, though in practice the review process will attempt to accommodate approximately a dozen large-scale projects each year. Consistent with the NSC objectives, NSC requests should not aggregate many smaller projects out of the same lab to meet the minimum request limit.","title":"NSC eligibility"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#request-format","text":"NSC requests must prepare Proposal Documents, which should follow the guidance and structure for large allocation requests for universities . Notably, NSC requests must include a five-page summary along with relevant supporting documentation.","title":"Request format"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#review-process","text":"NSC requests are subject to two-phase review, with final approval by the NCAR EC. The first phase is a scientific and strategic evaluation of the project. The panel conducting this review is composed of representatives of each NCAR lab and program who are appointed by the lab and program directors. This panel evaluates proposals according to several criteria: The three criteria used by the CHAP in its evaluation of large university requests \u2013 the effectiveness of the methodology, the appropriateness of the research plan, and the efficiency of resource use. The technical readiness assessment provides input into this aspect of the evaluation. The scientific appropriateness of the project and its relationship to NCAR strategic priorities. Whether the large-scale computational resources needed are commensurate with the project\u2019s strategic priority. The panel recommends allocation levels and identifies priority ranking for submitted requests. A written review summary is made available to requestors following the final decision on awards. All panel representatives from labs and programs not involved in the proposed project review each request. In the second phase, the EC approves or agrees to modify the recommendations of the review panel. CISL then establishes the final allocation awards. Should any awarded project encounter issues that require it to stop work or be unable to complete its proposed work, additional allocation awards may be made to unawarded requests in order of priority rank and as resource availability permits.","title":"Review process"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#review-schedule","text":"NSC requests are reviewed twice per year. Projects arising too late for NSC consideration will either need to wait, identify bridging allocations from NCAR labs, or apply for startup allocations via the NCAR Director\u2019s Reserve. Such NSC pre-awards must satisfy the criteria for a Director\u2019s Reserve award.","title":"Review schedule"},{"location":"allocations/ncar-allocations/ncar-strategic-capability-nsc-projects/#continuation-and-reporting","text":"NSC allocations come with commensurate reporting requirements. For those projects requiring more than one year\u2019s allocation, a continuation request will need to be submitted as part of the next year\u2019s NSC request and review process. The continuation request should include a short write-up (approximately one page) for CISL\u2019s portion of the NCAR Annual Report. At the completion of each project, the project lead documents the work conducted, resulting outcomes, and contributions toward the strategic priority by preparing both a short write-up (approximately one page) for CISL\u2019s portion of the NCAR Annual Report and a brief, 15-minute presentation to the Executive Committee.","title":"Continuation and reporting"},{"location":"allocations/university-allocations/","text":"University allocations \u00b6 The next university deadline for submitting Large Allocation Requests is September 12, 2023. University use of the NCAR HPC environment is intended to support Earth system science and related research by researchers at U.S. institutions. The emphasis is on projects that may be beyond the scope of a researcher\u2019s local university computing capabilities. Eligible researchers and activities incur no costs for the use of NCAR resources. Recent Changes We have recently updated these policies, both to prepare for Derecho and to expand opportunities for University researchers. Most notably, we have created a new Data Analysis opportunity to allow more researchers to analyze NCAR-hosted data sets, and we have redefined \u201cnew faculty\u201d to be any faculty member who has not computed at NCAR before. We have also called out options for expanding or extending the smaller-scale projects and clarified the eligibility language to emphasize the range of post-secondary institutions welcome to use NCAR\u2019s resources. While we often distinguish between the allocation opportunities based on the size of the HPC resource needs, most of these options allow you to request data analysis, visualization, and storage resources. To see what HPC resource limits apply to each type of allocation, refer to **the table below* . Small Derecho allocation requests will be accepted after the system enters production later this year. In the meantime, interested researchers can request a Cheyenne allocation now, and CISL will provide guidance later about how to add Derecho to Small and Exploratory projects. Allocation request Initial HPC limit* Supplement HPC limit Frequency Funding eligibility Large No upper limit (subject to availability) No upper limit (subject to availability) Semi-annual panel review NSF award required Small Cheyenne: 400,000 core-hours Derecho: 1 million core-hours Derecho GPU: 2,500 GPU-hours Cheyenne: 400,000 core-hours Derecho: 1 million core-hours Derecho GPU: 2,500 GPU-hours Continuous NSF award required Exploratory & Classroom Cheyenne: 400,000 core-hours Derecho: 500,000 core-hours Derecho GPU: 1,500 GPU-hours Cheyenne: 400,000 core-hours Derecho: 500,000 core-hours Derecho GPU: 1,500 GPU-hours Continuous No external funding award Data Analysis n/a (Casper only) n/a (Casper only) Continuous Any funding source *For Small, Exploratory, and Classroom projects, the amounts shown are the limits for requests on one system. For requests to use more than one system, proportionally smaller limits apply\u2013e.g., up to half the Derecho limit and half the Derecho GPU limit can be requested together. * Submitting Your Request For all types of university allocations, including any subsequent extension or supplement requests, requests should be submitted via the ARC portal\u2019s Allocations section . If you have questions about these options, please contact us via the Research Computing help desk. Large Allocations \u00b6 A university researcher may submit a large request for work that is beyond the scope of the amounts available via Small allocation requests. These requests have no upper limit, aside from the portion of the system available to the university community and ensuring that we can support the breadth of work from eligible university researchers. CISL accepts requests for large allocations of NCAR resources every six months, in March and September. Deadlines for submitting requests are announced approximately two months in advance. The CISL HPC Allocations Panel ( CHAP ) reviews requests in April and October, and projects begin in May and November. Note We strongly recommend that researchers begin any new project by submitting a Small allocation request first. With a Small allocation, you can get started quickly and conduct benchmark and test runs to confirm the estimated computational costs of your planned model configurations. Such preparatory work will maximize your success during CHAP review. The panel recommends awards on the basis of the computational experimental design, computational effectiveness, and availability of computing resources (see Review Criteria ). Check the submission instructions for preparing the required Request Summary document. If your Request Summary is ready, submit your Large Allocation Request here . Large allocations are made for the duration of the supporting NSF award. If the NSF award is extended, including no-cost extensions, you can ask for your allocation end date to be extended as well. At the end of your NSF award, you can request three extra months to wrap things up, and with concurrence from the NSF Program Officer, we can extend your allocation up to 12 months beyond the end of your NSF award. Small Allocations \u00b6 Small Derecho allocation requests will be accepted after the system enters production later this year. In the meantime, interested researchers can request a Cheyenne allocation now, and CISL will provide guidance later about how to add Derecho to Small and Exploratory projects. Small requests can be submitted at any time and decisions are typically made within a few days. U.S. university researchers who are supported by an NSF award can request an initial small allocation of up to 400,000 core-hours on Cheyenne; up to 1 million core-hours on Derecho; and up to 2,500 GPU-hours on Derecho for each NSF award. These allocations can be used to complete small projects or to conduct initial runs in preparation for submitting a request for a large allocation. If needed to complete your work, you can request a one-time supplement to a small allocation \u2013 as much as doubling the total hours available for your project \u2013 with a brief statement that you are on track to finish or that you are aware you will need to prepare a large allocation proposal for any additional resources. You may also request a large allocation at any point after receiving a small allocation. Small allocations are also made for the duration of the supporting NSF award, and they can be extended according to the same rules as for large allocations. Submit a Small Allocation Request here . Exploratory Allocations \u00b6 Exploratory requests can be submitted at any time and decisions are typically made within a few days. Resources for unsponsored graduate students, postdocs, and new faculty A graduate student, post-doctoral researcher, or new faculty member at a U.S. university can request a one-time allocation of up to 400,000 Cheyenne core-hours; 500,000 Derecho core-hours; and 1,500 Derecho GPU-hours. These awards typically support dissertations, help foster early career research, or provide seed resources for pursuing funded research. An individual can request a new exploratory project at each stage of their career path. A new faculty member is any eligible researcher who has not previously had an NCAR allocation as a faculty member. For these allocations, in addition to meeting the domain and affiliation eligibility requirements (below): The work must be the individual's own research project rather than a project assigned by the institution that is hosting the grad student, postdoc, or faculty member. The work should not lie within the scope of any funded grant. A letter from the individual\u2019s advisor or department head must affirm the quality of the proposed research and that the work is not within the scope of a funded grant. If needed to complete your work, you can request a one-time supplement of the allocation, up to twice the original amount, with a brief statement acknowledging that you will finish within the supplemental amount. Exploratory requests are accepted at any time and decisions are typically made within a few days. Exploratory allocations are made for one year, but may be extended up to two additional years to complete the original project. Submit an Exploratory Allocation Request here . Data Analysis Allocations (NEW!) \u00b6 Data Analysis allocation requests can be submitted at any time and decisions are typically made within a few days. Earth system scientists can request access to NCAR\u2019s Casper data analysis cluster and the Campaign Storage file system to allow them to conduct analyses on data sets curated by NCAR data services and accessible via our storage systems. We are pleased to make Data Analysis allocations available to researchers from any eligible institution regardless of the source of funding for the planned analysis. Researchers are only required to identify the specific NCAR-hosted data sets that are essential to completing their science objectives. (Because of this expanded eligibility, Data Analysis projects cannot make use of Cheyenne or Derecho.) Data Analysis allocations are made for one year, but may be extended up to two additional years to complete the original project. Requests for the CMIP Analysis Platform, unless the work requires use of NCAR\u2019s HPC resource, fall within the scope of a Data Analysis project. Submit a Data Analysis Allocation Request here . Classroom Allocations \u00b6 Classroom Allocation Requests can be submitted at any time and decisions are generally made within a few days. CISL offers opportunities to undergraduate and graduate faculty and instructors at U.S. 2- and 4-year institutions to use NCAR HPC and analysis resources in their courses in Earth system science and related areas. \u201cSpecial projects\u201d or honors thesis courses under the guidance of a faculty member are eligible. Classroom allocations can also be used for shorter-term training courses and workshops aimed at the university community with the goal of developing a skilled workforce relevant to all aspects of NCAR\u2019s mission. Individual accounts are provided to all students and the instructor(s), so be sure to allow at least a week for setting up accounts. Very large classes may require more setup time. NCAR can provide consulting assistance to the instructors. Classroom allocations are typically made for the duration of the course, plus a reasonable limited period afterwards. Submit a Classroom Allocation Request here . Eligibility Details \u00b6 NCAR\u2019s HPC environment represents a significant resource for the U.S. university community. Access to the environment is governed by three primary eligibility criteria. 1. Earth system science and related research \u00b6 To use NCAR resources, a project must be within Earth system science or be Earth system\u2013related research. According to a recent report from the National Academies, \u201cEarth system science\u201d aims to discover and integrate knowledge on the structure, nature, and scales of the dynamics and interactions among natural and social processes throughout the Earth system, which includes the atmosphere, hydrosphere, geosphere, cryosphere, biosphere, as well as the individuals, institutions, and technologies that respond to and influence these dynamics and their interactions and feedback through time. Because scientific progress often relies on contributions from many fields, NCAR resources are also available to support Earth system\u2013related work from other domains \u2013 that is, work that has a demonstrable benefit to or reliance on Earth system science. 2. Affiliation \u00b6 A prime component of NCAR\u2019s mission is to support atmospheric science at U.S. post-secondary educational institutions. Eligible institutions encompass 2- and 4-year colleges and universities, including community colleges, minority serving institutions (MSIs), and predominantly undergraduate-serving institutions, as well as non-profit research organizations. Recipients of NSF research grants in eligible domains from other types of institutions are also deemed eligible. NCAR resources normally do not support research groups in federal agencies. 3. Sponsorship \u00b6 NSF grants. Researchers can apply for an NCAR allocation in support of an associated NSF grant for Earth system science or related research as long as their proposed computing supports the objectives of the grant. The NSF is kept informed to ensure appropriate use of NCAR resources. Unsponsored projects. NCAR provides opportunities for graduate students, postdocs, and new faculty at eligible institutions for work not within the scope of a funded research grant. For this purpose, \u201cnew faculty\u201d includes any faculty member who has not previously had a university allocation at NCAR. The work must be the individual's own research project rather than a project assigned by the institution that is hosting the grad student, postdoc, or faculty member. Their work should not lie within the scope of any funded grant. They must provide a letter from their advisor or department head commenting on the quality of the proposed research and affirming that funds are not within the scope of a funded grant. Unsponsored researchers may also request Data Analysis allocations. These projects cannot use NCAR HPC resources and must make use of NCAR-hosted data sets. Non-NSF funding Due to high demand for NCAR resources at this time, we are unable to provide HPC support to Earth system scientists who have funding solely from non-NSF sources. However, researchers who want to analyze NCAR data sets can request use of NCAR analysis resources regardless of their funding source. The University of Wyoming allocation opportunity has eligibility criteria that permit funding by sources other than NSF, for projects involving U Wyoming collaborators.","title":"University allocations"},{"location":"allocations/university-allocations/#university-allocations","text":"The next university deadline for submitting Large Allocation Requests is September 12, 2023. University use of the NCAR HPC environment is intended to support Earth system science and related research by researchers at U.S. institutions. The emphasis is on projects that may be beyond the scope of a researcher\u2019s local university computing capabilities. Eligible researchers and activities incur no costs for the use of NCAR resources. Recent Changes We have recently updated these policies, both to prepare for Derecho and to expand opportunities for University researchers. Most notably, we have created a new Data Analysis opportunity to allow more researchers to analyze NCAR-hosted data sets, and we have redefined \u201cnew faculty\u201d to be any faculty member who has not computed at NCAR before. We have also called out options for expanding or extending the smaller-scale projects and clarified the eligibility language to emphasize the range of post-secondary institutions welcome to use NCAR\u2019s resources. While we often distinguish between the allocation opportunities based on the size of the HPC resource needs, most of these options allow you to request data analysis, visualization, and storage resources. To see what HPC resource limits apply to each type of allocation, refer to **the table below* . Small Derecho allocation requests will be accepted after the system enters production later this year. In the meantime, interested researchers can request a Cheyenne allocation now, and CISL will provide guidance later about how to add Derecho to Small and Exploratory projects. Allocation request Initial HPC limit* Supplement HPC limit Frequency Funding eligibility Large No upper limit (subject to availability) No upper limit (subject to availability) Semi-annual panel review NSF award required Small Cheyenne: 400,000 core-hours Derecho: 1 million core-hours Derecho GPU: 2,500 GPU-hours Cheyenne: 400,000 core-hours Derecho: 1 million core-hours Derecho GPU: 2,500 GPU-hours Continuous NSF award required Exploratory & Classroom Cheyenne: 400,000 core-hours Derecho: 500,000 core-hours Derecho GPU: 1,500 GPU-hours Cheyenne: 400,000 core-hours Derecho: 500,000 core-hours Derecho GPU: 1,500 GPU-hours Continuous No external funding award Data Analysis n/a (Casper only) n/a (Casper only) Continuous Any funding source *For Small, Exploratory, and Classroom projects, the amounts shown are the limits for requests on one system. For requests to use more than one system, proportionally smaller limits apply\u2013e.g., up to half the Derecho limit and half the Derecho GPU limit can be requested together. * Submitting Your Request For all types of university allocations, including any subsequent extension or supplement requests, requests should be submitted via the ARC portal\u2019s Allocations section . If you have questions about these options, please contact us via the Research Computing help desk.","title":"University allocations"},{"location":"allocations/university-allocations/#large-allocations","text":"A university researcher may submit a large request for work that is beyond the scope of the amounts available via Small allocation requests. These requests have no upper limit, aside from the portion of the system available to the university community and ensuring that we can support the breadth of work from eligible university researchers. CISL accepts requests for large allocations of NCAR resources every six months, in March and September. Deadlines for submitting requests are announced approximately two months in advance. The CISL HPC Allocations Panel ( CHAP ) reviews requests in April and October, and projects begin in May and November. Note We strongly recommend that researchers begin any new project by submitting a Small allocation request first. With a Small allocation, you can get started quickly and conduct benchmark and test runs to confirm the estimated computational costs of your planned model configurations. Such preparatory work will maximize your success during CHAP review. The panel recommends awards on the basis of the computational experimental design, computational effectiveness, and availability of computing resources (see Review Criteria ). Check the submission instructions for preparing the required Request Summary document. If your Request Summary is ready, submit your Large Allocation Request here . Large allocations are made for the duration of the supporting NSF award. If the NSF award is extended, including no-cost extensions, you can ask for your allocation end date to be extended as well. At the end of your NSF award, you can request three extra months to wrap things up, and with concurrence from the NSF Program Officer, we can extend your allocation up to 12 months beyond the end of your NSF award.","title":"Large Allocations"},{"location":"allocations/university-allocations/#small-allocations","text":"Small Derecho allocation requests will be accepted after the system enters production later this year. In the meantime, interested researchers can request a Cheyenne allocation now, and CISL will provide guidance later about how to add Derecho to Small and Exploratory projects. Small requests can be submitted at any time and decisions are typically made within a few days. U.S. university researchers who are supported by an NSF award can request an initial small allocation of up to 400,000 core-hours on Cheyenne; up to 1 million core-hours on Derecho; and up to 2,500 GPU-hours on Derecho for each NSF award. These allocations can be used to complete small projects or to conduct initial runs in preparation for submitting a request for a large allocation. If needed to complete your work, you can request a one-time supplement to a small allocation \u2013 as much as doubling the total hours available for your project \u2013 with a brief statement that you are on track to finish or that you are aware you will need to prepare a large allocation proposal for any additional resources. You may also request a large allocation at any point after receiving a small allocation. Small allocations are also made for the duration of the supporting NSF award, and they can be extended according to the same rules as for large allocations. Submit a Small Allocation Request here .","title":"Small Allocations"},{"location":"allocations/university-allocations/#exploratory-allocations","text":"Exploratory requests can be submitted at any time and decisions are typically made within a few days. Resources for unsponsored graduate students, postdocs, and new faculty A graduate student, post-doctoral researcher, or new faculty member at a U.S. university can request a one-time allocation of up to 400,000 Cheyenne core-hours; 500,000 Derecho core-hours; and 1,500 Derecho GPU-hours. These awards typically support dissertations, help foster early career research, or provide seed resources for pursuing funded research. An individual can request a new exploratory project at each stage of their career path. A new faculty member is any eligible researcher who has not previously had an NCAR allocation as a faculty member. For these allocations, in addition to meeting the domain and affiliation eligibility requirements (below): The work must be the individual's own research project rather than a project assigned by the institution that is hosting the grad student, postdoc, or faculty member. The work should not lie within the scope of any funded grant. A letter from the individual\u2019s advisor or department head must affirm the quality of the proposed research and that the work is not within the scope of a funded grant. If needed to complete your work, you can request a one-time supplement of the allocation, up to twice the original amount, with a brief statement acknowledging that you will finish within the supplemental amount. Exploratory requests are accepted at any time and decisions are typically made within a few days. Exploratory allocations are made for one year, but may be extended up to two additional years to complete the original project. Submit an Exploratory Allocation Request here .","title":"Exploratory Allocations"},{"location":"allocations/university-allocations/#data-analysis-allocations-new","text":"Data Analysis allocation requests can be submitted at any time and decisions are typically made within a few days. Earth system scientists can request access to NCAR\u2019s Casper data analysis cluster and the Campaign Storage file system to allow them to conduct analyses on data sets curated by NCAR data services and accessible via our storage systems. We are pleased to make Data Analysis allocations available to researchers from any eligible institution regardless of the source of funding for the planned analysis. Researchers are only required to identify the specific NCAR-hosted data sets that are essential to completing their science objectives. (Because of this expanded eligibility, Data Analysis projects cannot make use of Cheyenne or Derecho.) Data Analysis allocations are made for one year, but may be extended up to two additional years to complete the original project. Requests for the CMIP Analysis Platform, unless the work requires use of NCAR\u2019s HPC resource, fall within the scope of a Data Analysis project. Submit a Data Analysis Allocation Request here .","title":"Data Analysis Allocations (NEW!)"},{"location":"allocations/university-allocations/#classroom-allocations","text":"Classroom Allocation Requests can be submitted at any time and decisions are generally made within a few days. CISL offers opportunities to undergraduate and graduate faculty and instructors at U.S. 2- and 4-year institutions to use NCAR HPC and analysis resources in their courses in Earth system science and related areas. \u201cSpecial projects\u201d or honors thesis courses under the guidance of a faculty member are eligible. Classroom allocations can also be used for shorter-term training courses and workshops aimed at the university community with the goal of developing a skilled workforce relevant to all aspects of NCAR\u2019s mission. Individual accounts are provided to all students and the instructor(s), so be sure to allow at least a week for setting up accounts. Very large classes may require more setup time. NCAR can provide consulting assistance to the instructors. Classroom allocations are typically made for the duration of the course, plus a reasonable limited period afterwards. Submit a Classroom Allocation Request here .","title":"Classroom Allocations"},{"location":"allocations/university-allocations/#eligibility-details","text":"NCAR\u2019s HPC environment represents a significant resource for the U.S. university community. Access to the environment is governed by three primary eligibility criteria.","title":"Eligibility Details"},{"location":"allocations/university-allocations/#1-earth-system-science-and-related-research","text":"To use NCAR resources, a project must be within Earth system science or be Earth system\u2013related research. According to a recent report from the National Academies, \u201cEarth system science\u201d aims to discover and integrate knowledge on the structure, nature, and scales of the dynamics and interactions among natural and social processes throughout the Earth system, which includes the atmosphere, hydrosphere, geosphere, cryosphere, biosphere, as well as the individuals, institutions, and technologies that respond to and influence these dynamics and their interactions and feedback through time. Because scientific progress often relies on contributions from many fields, NCAR resources are also available to support Earth system\u2013related work from other domains \u2013 that is, work that has a demonstrable benefit to or reliance on Earth system science.","title":"1. Earth system science and related research"},{"location":"allocations/university-allocations/#2-affiliation","text":"A prime component of NCAR\u2019s mission is to support atmospheric science at U.S. post-secondary educational institutions. Eligible institutions encompass 2- and 4-year colleges and universities, including community colleges, minority serving institutions (MSIs), and predominantly undergraduate-serving institutions, as well as non-profit research organizations. Recipients of NSF research grants in eligible domains from other types of institutions are also deemed eligible. NCAR resources normally do not support research groups in federal agencies.","title":"2. Affiliation"},{"location":"allocations/university-allocations/#3-sponsorship","text":"NSF grants. Researchers can apply for an NCAR allocation in support of an associated NSF grant for Earth system science or related research as long as their proposed computing supports the objectives of the grant. The NSF is kept informed to ensure appropriate use of NCAR resources. Unsponsored projects. NCAR provides opportunities for graduate students, postdocs, and new faculty at eligible institutions for work not within the scope of a funded research grant. For this purpose, \u201cnew faculty\u201d includes any faculty member who has not previously had a university allocation at NCAR. The work must be the individual's own research project rather than a project assigned by the institution that is hosting the grad student, postdoc, or faculty member. Their work should not lie within the scope of any funded grant. They must provide a letter from their advisor or department head commenting on the quality of the proposed research and affirming that funds are not within the scope of a funded grant. Unsponsored researchers may also request Data Analysis allocations. These projects cannot use NCAR HPC resources and must make use of NCAR-hosted data sets. Non-NSF funding Due to high demand for NCAR resources at this time, we are unable to provide HPC support to Earth system scientists who have funding solely from non-NSF sources. However, researchers who want to analyze NCAR data sets can request use of NCAR analysis resources regardless of their funding source. The University of Wyoming allocation opportunity has eligibility criteria that permit funding by sources other than NSF, for projects involving U Wyoming collaborators.","title":"3. Sponsorship"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/","text":"University Large Allocation Request Preparation Instructions \u00b6 The next university deadline for submitting Large Allocation Requests is March 13, 2023. Note: In addition to Large Allocation Requests, CISL offers opportunities for NSF awardees, graduate students, and postdocs to request smaller allocations. See the University allocations page for eligibility requirements and other information. Submitting a Large Allocation Request \u00b6 Requesters must prepare and submit a Request Summary document and attach it to the Large Allocation Request form as a PDF file that is no larger than 5 MB. The instructions below explain how to prepare the document. When your document is ready, submit your request via the Large Allocation Request form. Request Summary document \u00b6 The Request Summary document must provide a self-contained description of your project and allocation request. The document may be no more than five (5) pages for Sections A\u2013D below; Sections E\u2013G should be included in the same uploaded document and are permitted an additional five (5) pages. The five-page limit is mandatory for all requests, and it is strongly recommended that you follow the template below to help the panel locate required information in your request. The CHAP Allocation Review Criteria provide further detail on considerations the review panel uses to identify meritorious requests. Recent successful requests are available: Click to download Proposal Sample 1 and Proposal Sample 2 . Due to the rapidly growing scale of data generated by many university projects and constraints on the available storage within the CISL environment, the CHAP is scrutinizing user requests for storage resources much more closely. As with poorly justified requests for computing time, poorly justified requests for storage resources will result in reduced storage allocation awards. Review the guidance below for Section D and ensure that your submission addresses the relevant points supporting your efficient, effective, and appropriate use of CISL storage resources. Keep in mind that CISL storage resources are typically available to university projects only for the lifetime of the allocated project and associated NSF award; see Campaign Storage . A. Project information Title of project Title of NSF award (if different from project title) supporting the computational experiments. Important: The NSF award must explicitly support the computational experiments being proposed. NSF award number. Due to high demand for resources, the CHAP no longer reviews requests for pending awards. Submit your request only after the NSF has awarded a grant for the research. Name of Project Lead and his or her institution Submission date B. Project overview The overview of the project should typically be less than half a page and include: A summary of the science question and computational plan. The relationship of the proposed work to atmospheric and closely related sciences. The explicit linkage between the NSF award and the computational experiments being proposed. This is especially important if the published NSF award abstract does not clearly describe the computational component of the work funded. C. Science objectives The science objectives should be described briefly . This section should give sufficient information for understanding the computational plan in section D; it is not necessary to justify the science objectives as they must have already passed NSF review. !!! note \"Advice for preparing your request. \"Brief\" is the operative word for your science description. The panel is not judging your science, only trying to understand how and if your computational experiments (described in Section D) will help you find answers to your science questions. This section should be between 1/2 and 1 page long. D. Computational experiments and resource requirements The bulk of the Request Summary should focus on Section D. Discuss your planned computational experiments and the resources needed to conduct the work in this section. Please cover these topics and follow the advice for how to best address each topic. Computational experiments \u00b6 Numerical approach. Briefly describe the numerical approach(es) or model(s). If a standard community model is being used, simply explain why it is appropriate for the scientific objectives and include a reference to a published description of the model or method. If a community model is being modified, include a description of the modification sufficient to explain any changes in the computational cost of the model and explain why modifications are necessary for the scientific objectives. For a non-standard or non-community model, the numerical description should briefly describe the approximations and other methods proposed to obtain valid solutions to the problem. Computational experiments. Describe the computational experiments needed to address the scientific objectives. Clearly indicate the number and type of experiments and how they relate to the objectives. Be sure to justify the model configuration choices that are relevant to the experiment, such as domain, grid size, time steps, simulated time span, ensemble size, and parameter choices. References on the selection of the ensemble size are strongly encouraged. Without an adequate justification of the model configuration, the panel may reduce or deny your computing request. Code performance. Include documentation on program code performance (for example, timings, performance monitoring tools). You may refer to a web page detailing code performance. Describe how flexible your code is in the number of processors it can use and why you may choose a particular number. The CHAP will evaluate the likelihood that a request can scale up its production runs based on this information. Information on the portability of the code to other platforms may also be useful to the CHAP; requesters are strongly encouraged to provide this information about the code and the team\u2019s knowledge/use of HPC systems similar to Cheyenne. Output and analysis. Describe the key variables to be output, plans for analysis, and any steps taken to use storage resources efficiently and effectively. Projects with large-scale data output will be scrutinized in greater detail. In addition, the section should include a brief description of the post-processing calculations to be carried out and what output needs to be retained beyond the post-processing stage. Resource requirements \u00b6 Provide a table summarizing the resources required for each experimental configuration and the complete set of computational experiments. This must include the number of core-hours needed and the data output volume. The table should be accompanied by a narrative that elaborates on how you arrived at the numbers in the table and describes how you will use appropriate storage options or data analysis and visualization resources as detailed below. With the petascale systems\u2019 ability to generate vast amounts of data, CISL allocation requests require users to consider their data workflows and to justify their storage resource use. Requesters should note that the CHAP Allocation Review Criteria also apply to requests for allocated storage resources \u2013 that is, GLADE project spaces and Campaign Storage archive use. The CHAP reviewers do not expect lengthy justifications; in most cases, a paragraph that demonstrates forethought commensurate with the scale of anticipated need will suffice. For requests over 50 TB, longer justifications become appropriate. In addressing the CHAP review criteria with respect to storage resources, justify the project\u2019s rationale for which data will be stored, the project\u2019s work plan for managing data, and the project\u2019s need to retain the data to be stored long term. While some statement of storage resource needs is expected in Section D, you may choose to provide details related to longer-term storage needs in Section E (Data Management Plan) to keep sections A-D within the five-page limit. While use of scratch disk space need not be formally justified in the project's allocation request, demonstrating effective use of scratch space within the overall work plan can help reviewers understand your needs for project space or Campaign Storage space. 1. HPC. The table should give the core-hours per simulated year or appropriate time period and the total core-hours needed for each experimental configuration as well as the total core-hours for the request. Describe how you arrived at the number of core-hours for each proposed computational experiment. If not provided elsewhere, details on how HPC resource requirements are estimated MUST be included to help reviewers evaluate whether the resources sought are justified and will be used efficiently. Estimating Derecho resource needs. Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77. When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). Derecho GPU-hour estimates can be based on any reasonable GPU performance estimate from another system, including Casper. Estimating Cheyenne core-hours. Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs is to perform benchmark runs. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. 2. Campaign Storage. In the table showing core-hours, include a column for the final, post-processed, post-analysis amount of data intended for Campaign Storage for each experimental configuration and include the total terabytes planned. Any plans to store raw, unprocessed or temporary data should be justified carefully. Projects with more than 50 TB planned for Campaign Storage are expected to provide more detailed justifications. Justifying archive space requests. A successful justification for archive space would describe, for example, that the data have a meaningful purpose beyond the post-processing phase; that variables or time steps not needed for planned analyses will be filtered out; and that HPC and analysis stages are interleaved where practical to eliminate the need for short-term use of archive space. A simple summation of all bytes generated by all proposed HPC runs may set an upper limit on archive space needs but will not automatically constitute a sufficient justification in and of itself . In most cases, it is not necessary to archive all output; only the subset of data that is critical to the science. Where appropriate, the justification should also describe how the project will reduce archival holdings in subsequent years (for example, a project may have a larger need during peak activity that will decrease in out-years). As with computational justifications, the detail for storage justifications should grow commensurately with the project\u2019s anticipated need. 3. Data analysis and visualization (DAV). Describe any planned need for CISL\u2019s DAV clusters to analyze or visualize your results. For standard interactive access to these clusters, we will provide up to 10,000 core-hours with no special justification. Projects with more extensive plans for use of the clusters should justify their needs in a manner similar to their HPC requests based on benchmarks and cost estimates from sample runs. Multi-year plan. While the CHAP makes an effort to award each project its full resource need, very large requests should consider providing a breakdown showing the projected use of core-hours and, if applicable, post-processed data during each year of the allocation. Tie this to the planned computational experiments completed or partially completed each year. Special requirements. Please specify any resource requirements that fall outside of the default environment limits, such as the need for longer job runtime limits, that may affect your ability to complete the proposed computational experiments. Additional supporting information \u00b6 Sections E through G together must be no more than an additional five pages. E. Data management plan. Consistent with NSF\u2019s requirement that all proposals include a data management plan, summarize your plan for managing the data resulting from this computational work throughout and beyond the period of performance for the NSF award. This section can be used to provide additional details or justification for the storage resource needs, to describe plans for sharing the project\u2019s data, and to summarize any anticipated long-term storage needs. A well-justified data management plan is critical because of the potential for large-scale projects to produce extensive data output. F. Accomplishment report. The accomplishment report should encompass computations performed using CISL resources by the PI or Project Lead. Clearly distinguish accomplishments on this CISL project (that is, for prior CISL use associated with the same NSF award) and accomplishments from all past use of CISL resources. Related work performed on CISL resources by other members of a larger research group may be described, if relevant to this request. Briefly describe the calculations and scientific accomplishments that were completed. Include publications submitted or published that resulted from use of CISL resources. List graduate students who used these computational resources and indicate if these resources supported their thesis or dissertation research. If so, please include the thesis or dissertation title(s). G. References. Please limit to those directly related to the proposed project and referenced in your Request Summary document. H. Figures and captions. Optional . Figures may be embedded within the main body of the Request Summary; embedded figures count against the five-page limit. Figures and charts at the end of the Request Summary will not count against the five-page limit.","title":"University Large Allocation Request Preparation Instructions"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#university-large-allocation-request-preparation-instructions","text":"The next university deadline for submitting Large Allocation Requests is March 13, 2023. Note: In addition to Large Allocation Requests, CISL offers opportunities for NSF awardees, graduate students, and postdocs to request smaller allocations. See the University allocations page for eligibility requirements and other information.","title":"University Large Allocation Request Preparation Instructions"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#submitting-a-large-allocation-request","text":"Requesters must prepare and submit a Request Summary document and attach it to the Large Allocation Request form as a PDF file that is no larger than 5 MB. The instructions below explain how to prepare the document. When your document is ready, submit your request via the Large Allocation Request form.","title":"Submitting a Large Allocation Request"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#request-summary-document","text":"The Request Summary document must provide a self-contained description of your project and allocation request. The document may be no more than five (5) pages for Sections A\u2013D below; Sections E\u2013G should be included in the same uploaded document and are permitted an additional five (5) pages. The five-page limit is mandatory for all requests, and it is strongly recommended that you follow the template below to help the panel locate required information in your request. The CHAP Allocation Review Criteria provide further detail on considerations the review panel uses to identify meritorious requests. Recent successful requests are available: Click to download Proposal Sample 1 and Proposal Sample 2 . Due to the rapidly growing scale of data generated by many university projects and constraints on the available storage within the CISL environment, the CHAP is scrutinizing user requests for storage resources much more closely. As with poorly justified requests for computing time, poorly justified requests for storage resources will result in reduced storage allocation awards. Review the guidance below for Section D and ensure that your submission addresses the relevant points supporting your efficient, effective, and appropriate use of CISL storage resources. Keep in mind that CISL storage resources are typically available to university projects only for the lifetime of the allocated project and associated NSF award; see Campaign Storage . A. Project information Title of project Title of NSF award (if different from project title) supporting the computational experiments. Important: The NSF award must explicitly support the computational experiments being proposed. NSF award number. Due to high demand for resources, the CHAP no longer reviews requests for pending awards. Submit your request only after the NSF has awarded a grant for the research. Name of Project Lead and his or her institution Submission date B. Project overview The overview of the project should typically be less than half a page and include: A summary of the science question and computational plan. The relationship of the proposed work to atmospheric and closely related sciences. The explicit linkage between the NSF award and the computational experiments being proposed. This is especially important if the published NSF award abstract does not clearly describe the computational component of the work funded. C. Science objectives The science objectives should be described briefly . This section should give sufficient information for understanding the computational plan in section D; it is not necessary to justify the science objectives as they must have already passed NSF review. !!! note \"Advice for preparing your request. \"Brief\" is the operative word for your science description. The panel is not judging your science, only trying to understand how and if your computational experiments (described in Section D) will help you find answers to your science questions. This section should be between 1/2 and 1 page long. D. Computational experiments and resource requirements The bulk of the Request Summary should focus on Section D. Discuss your planned computational experiments and the resources needed to conduct the work in this section. Please cover these topics and follow the advice for how to best address each topic.","title":"Request Summary document"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#computational-experiments","text":"Numerical approach. Briefly describe the numerical approach(es) or model(s). If a standard community model is being used, simply explain why it is appropriate for the scientific objectives and include a reference to a published description of the model or method. If a community model is being modified, include a description of the modification sufficient to explain any changes in the computational cost of the model and explain why modifications are necessary for the scientific objectives. For a non-standard or non-community model, the numerical description should briefly describe the approximations and other methods proposed to obtain valid solutions to the problem. Computational experiments. Describe the computational experiments needed to address the scientific objectives. Clearly indicate the number and type of experiments and how they relate to the objectives. Be sure to justify the model configuration choices that are relevant to the experiment, such as domain, grid size, time steps, simulated time span, ensemble size, and parameter choices. References on the selection of the ensemble size are strongly encouraged. Without an adequate justification of the model configuration, the panel may reduce or deny your computing request. Code performance. Include documentation on program code performance (for example, timings, performance monitoring tools). You may refer to a web page detailing code performance. Describe how flexible your code is in the number of processors it can use and why you may choose a particular number. The CHAP will evaluate the likelihood that a request can scale up its production runs based on this information. Information on the portability of the code to other platforms may also be useful to the CHAP; requesters are strongly encouraged to provide this information about the code and the team\u2019s knowledge/use of HPC systems similar to Cheyenne. Output and analysis. Describe the key variables to be output, plans for analysis, and any steps taken to use storage resources efficiently and effectively. Projects with large-scale data output will be scrutinized in greater detail. In addition, the section should include a brief description of the post-processing calculations to be carried out and what output needs to be retained beyond the post-processing stage.","title":"Computational experiments"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#resource-requirements","text":"Provide a table summarizing the resources required for each experimental configuration and the complete set of computational experiments. This must include the number of core-hours needed and the data output volume. The table should be accompanied by a narrative that elaborates on how you arrived at the numbers in the table and describes how you will use appropriate storage options or data analysis and visualization resources as detailed below. With the petascale systems\u2019 ability to generate vast amounts of data, CISL allocation requests require users to consider their data workflows and to justify their storage resource use. Requesters should note that the CHAP Allocation Review Criteria also apply to requests for allocated storage resources \u2013 that is, GLADE project spaces and Campaign Storage archive use. The CHAP reviewers do not expect lengthy justifications; in most cases, a paragraph that demonstrates forethought commensurate with the scale of anticipated need will suffice. For requests over 50 TB, longer justifications become appropriate. In addressing the CHAP review criteria with respect to storage resources, justify the project\u2019s rationale for which data will be stored, the project\u2019s work plan for managing data, and the project\u2019s need to retain the data to be stored long term. While some statement of storage resource needs is expected in Section D, you may choose to provide details related to longer-term storage needs in Section E (Data Management Plan) to keep sections A-D within the five-page limit. While use of scratch disk space need not be formally justified in the project's allocation request, demonstrating effective use of scratch space within the overall work plan can help reviewers understand your needs for project space or Campaign Storage space. 1. HPC. The table should give the core-hours per simulated year or appropriate time period and the total core-hours needed for each experimental configuration as well as the total core-hours for the request. Describe how you arrived at the number of core-hours for each proposed computational experiment. If not provided elsewhere, details on how HPC resource requirements are estimated MUST be included to help reviewers evaluate whether the resources sought are justified and will be used efficiently. Estimating Derecho resource needs. Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77. When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). Derecho GPU-hour estimates can be based on any reasonable GPU performance estimate from another system, including Casper. Estimating Cheyenne core-hours. Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs is to perform benchmark runs. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. 2. Campaign Storage. In the table showing core-hours, include a column for the final, post-processed, post-analysis amount of data intended for Campaign Storage for each experimental configuration and include the total terabytes planned. Any plans to store raw, unprocessed or temporary data should be justified carefully. Projects with more than 50 TB planned for Campaign Storage are expected to provide more detailed justifications. Justifying archive space requests. A successful justification for archive space would describe, for example, that the data have a meaningful purpose beyond the post-processing phase; that variables or time steps not needed for planned analyses will be filtered out; and that HPC and analysis stages are interleaved where practical to eliminate the need for short-term use of archive space. A simple summation of all bytes generated by all proposed HPC runs may set an upper limit on archive space needs but will not automatically constitute a sufficient justification in and of itself . In most cases, it is not necessary to archive all output; only the subset of data that is critical to the science. Where appropriate, the justification should also describe how the project will reduce archival holdings in subsequent years (for example, a project may have a larger need during peak activity that will decrease in out-years). As with computational justifications, the detail for storage justifications should grow commensurately with the project\u2019s anticipated need. 3. Data analysis and visualization (DAV). Describe any planned need for CISL\u2019s DAV clusters to analyze or visualize your results. For standard interactive access to these clusters, we will provide up to 10,000 core-hours with no special justification. Projects with more extensive plans for use of the clusters should justify their needs in a manner similar to their HPC requests based on benchmarks and cost estimates from sample runs. Multi-year plan. While the CHAP makes an effort to award each project its full resource need, very large requests should consider providing a breakdown showing the projected use of core-hours and, if applicable, post-processed data during each year of the allocation. Tie this to the planned computational experiments completed or partially completed each year. Special requirements. Please specify any resource requirements that fall outside of the default environment limits, such as the need for longer job runtime limits, that may affect your ability to complete the proposed computational experiments.","title":"Resource requirements"},{"location":"allocations/university-allocations/university-large-allocation-request-preparation-instructions/#additional-supporting-information","text":"Sections E through G together must be no more than an additional five pages. E. Data management plan. Consistent with NSF\u2019s requirement that all proposals include a data management plan, summarize your plan for managing the data resulting from this computational work throughout and beyond the period of performance for the NSF award. This section can be used to provide additional details or justification for the storage resource needs, to describe plans for sharing the project\u2019s data, and to summarize any anticipated long-term storage needs. A well-justified data management plan is critical because of the potential for large-scale projects to produce extensive data output. F. Accomplishment report. The accomplishment report should encompass computations performed using CISL resources by the PI or Project Lead. Clearly distinguish accomplishments on this CISL project (that is, for prior CISL use associated with the same NSF award) and accomplishments from all past use of CISL resources. Related work performed on CISL resources by other members of a larger research group may be described, if relevant to this request. Briefly describe the calculations and scientific accomplishments that were completed. Include publications submitted or published that resulted from use of CISL resources. List graduate students who used these computational resources and indicate if these resources supported their thesis or dissertation research. If so, please include the thesis or dissertation title(s). G. References. Please limit to those directly related to the proposed project and referenced in your Request Summary document. H. Figures and captions. Optional . Figures may be embedded within the main body of the Request Summary; embedded figures count against the five-page limit. Figures and charts at the end of the Request Summary will not count against the five-page limit.","title":"Additional supporting information"},{"location":"compute-systems/jupyter-and-ipython/","text":"Jupyter and IPython \u00b6 Consider NCAR's JupyterHub instance first!! This page describes an older, manual approach for launching Jupyter on NCAR resources, prior to the deployment of our hosted JupyterHub instance . Still. this approach is valid and may be useful in circumstances where the hosted JupyterHub is under maintenane. The Jupyter package is designed to facilitate interactive computing, especially for code editing, mathematical expressions, plots, code/data visualization, and parallel computing. The IPython kernel is included in the package. Jupyter supports many alternative kernels, also known as language interpreters. See details below. The instructions below describe how to start the browser-based JupyterLab , the IPython shell , and Jupyter QtConsole on the NCAR systems. visualization nodes. For additional information, see Jupyter documentation . Starting JupyterLab \u00b6 Start an interactive job using the qinteractive command. (Alternative: Start the job on Casper using execcasper if it requires GPUs or more memory than is available on Cheyenne.) Load the ncarenv and python modules. Run the ncar_pylib command to load the NCAR Python Library virtual environment. Run the start-jupyter command. The output includes instructions like those shown in the image just below. Including the -N after the ssh command establishes the tunneling that allows you to use JupyterLab in your local browser. It also prevents you from actually opening a second ssh session. If you prefer to have a second session open, omit the -N . When you close that session, you will be closing your browser connection to JupyterLab. On your local computer, run the ssh command as instructed. The session will appear to hang after you log in. At that point, start http://localhost:nnnn in your browser. (The port numbers may be different from those in the output example above.) JupyterLab will request a password or \"token,\" which is a long string as shown in the output above that you can copy and paste from your terminal. Your browser will open the JupyterLab web interface after you log in with the token. Related links \u00b6 Notebook: Extract NECOFS water levels using NetCDF4-Python and analyze/visualize with Pandas Notebook: Access data from the NECOFS (New England Coastal Ocean Forecast System) via OPeNDAP Starting IPython shell \u00b6 Start an interactive job using the qinteractive command. (Alternative: Start the job on Casper using execcasper if it requires GPUs or more memory than is available on Cheyenne.) Load the ncarenv and python modules. Run the ncar_pylib command to load the NCAR Python Library virtual environment. Run the ipython command to start the shell. Starting Jupyter QtConsole \u00b6 Log in with X tunneling (using the ssh -X option). Start an interactive job using the qinteractive command. (Alternative: Start the job on Casper using execcasper if it requires GPUs or more memory than is available on Cheyenne.) Load the ncarenv and python modules. Run the ncar_pylib command to load the NCAR Python Library virtual environment. Run the jupyter qtconsole command to start the console. Using alternative language kernels \u00b6 Jupyter supports multiple language interpreters (known as \"kernels\"). The Python interpreter is loaded by default as the language kernel when using Jupyter, but you can specify use of another kernel when invoking a particular command. To see a list of installed language kernels, run this command: jupyter kernelspec list To use a kernel, specify it by name when invoking a command. For example, to use the R 3.4.0 interpreter on Cheyenne in the Jupyter QtConsole, enter the following: jupyter qtconsole --kernel = r-3.4 The console will load with the R command line interpreter active. If you need a language kernel that has not been installed, you can install it yourself in your local directory or contact the NCAR Research Computing help desk to have it added in the system library.","title":"Jupyter and IPython"},{"location":"compute-systems/jupyter-and-ipython/#jupyter-and-ipython","text":"Consider NCAR's JupyterHub instance first!! This page describes an older, manual approach for launching Jupyter on NCAR resources, prior to the deployment of our hosted JupyterHub instance . Still. this approach is valid and may be useful in circumstances where the hosted JupyterHub is under maintenane. The Jupyter package is designed to facilitate interactive computing, especially for code editing, mathematical expressions, plots, code/data visualization, and parallel computing. The IPython kernel is included in the package. Jupyter supports many alternative kernels, also known as language interpreters. See details below. The instructions below describe how to start the browser-based JupyterLab , the IPython shell , and Jupyter QtConsole on the NCAR systems. visualization nodes. For additional information, see Jupyter documentation .","title":"Jupyter and IPython"},{"location":"compute-systems/jupyter-and-ipython/#starting-jupyterlab","text":"Start an interactive job using the qinteractive command. (Alternative: Start the job on Casper using execcasper if it requires GPUs or more memory than is available on Cheyenne.) Load the ncarenv and python modules. Run the ncar_pylib command to load the NCAR Python Library virtual environment. Run the start-jupyter command. The output includes instructions like those shown in the image just below. Including the -N after the ssh command establishes the tunneling that allows you to use JupyterLab in your local browser. It also prevents you from actually opening a second ssh session. If you prefer to have a second session open, omit the -N . When you close that session, you will be closing your browser connection to JupyterLab. On your local computer, run the ssh command as instructed. The session will appear to hang after you log in. At that point, start http://localhost:nnnn in your browser. (The port numbers may be different from those in the output example above.) JupyterLab will request a password or \"token,\" which is a long string as shown in the output above that you can copy and paste from your terminal. Your browser will open the JupyterLab web interface after you log in with the token.","title":"Starting JupyterLab"},{"location":"compute-systems/jupyter-and-ipython/#related-links","text":"Notebook: Extract NECOFS water levels using NetCDF4-Python and analyze/visualize with Pandas Notebook: Access data from the NECOFS (New England Coastal Ocean Forecast System) via OPeNDAP","title":"Related links"},{"location":"compute-systems/jupyter-and-ipython/#starting-ipython-shell","text":"Start an interactive job using the qinteractive command. (Alternative: Start the job on Casper using execcasper if it requires GPUs or more memory than is available on Cheyenne.) Load the ncarenv and python modules. Run the ncar_pylib command to load the NCAR Python Library virtual environment. Run the ipython command to start the shell.","title":"Starting IPython shell"},{"location":"compute-systems/jupyter-and-ipython/#starting-jupyter-qtconsole","text":"Log in with X tunneling (using the ssh -X option). Start an interactive job using the qinteractive command. (Alternative: Start the job on Casper using execcasper if it requires GPUs or more memory than is available on Cheyenne.) Load the ncarenv and python modules. Run the ncar_pylib command to load the NCAR Python Library virtual environment. Run the jupyter qtconsole command to start the console.","title":"Starting Jupyter QtConsole"},{"location":"compute-systems/jupyter-and-ipython/#using-alternative-language-kernels","text":"Jupyter supports multiple language interpreters (known as \"kernels\"). The Python interpreter is loaded by default as the language kernel when using Jupyter, but you can specify use of another kernel when invoking a particular command. To see a list of installed language kernels, run this command: jupyter kernelspec list To use a kernel, specify it by name when invoking a command. For example, to use the R 3.4.0 interpreter on Cheyenne in the Jupyter QtConsole, enter the following: jupyter qtconsole --kernel = r-3.4 The console will load with the R command line interpreter active. If you need a language kernel that has not been installed, you can install it yourself in your local directory or contact the NCAR Research Computing help desk to have it added in the system library.","title":"Using alternative language kernels"},{"location":"compute-systems/additional-resources/cron/","text":"Occasionally users may want to automate a common recurring task. Typical use cases are to initiate batch jobs, transfer input data from an external site, or run some automated testing. The UNIX cron service allows users to schedule scripts to be run based on a recurrence rule. As of December 2023 we have deployed a high-availability cron service independent of the individual HPC systems to support these workflows. This separated, HA solution allows us to perform maintenance on the HPC resources while not interrupting cron workflows that can tolerate the downtime. Logging in \u00b6 Once you have an HPC systems account , you can log in to cron.hpc.ucar.edu via ssh command: ssh username@cron.hpc.ucar.edu After the usual two-factor authentication process , this will place you on the high-availability cron server. Cron server IP address \u00b6 For certain automation workflows external sites may need to \"allow\" access from NCAR's systems based on IP address. cron.hpc.ucar.edu : 128.117.211.234 If you are performing automated connections to remote sites and encounter access issues, it may be necessary to work with the remote site's administrators to add this IP address to their trusted connections configuration (details are site- and process-specific, work with your remote site support team). Appropriate usage, user environment, and resource restrictions \u00b6 The primary use case for this resources is to initiate routine, scheduled work that is primarily performed elsewhere, such as in the HPC batch environment on either Derecho or Casper. As a result, the user software environment is intentionally sparse, and each user is placed into a control group limited to 1GB of system memory to protect system resource utilization. The typical GLADE file systems are accessible, however there is no default software environment provided. Typical usage of this cron resource is: Interacting with PBS, Performing small, automated file processing activities, and Connecting to the HPC systems directly through ssh to perform additional processing tasks. Accessing PBS commands \u00b6 The typical PBS commands `qsub , qstat , etc... are available by default, and users can access both Derecho and Casper PBS queues from the cron system provided that PBS server names are appended to the usual queue specifications (similar to the usual PBS cross-submission described here ): Derecho Access Casper Access Command-line specification of a Derecho queue: cron$ qsub -q main@desched1 [...] PBS Script specification of a Derecho queue: #PBS -q main@desched1 Command-line specification of a Casper queue: cron$ qsub -q casper@casper-pbs [...] PBS Script specification of a Casper queue: #PBS -q casper@casper-pbs Connecting to other NCAR HPC resources \u00b6 The cron servers are trusted by other HPC resources, allowing users to ssh to other systems without additional two-factor authentication. A common workflow then is for a small, lightweight script to be initiated on the cron servers which in turn runs additional commands on Derecho or Casper. Derecho Access Casper Access # ssh to a Derecho login node and do something useful... cron$ ssh derecho \"hostname && uptime\" # ssh to a Casper login node and do something useful... cron$ ssh casper \"hostname && uptime\" Installing and editing crontab entries \u00b6 To schedule a process with cron , a user must establish a crontab entry. This can be done interactively by running the command crontab -e to edit your crontab directly, or by creating a file and \"installing\" it with crontab <filename> . Additionally, you can list your current crontab entries via crontab -l . (See man crontab for more details.) In either case, the crontab entry has a very particular, fixed format. crontab syntax \u00b6 sample crontab entry format # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0\u201359) # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0\u201323) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1\u201331) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1\u201312) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0\u20136) (Sunday to Saturday); # \u2502 \u2502 \u2502 \u2502 \u2502 # \u2502 \u2502 \u2502 \u2502 \u2502 # \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * <command to execute> That is, 5 fields defining the recurrence rule, and a command to execute. The syntax also supports ranges and stepping values. Some examples: sample crontab entries # run every 15 minutes: */15 * * * * <my rapid command> # run every night at 23:04 (11:04 PM): 4 23 * * * <my daily command> # 09:23 on every day-of-week from Monday through Friday. 23 9 * * 1-5 <my weekday commands> # the first day of every-other month 0 0 1 */2 * <my infrequent command> The crontab guru is a helpful resource for translating crontab time syntax into human-friendly time specifications. crontab commands \u00b6 Keep your crontab commands as simple as possible, and do not make any assumptions regarding the execution environment (initial working directories, environment variables, etc...). We also recommend redirecting script output to aid in monitoring and debugging. The command can be a short sequence of commands chained together with the shell operator && if desired, for example: # run every night at 23:04 (11:04 PM): 4 23 * * * cd /glade/work/<username>/my_cron_stuff/ && ./run_nightly.sh &>> ./run_nightly.log This will run the command run_nightly.sh from within the directory /glade/work/<username>/my_cron_stuff/ , appending both standard output and standard error into the file run_nightly.log . Best practices for cron jobs \u00b6 Locking for exclusive execution \u00b6 Many shell scripts are not designed to be run concurrently, and even for those that are, concurrent execution is often not the users' intent. In such scenarios the user should make provisions so that only one occurrence of the script is running at a time. File locking is a convenient mechanism to implement this behavior, whereby a process will only run after it has gained exclusive access to a resource - in this case a \"lock file\". This approach is particularly useful under cron - if a particular instance of a script is running slow, cron could re-launch the same script potentially many times. File locking prevents such script \"pile-up.\" The utility lockfile can be incorporated into shell scripts as follows: using lockfile to prevent concurrent execution LOCK = \" ${ HOME } /.my_cron_job.lock\" remove_lock () { rm -f \" ${ LOCK } \" } another_instance () { echo \"Cannot acquire lock on ${ LOCK } \" echo \"There is another instance running, exiting\" exit 1 } lockfile -r 5 -l 3600 \" ${ LOCK } \" || another_instance trap remove_lock EXIT We declare two utility functions: remove_lock and another_instance . The command lockfile will attempt to gain exclusive access to our ${LOCK} file, retrying 5 times ( -r 5 ). If we cannot acquire the desired lock, another_instance prints some information and exits the script. Note that when the script exits - cleanly or not - we want to remove our lock file. We use the bash trap mechanism to accomplish this by calling remove_lock at EXIT . In this case we forcibly remove any \"stale\" lock files when more than an hour old (3,600 seconds, -l 3600 ) as defensive measure. This would be appropriate for a short running script, when we can assume any leftover lock files beyond some threshold age are invalid. See man lockfile for additional options. Pedantic error checking \u00b6 It is always a good idea to perform error checking inside shell scripts, but especially so when running under cron. For example, when changing directories inside a script: # go to desired directory, exit on failure: cd /glade/work/ ${ USER } /mydir || exit 1 [ ... ] This will abort the job if the cd fails. (The || exit 1 construct is executed only if the first command exits with a failure status.) Without this type of pedantic error checking the script would continue to run, and the remainder of the script would attempt to run, but in the wrong directory - especially dangerous if later steps remove files!! Logging \u00b6 In addition to any typical logging of expected output from your automated processes, it is beneficial to capture some information from the system as well. For example, logging the execution environment timestamp = \" $( date +%F@%H:%M ) \" cron_logdir = \" ${ HOME } /.my_cron_logs/\" scriptdir = \" $( cd \" $( dirname \" ${ BASH_SOURCE [0] } \" ) \" >/dev/null 2 > & 1 && pwd ) \" mkdir -p ${ cron_logdir } || exit 1 echo \"[ ${ timestamp } ]: Running ${ 0 } on $( hostname ) from $( pwd ) ; scriptdir= ${ scriptdir } \" \\ | tee -a ${ cron_logdir } /particular_tool.log creates the following output: [2023-12-11@14:33]: Running ./sample_cron_job.sh on derecho6 from [...]; scriptdir=[...] which can be useful in the future; particularly many years from now if cron stops working for you and you have forgotten where your cron scripts were located. Sample Cron script and crontab processes \u00b6 Sample Cron script and crontab installation processes Our cron_driver.sh script will submit two PBS batch jobs: one for prepossessing to run on Casper, and another model execution to run on Derecho. cron_driver.sh #!/bin/bash -l #-------------------------------------------------------------- LOCK = \" ${ HOME } /.my_cron_job.lock\" remove_lock () { rm -f \" ${ LOCK } \" } another_instance () { echo \"Cannot acquire lock on ${ LOCK } \" echo \"There is another instance running, exiting\" exit 1 } # acquire an exclusive lock on our ${LOCK} file to make sure # only one copy of this script is running at a time. lockfile -r 5 -l 3600 \" ${ LOCK } \" || another_instance trap remove_lock EXIT #-------------------------------------------------------------- # logging: Print status to standard out, and redirect also to our # specified cron_logdir. timestamp = \" $( date +%F@%H:%M ) \" cron_logdir = \" ${ HOME } /.my_cron_logs/\" scriptdir = \" $( cd \" $( dirname \" ${ BASH_SOURCE [0] } \" ) \" >/dev/null 2 > & 1 && pwd ) \" mkdir -p ${ cron_logdir } || exit 1 echo -e \"[ ${ timestamp } ]: Running ${ 0 } on $( hostname ) \\n\\tfrom $( pwd ) \\n\\tscriptdir= ${ scriptdir } \" \\ | tee -a ${ cron_logdir } /sample_job.log #-------------------------------------------------------------- # go to desired directory, exit on failure: cd /glade/work/ ${ USER } /my_cron_job/ || { echo \"cannot cd to the desired directory!!\" ; exit 1 ; } # clean up any old INPUT_DATA rm -f ./INPUT_DATA # lauch preprocessing job, capturing its job ID PREREQ = $( qsub -q casper@casper-pbs ./prep_job.pbs ) || { echo \"cannot connect to Casper PBS\" ; exit 1 ; } echo ${ PREREQ } # lauch model job qsub -q main@desched1 -W depend = afterok: ${ PREREQ } ./run_model.pbs || { echo \"cannot connect to Derecho PBS\" ; exit 1 ; } The script begins by establishing an exclusive file lock, performing some logging, and then moving to the intended run directory. The first job ( prep_job.pbs ) will create a file called INPUT_DATA which is used by the second job ( run_model.pbs ). Because the second job depends on the first, we submit the second using a PBS job dependency . We also remove the INPUT_DATA at the beginning of the script - since it should be created by prep_job.pbs , we want to make sure that step functioned as intended. In this way if INPUT_DATA exists when we execute run_model.pbs it can only be because the preceding step ran successfully - and our INPUT_DATA is not stale. Sample crontab entries First we prepare a simple text file that contains the entries for all our desired cron processes: my_crontab: # run my frequent cron job every 15 minutes */15 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./frequent.sh &>> logs/frequent.log # run my hourly cron jobs at 10 minutes past every hour 10 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./hourly.sh &>> logs/hourly.log # run my daily cron jobs at 00:45 am. 45 0 * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./daily.sh &>> logs/daily.log Installing crontab entries We can then install, inspect, and edit our entries with the crontab command. # Install my_crontab: cron$ crontab ./my_crontab Inspecting crontab entries # Inspect installed crontab: cron$ crontab -l # run my frequent cron job every 15 minutes */15 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./frequent.sh &>> logs/frequent.log # run my hourly cron jobs at 10 minutes past every hour 10 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./hourly.sh &>> logs/hourly.log # run my daily cron jobs at 00 :45 am. 45 0 * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./daily.sh &>> logs/daily.log Editing crontab entries # edit my crontab # (uses the default editor, or whatever is specified by the EDITOR environment variable) cron$ crontab -e","title":"Cron services"},{"location":"compute-systems/additional-resources/cron/#logging-in","text":"Once you have an HPC systems account , you can log in to cron.hpc.ucar.edu via ssh command: ssh username@cron.hpc.ucar.edu After the usual two-factor authentication process , this will place you on the high-availability cron server.","title":"Logging in"},{"location":"compute-systems/additional-resources/cron/#cron-server-ip-address","text":"For certain automation workflows external sites may need to \"allow\" access from NCAR's systems based on IP address. cron.hpc.ucar.edu : 128.117.211.234 If you are performing automated connections to remote sites and encounter access issues, it may be necessary to work with the remote site's administrators to add this IP address to their trusted connections configuration (details are site- and process-specific, work with your remote site support team).","title":"Cron server IP address"},{"location":"compute-systems/additional-resources/cron/#appropriate-usage-user-environment-and-resource-restrictions","text":"The primary use case for this resources is to initiate routine, scheduled work that is primarily performed elsewhere, such as in the HPC batch environment on either Derecho or Casper. As a result, the user software environment is intentionally sparse, and each user is placed into a control group limited to 1GB of system memory to protect system resource utilization. The typical GLADE file systems are accessible, however there is no default software environment provided. Typical usage of this cron resource is: Interacting with PBS, Performing small, automated file processing activities, and Connecting to the HPC systems directly through ssh to perform additional processing tasks.","title":"Appropriate usage, user environment, and resource restrictions"},{"location":"compute-systems/additional-resources/cron/#accessing-pbs-commands","text":"The typical PBS commands `qsub , qstat , etc... are available by default, and users can access both Derecho and Casper PBS queues from the cron system provided that PBS server names are appended to the usual queue specifications (similar to the usual PBS cross-submission described here ): Derecho Access Casper Access Command-line specification of a Derecho queue: cron$ qsub -q main@desched1 [...] PBS Script specification of a Derecho queue: #PBS -q main@desched1 Command-line specification of a Casper queue: cron$ qsub -q casper@casper-pbs [...] PBS Script specification of a Casper queue: #PBS -q casper@casper-pbs","title":"Accessing PBS commands"},{"location":"compute-systems/additional-resources/cron/#connecting-to-other-ncar-hpc-resources","text":"The cron servers are trusted by other HPC resources, allowing users to ssh to other systems without additional two-factor authentication. A common workflow then is for a small, lightweight script to be initiated on the cron servers which in turn runs additional commands on Derecho or Casper. Derecho Access Casper Access # ssh to a Derecho login node and do something useful... cron$ ssh derecho \"hostname && uptime\" # ssh to a Casper login node and do something useful... cron$ ssh casper \"hostname && uptime\"","title":"Connecting to other NCAR HPC resources"},{"location":"compute-systems/additional-resources/cron/#installing-and-editing-crontab-entries","text":"To schedule a process with cron , a user must establish a crontab entry. This can be done interactively by running the command crontab -e to edit your crontab directly, or by creating a file and \"installing\" it with crontab <filename> . Additionally, you can list your current crontab entries via crontab -l . (See man crontab for more details.) In either case, the crontab entry has a very particular, fixed format.","title":"Installing and editing crontab entries"},{"location":"compute-systems/additional-resources/cron/#crontab-syntax","text":"sample crontab entry format # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0\u201359) # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0\u201323) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1\u201331) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1\u201312) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0\u20136) (Sunday to Saturday); # \u2502 \u2502 \u2502 \u2502 \u2502 # \u2502 \u2502 \u2502 \u2502 \u2502 # \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * <command to execute> That is, 5 fields defining the recurrence rule, and a command to execute. The syntax also supports ranges and stepping values. Some examples: sample crontab entries # run every 15 minutes: */15 * * * * <my rapid command> # run every night at 23:04 (11:04 PM): 4 23 * * * <my daily command> # 09:23 on every day-of-week from Monday through Friday. 23 9 * * 1-5 <my weekday commands> # the first day of every-other month 0 0 1 */2 * <my infrequent command> The crontab guru is a helpful resource for translating crontab time syntax into human-friendly time specifications.","title":"crontab syntax"},{"location":"compute-systems/additional-resources/cron/#crontab-commands","text":"Keep your crontab commands as simple as possible, and do not make any assumptions regarding the execution environment (initial working directories, environment variables, etc...). We also recommend redirecting script output to aid in monitoring and debugging. The command can be a short sequence of commands chained together with the shell operator && if desired, for example: # run every night at 23:04 (11:04 PM): 4 23 * * * cd /glade/work/<username>/my_cron_stuff/ && ./run_nightly.sh &>> ./run_nightly.log This will run the command run_nightly.sh from within the directory /glade/work/<username>/my_cron_stuff/ , appending both standard output and standard error into the file run_nightly.log .","title":"crontab commands"},{"location":"compute-systems/additional-resources/cron/#best-practices-for-cron-jobs","text":"","title":"Best practices for cron jobs"},{"location":"compute-systems/additional-resources/cron/#locking-for-exclusive-execution","text":"Many shell scripts are not designed to be run concurrently, and even for those that are, concurrent execution is often not the users' intent. In such scenarios the user should make provisions so that only one occurrence of the script is running at a time. File locking is a convenient mechanism to implement this behavior, whereby a process will only run after it has gained exclusive access to a resource - in this case a \"lock file\". This approach is particularly useful under cron - if a particular instance of a script is running slow, cron could re-launch the same script potentially many times. File locking prevents such script \"pile-up.\" The utility lockfile can be incorporated into shell scripts as follows: using lockfile to prevent concurrent execution LOCK = \" ${ HOME } /.my_cron_job.lock\" remove_lock () { rm -f \" ${ LOCK } \" } another_instance () { echo \"Cannot acquire lock on ${ LOCK } \" echo \"There is another instance running, exiting\" exit 1 } lockfile -r 5 -l 3600 \" ${ LOCK } \" || another_instance trap remove_lock EXIT We declare two utility functions: remove_lock and another_instance . The command lockfile will attempt to gain exclusive access to our ${LOCK} file, retrying 5 times ( -r 5 ). If we cannot acquire the desired lock, another_instance prints some information and exits the script. Note that when the script exits - cleanly or not - we want to remove our lock file. We use the bash trap mechanism to accomplish this by calling remove_lock at EXIT . In this case we forcibly remove any \"stale\" lock files when more than an hour old (3,600 seconds, -l 3600 ) as defensive measure. This would be appropriate for a short running script, when we can assume any leftover lock files beyond some threshold age are invalid. See man lockfile for additional options.","title":"Locking for exclusive execution"},{"location":"compute-systems/additional-resources/cron/#pedantic-error-checking","text":"It is always a good idea to perform error checking inside shell scripts, but especially so when running under cron. For example, when changing directories inside a script: # go to desired directory, exit on failure: cd /glade/work/ ${ USER } /mydir || exit 1 [ ... ] This will abort the job if the cd fails. (The || exit 1 construct is executed only if the first command exits with a failure status.) Without this type of pedantic error checking the script would continue to run, and the remainder of the script would attempt to run, but in the wrong directory - especially dangerous if later steps remove files!!","title":"Pedantic error checking"},{"location":"compute-systems/additional-resources/cron/#logging","text":"In addition to any typical logging of expected output from your automated processes, it is beneficial to capture some information from the system as well. For example, logging the execution environment timestamp = \" $( date +%F@%H:%M ) \" cron_logdir = \" ${ HOME } /.my_cron_logs/\" scriptdir = \" $( cd \" $( dirname \" ${ BASH_SOURCE [0] } \" ) \" >/dev/null 2 > & 1 && pwd ) \" mkdir -p ${ cron_logdir } || exit 1 echo \"[ ${ timestamp } ]: Running ${ 0 } on $( hostname ) from $( pwd ) ; scriptdir= ${ scriptdir } \" \\ | tee -a ${ cron_logdir } /particular_tool.log creates the following output: [2023-12-11@14:33]: Running ./sample_cron_job.sh on derecho6 from [...]; scriptdir=[...] which can be useful in the future; particularly many years from now if cron stops working for you and you have forgotten where your cron scripts were located.","title":"Logging"},{"location":"compute-systems/additional-resources/cron/#sample-cron-script-and-crontab-processes","text":"Sample Cron script and crontab installation processes Our cron_driver.sh script will submit two PBS batch jobs: one for prepossessing to run on Casper, and another model execution to run on Derecho. cron_driver.sh #!/bin/bash -l #-------------------------------------------------------------- LOCK = \" ${ HOME } /.my_cron_job.lock\" remove_lock () { rm -f \" ${ LOCK } \" } another_instance () { echo \"Cannot acquire lock on ${ LOCK } \" echo \"There is another instance running, exiting\" exit 1 } # acquire an exclusive lock on our ${LOCK} file to make sure # only one copy of this script is running at a time. lockfile -r 5 -l 3600 \" ${ LOCK } \" || another_instance trap remove_lock EXIT #-------------------------------------------------------------- # logging: Print status to standard out, and redirect also to our # specified cron_logdir. timestamp = \" $( date +%F@%H:%M ) \" cron_logdir = \" ${ HOME } /.my_cron_logs/\" scriptdir = \" $( cd \" $( dirname \" ${ BASH_SOURCE [0] } \" ) \" >/dev/null 2 > & 1 && pwd ) \" mkdir -p ${ cron_logdir } || exit 1 echo -e \"[ ${ timestamp } ]: Running ${ 0 } on $( hostname ) \\n\\tfrom $( pwd ) \\n\\tscriptdir= ${ scriptdir } \" \\ | tee -a ${ cron_logdir } /sample_job.log #-------------------------------------------------------------- # go to desired directory, exit on failure: cd /glade/work/ ${ USER } /my_cron_job/ || { echo \"cannot cd to the desired directory!!\" ; exit 1 ; } # clean up any old INPUT_DATA rm -f ./INPUT_DATA # lauch preprocessing job, capturing its job ID PREREQ = $( qsub -q casper@casper-pbs ./prep_job.pbs ) || { echo \"cannot connect to Casper PBS\" ; exit 1 ; } echo ${ PREREQ } # lauch model job qsub -q main@desched1 -W depend = afterok: ${ PREREQ } ./run_model.pbs || { echo \"cannot connect to Derecho PBS\" ; exit 1 ; } The script begins by establishing an exclusive file lock, performing some logging, and then moving to the intended run directory. The first job ( prep_job.pbs ) will create a file called INPUT_DATA which is used by the second job ( run_model.pbs ). Because the second job depends on the first, we submit the second using a PBS job dependency . We also remove the INPUT_DATA at the beginning of the script - since it should be created by prep_job.pbs , we want to make sure that step functioned as intended. In this way if INPUT_DATA exists when we execute run_model.pbs it can only be because the preceding step ran successfully - and our INPUT_DATA is not stale. Sample crontab entries First we prepare a simple text file that contains the entries for all our desired cron processes: my_crontab: # run my frequent cron job every 15 minutes */15 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./frequent.sh &>> logs/frequent.log # run my hourly cron jobs at 10 minutes past every hour 10 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./hourly.sh &>> logs/hourly.log # run my daily cron jobs at 00:45 am. 45 0 * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./daily.sh &>> logs/daily.log Installing crontab entries We can then install, inspect, and edit our entries with the crontab command. # Install my_crontab: cron$ crontab ./my_crontab Inspecting crontab entries # Inspect installed crontab: cron$ crontab -l # run my frequent cron job every 15 minutes */15 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./frequent.sh &>> logs/frequent.log # run my hourly cron jobs at 10 minutes past every hour 10 * * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./hourly.sh &>> logs/hourly.log # run my daily cron jobs at 00 :45 am. 45 0 * * * cd /glade/u/home/benkirk/my_cron/ && mkdir -p logs && ./daily.sh &>> logs/daily.log Editing crontab entries # edit my crontab # (uses the default editor, or whatever is specified by the EDITOR environment variable) cron$ crontab -e","title":"Sample Cron script and crontab processes"},{"location":"compute-systems/additional-resources/thunder-test-system/","text":"The Thunder cluster is a test system that features Marvell's ThunderX2 Arm processors. These processors use the aarch64 instruction set, rather than the x86-64 instruction set used by Intel and AMD processors. Thunder consists of: one node with 128 GB of memory four nodes with 256 GB of memory 100GbE Mellanox Ethernet interconnect Follow the procedures below to begin using the system. Additional information and screen captures depicting various steps in the process are included in this slide presentation : ChameleonCloud-WIP-09212022.pdf Getting started \u00b6 Users access the Thunder nodes by establishing a connection to them via Chameleon Cloud. To get started: Create a Chameleon Cloud account. Email hpcrd@ucar.edu and request access to the NCARExplore project. Accessing Thunder \u00b6 Once you have been added to the NCARExplore project, access the Thunder nodes by going to CHI@NCAR and logging into your Chameleon account. From there you'll be able to lease nodes and start up instances . Leasing Thunder nodes \u00b6 An individual user can request or lease one or more nodes for up to seven (7) days and create an IP address for accessing them. Follow these steps: From the Project menu, select Reservations , then Leases . Select the Create Lease button. Complete the General section by specifying a name (required), start time (defaults to now), and the length of your lease (defaults to 1 day). Complete the Hosts section by checking the Reserve Hosts box and selecting the minimum and maximum number of hosts (both default to 1). There is only one type of node on Thunder, so there is no need to select Resource Properties . Complete the Networks section by checking the Reserve Floating IPs box and specifying the number of Floating IP addresses you want to reserve (typically 1). There is only one physical network on Thunder controlled by Chameleon, so there is no need to select Reserve Network . Once you have entered all your selections, select the Create button. Launching an instance \u00b6 When your lease status is ACTIVE, you can launch an instance on your leased Thunder nodes. From the Project menu, select Compute , then Instances . Select the Launch Instance button. Complete the Details section by specifying an Instance Name , selecting your lease in the Reservation box, and selecting the number of instances you want to start in the Count box (defaults to 1). Complete the Source section by choosing an image: Click the ^ next to the CC-Ubuntu20.04-ARM64 RAW image to choose the default image. Complete the Key Pair section by selecting either the Create Key Pair button or the Import KeyPair button. This key will be used to log in to your instance using the \"cc\" user account. If you have already uploaded a key pair to CHI@NCAR , that key pair will already be selected as the default. Once you have entered all your selection, select Launch Instance . Accessing your instance \u00b6 Once your instance has been provisioned and starts (status is Active ), select Floating IPs on the Network menu, then the Associate button next to your reserved IP address. Associate your floating IP address with a port by selecting your instance under \"Port to be associated.\" (See image). Once the status of that association is \"Up\" you can use your IP address and your key to ssh to your active instance by following this example (substituting your own IP address): ssh cc@128.117.250.23 At that point, you can use the reserved Thunder nodes for your work. More information and getting help \u00b6 Detailed Chameleon Cloud documentation is available here . Since Thunder is a test system, send any support inquiries, software and hardware concerns, or requests for access to hpcfl@ucar.edu instead of the CISL Help Desk. We also welcome any feedback and performance reports that you can share as you test your workflows on Thunder.","title":"Thunder test system"},{"location":"compute-systems/additional-resources/thunder-test-system/#getting-started","text":"Users access the Thunder nodes by establishing a connection to them via Chameleon Cloud. To get started: Create a Chameleon Cloud account. Email hpcrd@ucar.edu and request access to the NCARExplore project.","title":"Getting started"},{"location":"compute-systems/additional-resources/thunder-test-system/#accessing-thunder","text":"Once you have been added to the NCARExplore project, access the Thunder nodes by going to CHI@NCAR and logging into your Chameleon account. From there you'll be able to lease nodes and start up instances .","title":"Accessing Thunder"},{"location":"compute-systems/additional-resources/thunder-test-system/#leasing-thunder-nodes","text":"An individual user can request or lease one or more nodes for up to seven (7) days and create an IP address for accessing them. Follow these steps: From the Project menu, select Reservations , then Leases . Select the Create Lease button. Complete the General section by specifying a name (required), start time (defaults to now), and the length of your lease (defaults to 1 day). Complete the Hosts section by checking the Reserve Hosts box and selecting the minimum and maximum number of hosts (both default to 1). There is only one type of node on Thunder, so there is no need to select Resource Properties . Complete the Networks section by checking the Reserve Floating IPs box and specifying the number of Floating IP addresses you want to reserve (typically 1). There is only one physical network on Thunder controlled by Chameleon, so there is no need to select Reserve Network . Once you have entered all your selections, select the Create button.","title":"Leasing Thunder nodes"},{"location":"compute-systems/additional-resources/thunder-test-system/#launching-an-instance","text":"When your lease status is ACTIVE, you can launch an instance on your leased Thunder nodes. From the Project menu, select Compute , then Instances . Select the Launch Instance button. Complete the Details section by specifying an Instance Name , selecting your lease in the Reservation box, and selecting the number of instances you want to start in the Count box (defaults to 1). Complete the Source section by choosing an image: Click the ^ next to the CC-Ubuntu20.04-ARM64 RAW image to choose the default image. Complete the Key Pair section by selecting either the Create Key Pair button or the Import KeyPair button. This key will be used to log in to your instance using the \"cc\" user account. If you have already uploaded a key pair to CHI@NCAR , that key pair will already be selected as the default. Once you have entered all your selection, select Launch Instance .","title":"Launching an instance"},{"location":"compute-systems/additional-resources/thunder-test-system/#accessing-your-instance","text":"Once your instance has been provisioned and starts (status is Active ), select Floating IPs on the Network menu, then the Associate button next to your reserved IP address. Associate your floating IP address with a port by selecting your instance under \"Port to be associated.\" (See image). Once the status of that association is \"Up\" you can use your IP address and your key to ssh to your active instance by following this example (substituting your own IP address): ssh cc@128.117.250.23 At that point, you can use the reserved Thunder nodes for your work.","title":"Accessing your instance"},{"location":"compute-systems/additional-resources/thunder-test-system/#more-information-and-getting-help","text":"Detailed Chameleon Cloud documentation is available here . Since Thunder is a test system, send any support inquiries, software and hardware concerns, or requests for access to hpcfl@ucar.edu instead of the CISL Help Desk. We also welcome any feedback and performance reports that you can share as you test your workflows on Thunder.","title":"More information and getting help"},{"location":"compute-systems/casper/","text":"Casper cluster \u00b6 The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes. Casper is composed of over 100 nodes featuring a mixture of Intel and AMD processors, with a variety of NVIDIA General Purpose Graphical Processing Units. Please refer to the hardware summary table below for detailed specifications. Quick Start \u00b6 Logging in \u00b6 Once you have an account , have reviewed the Casper Use Policies , and have a Casper resource allocation you can log in and run jobs on the Casper data analysis and visualization cluster. To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@casper.hpc.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can omit username in the command above if your Casper username is the same as your username on your local computer. After running the ssh command, you will be asked to authenticate to finish logging in. Casper has full access to NCAR storage resources , including GLADE . Users can transfer data to and from Casper. To run data analysis and visualization jobs on the Casper system's nodes, follow the procedures described here . There is no need to transfer output files from Derecho for this since Derecho and Casper mount the same GLADE file systems. Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators. Environment \u00b6 The Casper HPC system uses OpenSUSE Linux Version 15 and supports widely used shells on its login and compute nodes. Users also have several compiler and MPI library choices. Shells \u00b6 The default login shell for new Casper users is bash . You can change the default after logging in to the Systems Accounting Manager (SAM) . It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering echo $SHELL on your Casper command line. Environment modules \u00b6 The Casper module utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. See the Environment modules page for a general discussion of module usage. Casper's default module environment is listed here . Accessing software and compiling code \u00b6 Casper users have access to Intel, NVIDIA, and GNU compilers. The Intel compiler and OpenMPI modules are loaded by default and provide access to pre-compiled HPC Software and Data Analysis and Visualization Resources . See this page for a full discussion of compiling on Casper . Many Casper data analysis and AI/ML workflows benefit instead from using Conda , especially NCAR's Python Library (NPL) or to gain access to several Machine Learning Frameworks . Running jobs on Casper \u00b6 Users can run a variety of types of jobs on Casper, including both traditional batch jobs submitted through PBS and also interactive and/or graphics-intensive analysis, often through remote desktops on Casper . Job scripts \u00b6 Job scripts are discussed broadly here . Users already familiar with PBS and batch submission may find Casper-specific PBS job scripts helpful in porting their work. Casper hardware \u00b6 Data Analysis & Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes Up to 384 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR100 InfiniBand link 1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes 1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes Machine Learning/Deep Learning & General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs 768 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket 4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs 1152 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket 8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 4 Supermicro nodes with 4 A100 GPUs 1024 GB memory per node 2 64-core 2.45-GHz AMD EPYC Milan 7763 processors per node 1.5 TB local NVMe Solid State Disk 4 Mellanox ConnectX-6 network adapters 4 NVIDIA Ampere A100 80GB SXM4 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes 384 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes 1.5 TB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket Research Data Archive nodes (reserved for RDA use) 4 Supermicro Workstation nodes 94 GB DDR4-2666 memory per node 2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node 1.92 TB local Solid State Disk 1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity) Status \u00b6 Nodes \u00b6 GPU Usage \u00b6 Queues \u00b6","title":"Casper cluster"},{"location":"compute-systems/casper/#casper-cluster","text":"The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes. Casper is composed of over 100 nodes featuring a mixture of Intel and AMD processors, with a variety of NVIDIA General Purpose Graphical Processing Units. Please refer to the hardware summary table below for detailed specifications.","title":"Casper cluster"},{"location":"compute-systems/casper/#quick-start","text":"","title":"Quick Start"},{"location":"compute-systems/casper/#logging-in","text":"Once you have an account , have reviewed the Casper Use Policies , and have a Casper resource allocation you can log in and run jobs on the Casper data analysis and visualization cluster. To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@casper.hpc.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can omit username in the command above if your Casper username is the same as your username on your local computer. After running the ssh command, you will be asked to authenticate to finish logging in. Casper has full access to NCAR storage resources , including GLADE . Users can transfer data to and from Casper. To run data analysis and visualization jobs on the Casper system's nodes, follow the procedures described here . There is no need to transfer output files from Derecho for this since Derecho and Casper mount the same GLADE file systems. Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.","title":"Logging in"},{"location":"compute-systems/casper/#environment","text":"The Casper HPC system uses OpenSUSE Linux Version 15 and supports widely used shells on its login and compute nodes. Users also have several compiler and MPI library choices.","title":"Environment"},{"location":"compute-systems/casper/#shells","text":"The default login shell for new Casper users is bash . You can change the default after logging in to the Systems Accounting Manager (SAM) . It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering echo $SHELL on your Casper command line.","title":"Shells"},{"location":"compute-systems/casper/#environment-modules","text":"The Casper module utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. See the Environment modules page for a general discussion of module usage. Casper's default module environment is listed here .","title":"Environment modules"},{"location":"compute-systems/casper/#accessing-software-and-compiling-code","text":"Casper users have access to Intel, NVIDIA, and GNU compilers. The Intel compiler and OpenMPI modules are loaded by default and provide access to pre-compiled HPC Software and Data Analysis and Visualization Resources . See this page for a full discussion of compiling on Casper . Many Casper data analysis and AI/ML workflows benefit instead from using Conda , especially NCAR's Python Library (NPL) or to gain access to several Machine Learning Frameworks .","title":"Accessing software and compiling code"},{"location":"compute-systems/casper/#running-jobs-on-casper","text":"Users can run a variety of types of jobs on Casper, including both traditional batch jobs submitted through PBS and also interactive and/or graphics-intensive analysis, often through remote desktops on Casper .","title":"Running jobs on Casper"},{"location":"compute-systems/casper/#job-scripts","text":"Job scripts are discussed broadly here . Users already familiar with PBS and batch submission may find Casper-specific PBS job scripts helpful in porting their work.","title":"Job scripts"},{"location":"compute-systems/casper/#casper-hardware","text":"Data Analysis & Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes Up to 384 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR100 InfiniBand link 1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes 1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes Machine Learning/Deep Learning & General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs 768 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket 4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs 1152 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket 8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 4 Supermicro nodes with 4 A100 GPUs 1024 GB memory per node 2 64-core 2.45-GHz AMD EPYC Milan 7763 processors per node 1.5 TB local NVMe Solid State Disk 4 Mellanox ConnectX-6 network adapters 4 NVIDIA Ampere A100 80GB SXM4 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes 384 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes 1.5 TB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket Research Data Archive nodes (reserved for RDA use) 4 Supermicro Workstation nodes 94 GB DDR4-2666 memory per node 2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node 1.92 TB local Solid State Disk 1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity)","title":"Casper hardware"},{"location":"compute-systems/casper/#status","text":"","title":"Status"},{"location":"compute-systems/casper/#nodes","text":"","title":"Nodes"},{"location":"compute-systems/casper/#gpu-usage","text":"","title":"GPU Usage"},{"location":"compute-systems/casper/#queues","text":"","title":"Queues"},{"location":"compute-systems/casper/casper-modules-list/","text":"Casper default module environment \u00b6 -------------------------- Module Stack Environments --------------------------- ncarenv/23.09 (S,L) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.9 ncl/6.6.2 cdo/2.2.2 nco/5.1.6 charliecloud/0.33 ncview/2.1.9 clang/16.0.6 ncvis/2022.08.28 cmake/3.26.3 nvhpc/23.7 conda/latest octave/8.2.0 cuda/11.8.0 (L) paraview/5.11.1 cudnn/8.7.0.84-11.8 pcre/8.45 darshan-util/3.4.2 peak-memusage/3.0.1 doxygen/1.8.20 perl/5.38.0 eigen/3.4.0 pocl/3.0 gcc/12.2.0 podman/4.5.1 go/1.20.6 texlive/20220321 grads/2.2.3 ucx/1.14.1 (L) grib-util/1.2.4 vapor/3.9.0 intel-classic/2023.2.1 vexcl/1.4.3 intel-oneapi/2023.2.1 visit/3.3.3 intel/2023.2.1 (L) vtune/2023.2.0 julia/1.9.2 wgrib2/3.1.1 linaro-forge/23.0 xxdiff/latest nccmp/1.9.1.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- eccodes/2.25.0 hdf5/1.12.2 (L) netcdf/4.9.2 (L) fftw/3.3.10 mkl/2023.2.0 openmpi/4.1.5 (L) geos/3.9.1 mpi-serial/2.3.0 proj/8.2.1 hdf/4.2.15 ncarcompilers/1.0.0 (L) udunits/2.2.28 ----------------- MPI-dependent Software - [oneapi + openmpi] ------------------ adios2/2.9.1 fftw-mpi/3.3.10 parallel-netcdf/1.12.3 darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.1 esmf/8.5.0 netcdf-mpi/4.9.2 parallelio/2.6.2 (D) Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Casper complete module listing \u00b6 ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- adios2: adios2/2.9.1 apptainer: apptainer/1.1.9 cdo: cdo/2.2.2 charliecloud: charliecloud/0.33 clang: clang/16.0.6 cmake: cmake/3.26.3 conda: conda/latest cuda: cuda/11.8.0 cudnn: cudnn/8.7.0.84-11.8 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 doxygen: doxygen/1.8.20 eccodes: eccodes/2.25.0 eigen: eigen/3.4.0 esmf: esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.7.1 geos: geos/3.9.1 go: go/1.20.6 grads: grads/2.2.3 grib-util: grib-util/1.2.4 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 intel: intel/2023.2.1 intel-classic: intel-classic/2023.2.1 intel-oneapi: intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 mkl: mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.09 nccmp: nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.6 ncview: ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/23.7 octave: octave/8.2.0 openblas: openblas/0.3.23 openmpi: openmpi/4.1.5 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/2.6.1, parallelio/2.6.2 paraview: paraview/5.11.1 pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perl: perl/5.38.0 pocl: pocl/3.0 podman: podman/4.5.1 proj: proj/8.2.1 texlive: texlive/20220321 ucx: ucx/1.14.1 udunits: udunits/2.2.28 vapor: vapor/3.9.0 vexcl: vexcl/1.4.3 visit: visit/3.3.3 vtune: vtune/2023.2.0 wgrib2: wgrib2/3.1.1 xxdiff: xxdiff/latest ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Casper modules list"},{"location":"compute-systems/casper/casper-modules-list/#casper-default-module-environment","text":"-------------------------- Module Stack Environments --------------------------- ncarenv/23.09 (S,L) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.9 ncl/6.6.2 cdo/2.2.2 nco/5.1.6 charliecloud/0.33 ncview/2.1.9 clang/16.0.6 ncvis/2022.08.28 cmake/3.26.3 nvhpc/23.7 conda/latest octave/8.2.0 cuda/11.8.0 (L) paraview/5.11.1 cudnn/8.7.0.84-11.8 pcre/8.45 darshan-util/3.4.2 peak-memusage/3.0.1 doxygen/1.8.20 perl/5.38.0 eigen/3.4.0 pocl/3.0 gcc/12.2.0 podman/4.5.1 go/1.20.6 texlive/20220321 grads/2.2.3 ucx/1.14.1 (L) grib-util/1.2.4 vapor/3.9.0 intel-classic/2023.2.1 vexcl/1.4.3 intel-oneapi/2023.2.1 visit/3.3.3 intel/2023.2.1 (L) vtune/2023.2.0 julia/1.9.2 wgrib2/3.1.1 linaro-forge/23.0 xxdiff/latest nccmp/1.9.1.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- eccodes/2.25.0 hdf5/1.12.2 (L) netcdf/4.9.2 (L) fftw/3.3.10 mkl/2023.2.0 openmpi/4.1.5 (L) geos/3.9.1 mpi-serial/2.3.0 proj/8.2.1 hdf/4.2.15 ncarcompilers/1.0.0 (L) udunits/2.2.28 ----------------- MPI-dependent Software - [oneapi + openmpi] ------------------ adios2/2.9.1 fftw-mpi/3.3.10 parallel-netcdf/1.12.3 darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.1 esmf/8.5.0 netcdf-mpi/4.9.2 parallelio/2.6.2 (D) Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"Casper default module environment"},{"location":"compute-systems/casper/casper-modules-list/#casper-complete-module-listing","text":"---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- adios2: adios2/2.9.1 apptainer: apptainer/1.1.9 cdo: cdo/2.2.2 charliecloud: charliecloud/0.33 clang: clang/16.0.6 cmake: cmake/3.26.3 conda: conda/latest cuda: cuda/11.8.0 cudnn: cudnn/8.7.0.84-11.8 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 doxygen: doxygen/1.8.20 eccodes: eccodes/2.25.0 eigen: eigen/3.4.0 esmf: esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.7.1 geos: geos/3.9.1 go: go/1.20.6 grads: grads/2.2.3 grib-util: grib-util/1.2.4 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 intel: intel/2023.2.1 intel-classic: intel-classic/2023.2.1 intel-oneapi: intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 mkl: mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.09 nccmp: nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.6 ncview: ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/23.7 octave: octave/8.2.0 openblas: openblas/0.3.23 openmpi: openmpi/4.1.5 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/2.6.1, parallelio/2.6.2 paraview: paraview/5.11.1 pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perl: perl/5.38.0 pocl: pocl/3.0 podman: podman/4.5.1 proj: proj/8.2.1 texlive: texlive/20220321 ucx: ucx/1.14.1 udunits: udunits/2.2.28 vapor: vapor/3.9.0 vexcl: vexcl/1.4.3 visit: visit/3.3.3 vtune: vtune/2023.2.0 wgrib2: wgrib2/3.1.1 xxdiff: xxdiff/latest ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Casper complete module listing"},{"location":"compute-systems/casper/casper-modules/","text":"Casper Modules \u00b6 Casper default module environment \u00b6 -------------------------- Module Stack Environments --------------------------- ncarenv/23.09 (S,L) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.9 ncl/6.6.2 cdo/2.2.2 nco/5.1.6 charliecloud/0.33 ncview/2.1.9 clang/16.0.6 ncvis/2022.08.28 cmake/3.26.3 nvhpc/23.7 conda/latest octave/8.2.0 cuda/11.8.0 (L) paraview/5.11.1 cudnn/8.7.0.84-11.8 pcre/8.45 darshan-util/3.4.2 peak-memusage/3.0.1 doxygen/1.8.20 perl/5.38.0 eigen/3.4.0 pocl/3.0 gcc/12.2.0 podman/4.5.1 go/1.20.6 texlive/20220321 grads/2.2.3 ucx/1.14.1 (L) grib-util/1.2.4 vapor/3.9.0 intel-classic/2023.2.1 vexcl/1.4.3 intel-oneapi/2023.2.1 visit/3.3.3 intel/2023.2.1 (L) vtune/2023.2.0 julia/1.9.2 wgrib2/3.1.1 linaro-forge/23.0 xxdiff/latest nccmp/1.9.1.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- eccodes/2.25.0 hdf5/1.12.2 (L) netcdf/4.9.2 (L) fftw/3.3.10 mkl/2023.2.0 openmpi/4.1.5 (L) geos/3.9.1 mpi-serial/2.3.0 proj/8.2.1 hdf/4.2.15 ncarcompilers/1.0.0 (L) udunits/2.2.28 ----------------- MPI-dependent Software - [oneapi + openmpi] ------------------ adios2/2.9.1 fftw-mpi/3.3.10 parallel-netcdf/1.12.3 darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.1 esmf/8.5.0 netcdf-mpi/4.9.2 parallelio/2.6.2 (D) Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Casper complete module listing \u00b6 ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- adios2: adios2/2.9.1 apptainer: apptainer/1.1.9 cdo: cdo/2.2.2 charliecloud: charliecloud/0.33 clang: clang/16.0.6 cmake: cmake/3.26.3 conda: conda/latest cuda: cuda/11.8.0 cudnn: cudnn/8.7.0.84-11.8 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 doxygen: doxygen/1.8.20 eccodes: eccodes/2.25.0 eigen: eigen/3.4.0 esmf: esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.7.1 geos: geos/3.9.1 go: go/1.20.6 grads: grads/2.2.3 grib-util: grib-util/1.2.4 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 intel: intel/2023.2.1 intel-classic: intel-classic/2023.2.1 intel-oneapi: intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 mkl: mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.09 nccmp: nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.6 ncview: ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/23.7 octave: octave/8.2.0 openblas: openblas/0.3.23 openmpi: openmpi/4.1.5 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/2.6.1, parallelio/2.6.2 paraview: paraview/5.11.1 pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perl: perl/5.38.0 pocl: pocl/3.0 podman: podman/4.5.1 proj: proj/8.2.1 texlive: texlive/20220321 ucx: ucx/1.14.1 udunits: udunits/2.2.28 vapor: vapor/3.9.0 vexcl: vexcl/1.4.3 visit: visit/3.3.3 vtune: vtune/2023.2.0 wgrib2: wgrib2/3.1.1 xxdiff: xxdiff/latest ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Casper Software"},{"location":"compute-systems/casper/casper-modules/#casper-modules","text":"","title":"Casper Modules"},{"location":"compute-systems/casper/casper-modules/#casper-default-module-environment","text":"-------------------------- Module Stack Environments --------------------------- ncarenv/23.09 (S,L) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.9 ncl/6.6.2 cdo/2.2.2 nco/5.1.6 charliecloud/0.33 ncview/2.1.9 clang/16.0.6 ncvis/2022.08.28 cmake/3.26.3 nvhpc/23.7 conda/latest octave/8.2.0 cuda/11.8.0 (L) paraview/5.11.1 cudnn/8.7.0.84-11.8 pcre/8.45 darshan-util/3.4.2 peak-memusage/3.0.1 doxygen/1.8.20 perl/5.38.0 eigen/3.4.0 pocl/3.0 gcc/12.2.0 podman/4.5.1 go/1.20.6 texlive/20220321 grads/2.2.3 ucx/1.14.1 (L) grib-util/1.2.4 vapor/3.9.0 intel-classic/2023.2.1 vexcl/1.4.3 intel-oneapi/2023.2.1 visit/3.3.3 intel/2023.2.1 (L) vtune/2023.2.0 julia/1.9.2 wgrib2/3.1.1 linaro-forge/23.0 xxdiff/latest nccmp/1.9.1.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- eccodes/2.25.0 hdf5/1.12.2 (L) netcdf/4.9.2 (L) fftw/3.3.10 mkl/2023.2.0 openmpi/4.1.5 (L) geos/3.9.1 mpi-serial/2.3.0 proj/8.2.1 hdf/4.2.15 ncarcompilers/1.0.0 (L) udunits/2.2.28 ----------------- MPI-dependent Software - [oneapi + openmpi] ------------------ adios2/2.9.1 fftw-mpi/3.3.10 parallel-netcdf/1.12.3 darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.1 esmf/8.5.0 netcdf-mpi/4.9.2 parallelio/2.6.2 (D) Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"Casper default module environment"},{"location":"compute-systems/casper/casper-modules/#casper-complete-module-listing","text":"---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- adios2: adios2/2.9.1 apptainer: apptainer/1.1.9 cdo: cdo/2.2.2 charliecloud: charliecloud/0.33 clang: clang/16.0.6 cmake: cmake/3.26.3 conda: conda/latest cuda: cuda/11.8.0 cudnn: cudnn/8.7.0.84-11.8 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 doxygen: doxygen/1.8.20 eccodes: eccodes/2.25.0 eigen: eigen/3.4.0 esmf: esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.7.1 geos: geos/3.9.1 go: go/1.20.6 grads: grads/2.2.3 grib-util: grib-util/1.2.4 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 intel: intel/2023.2.1 intel-classic: intel-classic/2023.2.1 intel-oneapi: intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 mkl: mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.09 nccmp: nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.6 ncview: ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/23.7 octave: octave/8.2.0 openblas: openblas/0.3.23 openmpi: openmpi/4.1.5 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/2.6.1, parallelio/2.6.2 paraview: paraview/5.11.1 pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perl: perl/5.38.0 pocl: pocl/3.0 podman: podman/4.5.1 proj: proj/8.2.1 texlive: texlive/20220321 ucx: ucx/1.14.1 udunits: udunits/2.2.28 vapor: vapor/3.9.0 vexcl: vexcl/1.4.3 visit: visit/3.3.3 vtune: vtune/2023.2.0 wgrib2: wgrib2/3.1.1 xxdiff: xxdiff/latest ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Casper complete module listing"},{"location":"compute-systems/casper/casper-use-policies/","text":"Casper use policies \u00b6 Appropriate use of login nodes \u00b6 Users may run short, non-memory-intensive processes interactively on the Derecho system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster. Job scheduling priorities \u00b6 The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following: Job priority (user selectable) Fair share factor Eligible time in queue Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: The necessary resources are not yet available. The system has been reserved for a scheduled outage. The job has been placed on hold. You have reached your concurrent core-usage limit when using the develop queue. A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job. PBS sorting factors \u00b6 Stakeholder shares and fair-share factor \u00b6 CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half. Job size \u00b6 Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start. GPU usage \u00b6 In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected. Backfilling \u00b6 When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.","title":"Casper Use Policies"},{"location":"compute-systems/casper/casper-use-policies/#casper-use-policies","text":"","title":"Casper use policies"},{"location":"compute-systems/casper/casper-use-policies/#appropriate-use-of-login-nodes","text":"Users may run short, non-memory-intensive processes interactively on the Derecho system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster.","title":"Appropriate use of login nodes"},{"location":"compute-systems/casper/casper-use-policies/#job-scheduling-priorities","text":"The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following: Job priority (user selectable) Fair share factor Eligible time in queue Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: The necessary resources are not yet available. The system has been reserved for a scheduled outage. The job has been placed on hold. You have reached your concurrent core-usage limit when using the develop queue. A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.","title":"Job scheduling priorities"},{"location":"compute-systems/casper/casper-use-policies/#pbs-sorting-factors","text":"","title":"PBS sorting factors"},{"location":"compute-systems/casper/casper-use-policies/#stakeholder-shares-and-fair-share-factor","text":"CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half.","title":"Stakeholder shares and fair-share factor"},{"location":"compute-systems/casper/casper-use-policies/#job-size","text":"Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start.","title":"Job size"},{"location":"compute-systems/casper/casper-use-policies/#gpu-usage","text":"In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected.","title":"GPU usage"},{"location":"compute-systems/casper/casper-use-policies/#backfilling","text":"When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.","title":"Backfilling"},{"location":"compute-systems/casper/remote-desktops/","text":"Using remote desktops on Casper \u00b6 Using VNC \u00b6 Most programs on Casper and Cheyenne are designed to run in the terminal, but a few either require the use of a graphical interface or work best in one. While using the default method \u2013 X-forwarding \u2013 is sufficient for simple programs like text editors, it can be prohibitively slow for more complex programs like MATLAB, IDL, VAPOR, and RStudio. When you log on to your workstation, you typically interact with programs using a graphical desktop shell. With virtual network computing (VNC), you can use a graphical desktop shell to work remotely on the Casper data analysis and visualization cluster. The remote desktop runs on Casper in a VNC session that is managed by the VNC server . You access it with a VNC client , which runs on your local workstation. To get started \u00b6 Download and install a VNC client on your local machine. CISL recommends the TigerVNC client and provides this video to help Mac users install it: Installing TigerVNC on a Mac laptop . (Installing on Windows machines is less complex.) TurboVNC also works, but systems using Java versions >8 do not include the necessary runtime libraries to enable TurboVNC's VNC viewer. Some other VNC clients \u2013 RealVNC, for example \u2013 do not work well with the VNC software installed on Casper. Connecting to a VNC session \u00b6 To begin using a remote desktop with VNC, you will need to start a VNC session. Your session will run within a Casper job and can persist for up to 24 hours. These basic steps for starting a session are described in detail below: Run the vncmgr script, which enables you to configure your VNC desktop session and start both a Casper batch job and the VNC server. Connect to the Casper batch node with your VNC client, using a port that VNC specifies. Enter a one-time password to access your VNC session. If you are connected to the NCAR Internal Network or using the NCAR VPN , you will be able to connect directly to your session on the Casper batch node using the VNC client. If you are not on the NCAR network or VPN, you will need to create an SSH tunnel to connect your local machine and remote desktop. How to create the SSH tunnel is described below. Overview of the vncmgr script \u00b6 CISL provides the vncmgr script for initiating and managing VNC sessions on Casper. How to run it is described in detail in the following section. It can be used in interactive mode or command-line mode. Interactive mode \u00b6 If you run the script without any command-line arguments, it will launch in the interactive mode. In this mode, you can start a new session, list existing sessions, query a session to retrieve connection instructions and obtain a new one-time-password, and kill a running session. The script enables you to name your session, state how long you want the server to run, and select which desktop shell to use (GNOME2, GNOME3, or KDE). It also allows for custom requests to both the job scheduler and the VNC server program. Command-line mode \u00b6 In command-line mode, you specify a subcommand and provide any desired options as command line arguments. Here are the available commands: vncmgr list vncmgr create [SESSION] --account PROJECT [--time WALLTIME \u2026] vncmgr query [SESSION] vncmgr kill [SESSION] Choosing a name SESSION is optional. If you do not provide a name, the name default will be assigned and referenced in each subcommand. Customizing the Casper job and VNC server \u00b6 The vncmgr script allows you to customize both the Casper session in which the server will run and the server itself. This customization can be done in both interactive and command-line modes. The most common uses involve increasing the resources allocated to your job. For example, you could allocate 4 CPUs and 20 GB of memory to a 2-hour VNC session using the command-line mode as follows: vncmgr create \\ --account PROJECT \\ --time 2:00:00 \\ --job-opts=\"-l select=1:ncpus=4:mem=20GB\" You do not need to specify GPU resources, as all VNC jobs are automatically placed on nodes with NVIDIA Quadro GP100 GPUs. Run vncmgr --help in a Casper login session for more information about using the script and customizing your session. Running vncmgr from your local machine \u00b6 You can run the vncmgr command directly from your local machine as shown in the example below without first starting a login session on Casper. While both command-line and interactive mode will work, CISL recommends interactive mode as it will allow you to generate new one-time passwords via the query option without having to authenticate to Caspe every time. Example VNC Session \u00b6 This demonstrates how to run vncmgr , create and configure a customized VNC session, and then connect to the session with a VNC client. In this example, the user is not connected to the NCAR VPN and needs to use an SSH tunnel. ( Alternative for PuTTY users . Run this command to get started, using your own username: ssh -t -l username casper.hpc.ucar.edu /glade/u/apps/opt/vncmgr/bin/vncmgr You will be prompted to authenticate, after which you will have access to the vncmgr menu. (If you exit vncmgr, you will have to rerun the command to regain access to it.) Create your new VNC session with a name up to 10 characters long. The session is named \"vapor\" in this example. (Choose names that will help you avoid confusion if you run multiple sessions.) A series of prompts on the next screen (below) will ask you to specify: Your project code. The wallclock time you want in HH:MM:SS format. The default is 4 hours and the maximum is 24 hours. Which desktop shell to use. The default setting is 2d, which configures your desktop to use a shell with the MATE user interface. Any optional arguments. In the example, the user requests 20 GB of memory. All VNC jobs must run on a node with a GP100 GPU. If you specify custom resource requirements, those requirements will be modified if necessary to ensure that the job can run on the correct node. When the job starts, follow the instructions and choose how you want to connect to your VNC session: tunneling or using the UCAR internal network or VPN. Pressing enter at this point will return you to the interactive menu. Use the query option if you need to return to the instructions for creating the SSH tunnel later. Your desktop on Casper will be displayed after you connect to the specified host and enter the onetime password. On the desktop, start a terminal from the list of applications. To start an application, load any required modules and run the executable. SSH tunneling with PuTTY \u00b6 The general process for creating an ssh tunnel described above will work well for clients with a command line ssh installation, however additional steps are required for Windows users with PuTTY as their ssh client. Expand the note below for additional details. SSH tunneling with PuTTY The output from your vncmgr command includes a line in this format. (Each x is a number.) ssh -l username -L xxxx:localhost4:xxxx casperxx.ucar.edu \"bash .vnctunnel-default\" Follow these steps to copy and paste the necessary information from the vncmgr command into the PuTTY interface for Windows. Load a PuTTY session with casperxx.ucar.edu as the hostname. Select Connection , then SSH . Enter the following in the Remote command field. bash .vnctunnel-default Under SSH , select Tunnels . Paste the first four digits from the ssh command above into the Source port field. Paste the localhost4:xxxx into the Destination field. Click Add , Click Open , then log in. Follow the instructions provided in your terminal window to start your VNC client. Starting SSH tunnel to the VNC server... Now load VNC on your local computer and connect to: localhost:xxxx VNC will ask for a one-time password. Use the following: xxxxxxxx This terminal session will hang until the tunnel is killed. To kill the tunnel, simply type C-c/Control-C. Using FastX \u00b6 The FastX remote desktop service gives users access to Casper for performing lightweight tasks such as text editing, running programs such as xxdiff and ncview , or running analysis scripts that consume little in the way of graphics resources. A user can log out of a FastX remote desktop and return to it later. This service will remain available while NCAR and UCAR building closures are in effect. FastX sessions that consume excessive resources are subject to being killed. For resource-intensive workloads that do not require GPU-based rendering, consider starting a Casper job instead by running an execcasper command from a FastX terminal window. For more resource-intensive work with high-end, GPU-accelerated graphics, consider using VNC as described above rather than FastX. FastX can be accessed through a web browser or a desktop client. How to use both of these options is described below. FastX via web browser with VPN access \u00b6 To use FastX without installing any software, connect to the NCAR VPN and use an updated version of any common browser. See the following section for an alternative to using the VPN. Connect to the NCAR VPN . Go to https://fastx.ucar.edu:3300 . Authenticate with your username and token response. Click the + button in the upper-left corner of the FastX window. In the next window, click the KDE button and then Launch . The KDE desktop will open in a new tab or a new browser window. Right-click on the desktop to start a Kconsole terminal window. Missing Modules under FastX? Some users will find that their terminal environment configuration is not complete (module commands will not be available, for example). To initialize your environment, run the following command once you open the Kconsole: bash/zsh tcsh # source bash/zsh login environment definition scripts: source /etc/profile # source tcsh login environment definition scripts: source /etc/csh.login Reconnecting to your FastX session \u00b6 If you want to retain your session to return to it later, just log out or kill the browser window. When you log in again, select the icon to re-open the session. Terminating your FastX session \u00b6 If you do not want to retain your session, terminate it as shown here before logging out: FastX via web browser and ssh tunnel \u00b6 If you are not connected to or do not want to connect to the NCAR VPN, you can still use FastX via web browser by creating an ssh tunnel from your laptop or desktop to FastX. Start by running the following on your command line, inserting your own username. ssh -L 3300:fastx.ucar.edu:3300 username@fastx.ucar.edu Authenticate as usual and you will be in a terminal session on a Casper node. Leave that terminal session running, open your browser, and go to https://localhost:3300/ . You may see a warning about the site being unsafe, but you may ignore the warning and continue. Authenticate with your username and token response. Continue as described in the previous section to launch the KDE desktop. Alternatives for creating ssh tunnel \u00b6 To create a tunnel using PuTTY or SecureCRT, following the examples in these video demonstrations: PuTTY ssh tunnel SecureCRT ssh tunnel After creating the tunnel, proceed as described in the previous section. FastX via desktop client \u00b6 If you\u2019re not connected to the NCAR VPN or prefer to use a faster, more robust remote desktop service, consider downloading and installing the FastX desktop client . FastX desktop client for Windows users If you do not have admin privileges to install the client on your machine, choose the \u201cWindows Nonroot\u201d client. Start the FastX client. Click the + button in the upper-left corner. Fill in the fields of the pop-up boxes as follows: Host: fastx.ucar.edu User: Enter your username Port: 22 Name: CISL remote desktop ( or any string ) Run (advanced tab): /ncar/opt/fastx/latest/bin/fastx-protocol Click OK . Select the remote desktop collection (double-click or press Enter ). Authenticate with your username and token response. Click the + button in the upper-left corner of the next window. Select the KDE icon that displays startplasma-x11 in the required command field. Double-click the icon to start the KDE desktop. You can then right-click on the desktop to get a Kconsole menu, OR Double-click the xterm icon instead to get a single xterm.","title":"Using Remote Desktops"},{"location":"compute-systems/casper/remote-desktops/#using-remote-desktops-on-casper","text":"","title":"Using remote desktops on Casper"},{"location":"compute-systems/casper/remote-desktops/#using-vnc","text":"Most programs on Casper and Cheyenne are designed to run in the terminal, but a few either require the use of a graphical interface or work best in one. While using the default method \u2013 X-forwarding \u2013 is sufficient for simple programs like text editors, it can be prohibitively slow for more complex programs like MATLAB, IDL, VAPOR, and RStudio. When you log on to your workstation, you typically interact with programs using a graphical desktop shell. With virtual network computing (VNC), you can use a graphical desktop shell to work remotely on the Casper data analysis and visualization cluster. The remote desktop runs on Casper in a VNC session that is managed by the VNC server . You access it with a VNC client , which runs on your local workstation.","title":"Using VNC"},{"location":"compute-systems/casper/remote-desktops/#to-get-started","text":"Download and install a VNC client on your local machine. CISL recommends the TigerVNC client and provides this video to help Mac users install it: Installing TigerVNC on a Mac laptop . (Installing on Windows machines is less complex.) TurboVNC also works, but systems using Java versions >8 do not include the necessary runtime libraries to enable TurboVNC's VNC viewer. Some other VNC clients \u2013 RealVNC, for example \u2013 do not work well with the VNC software installed on Casper.","title":"To get started"},{"location":"compute-systems/casper/remote-desktops/#connecting-to-a-vnc-session","text":"To begin using a remote desktop with VNC, you will need to start a VNC session. Your session will run within a Casper job and can persist for up to 24 hours. These basic steps for starting a session are described in detail below: Run the vncmgr script, which enables you to configure your VNC desktop session and start both a Casper batch job and the VNC server. Connect to the Casper batch node with your VNC client, using a port that VNC specifies. Enter a one-time password to access your VNC session. If you are connected to the NCAR Internal Network or using the NCAR VPN , you will be able to connect directly to your session on the Casper batch node using the VNC client. If you are not on the NCAR network or VPN, you will need to create an SSH tunnel to connect your local machine and remote desktop. How to create the SSH tunnel is described below.","title":"Connecting to a VNC session"},{"location":"compute-systems/casper/remote-desktops/#overview-of-the-vncmgr-script","text":"CISL provides the vncmgr script for initiating and managing VNC sessions on Casper. How to run it is described in detail in the following section. It can be used in interactive mode or command-line mode.","title":"Overview of the vncmgr script"},{"location":"compute-systems/casper/remote-desktops/#interactive-mode","text":"If you run the script without any command-line arguments, it will launch in the interactive mode. In this mode, you can start a new session, list existing sessions, query a session to retrieve connection instructions and obtain a new one-time-password, and kill a running session. The script enables you to name your session, state how long you want the server to run, and select which desktop shell to use (GNOME2, GNOME3, or KDE). It also allows for custom requests to both the job scheduler and the VNC server program.","title":"Interactive mode"},{"location":"compute-systems/casper/remote-desktops/#command-line-mode","text":"In command-line mode, you specify a subcommand and provide any desired options as command line arguments. Here are the available commands: vncmgr list vncmgr create [SESSION] --account PROJECT [--time WALLTIME \u2026] vncmgr query [SESSION] vncmgr kill [SESSION] Choosing a name SESSION is optional. If you do not provide a name, the name default will be assigned and referenced in each subcommand.","title":"Command-line mode"},{"location":"compute-systems/casper/remote-desktops/#customizing-the-casper-job-and-vnc-server","text":"The vncmgr script allows you to customize both the Casper session in which the server will run and the server itself. This customization can be done in both interactive and command-line modes. The most common uses involve increasing the resources allocated to your job. For example, you could allocate 4 CPUs and 20 GB of memory to a 2-hour VNC session using the command-line mode as follows: vncmgr create \\ --account PROJECT \\ --time 2:00:00 \\ --job-opts=\"-l select=1:ncpus=4:mem=20GB\" You do not need to specify GPU resources, as all VNC jobs are automatically placed on nodes with NVIDIA Quadro GP100 GPUs. Run vncmgr --help in a Casper login session for more information about using the script and customizing your session.","title":"Customizing the Casper job and VNC server"},{"location":"compute-systems/casper/remote-desktops/#running-vncmgr-from-your-local-machine","text":"You can run the vncmgr command directly from your local machine as shown in the example below without first starting a login session on Casper. While both command-line and interactive mode will work, CISL recommends interactive mode as it will allow you to generate new one-time passwords via the query option without having to authenticate to Caspe every time.","title":"Running vncmgr from your local machine"},{"location":"compute-systems/casper/remote-desktops/#example-vnc-session","text":"This demonstrates how to run vncmgr , create and configure a customized VNC session, and then connect to the session with a VNC client. In this example, the user is not connected to the NCAR VPN and needs to use an SSH tunnel. ( Alternative for PuTTY users . Run this command to get started, using your own username: ssh -t -l username casper.hpc.ucar.edu /glade/u/apps/opt/vncmgr/bin/vncmgr You will be prompted to authenticate, after which you will have access to the vncmgr menu. (If you exit vncmgr, you will have to rerun the command to regain access to it.) Create your new VNC session with a name up to 10 characters long. The session is named \"vapor\" in this example. (Choose names that will help you avoid confusion if you run multiple sessions.) A series of prompts on the next screen (below) will ask you to specify: Your project code. The wallclock time you want in HH:MM:SS format. The default is 4 hours and the maximum is 24 hours. Which desktop shell to use. The default setting is 2d, which configures your desktop to use a shell with the MATE user interface. Any optional arguments. In the example, the user requests 20 GB of memory. All VNC jobs must run on a node with a GP100 GPU. If you specify custom resource requirements, those requirements will be modified if necessary to ensure that the job can run on the correct node. When the job starts, follow the instructions and choose how you want to connect to your VNC session: tunneling or using the UCAR internal network or VPN. Pressing enter at this point will return you to the interactive menu. Use the query option if you need to return to the instructions for creating the SSH tunnel later. Your desktop on Casper will be displayed after you connect to the specified host and enter the onetime password. On the desktop, start a terminal from the list of applications. To start an application, load any required modules and run the executable.","title":"Example VNC Session"},{"location":"compute-systems/casper/remote-desktops/#ssh-tunneling-with-putty","text":"The general process for creating an ssh tunnel described above will work well for clients with a command line ssh installation, however additional steps are required for Windows users with PuTTY as their ssh client. Expand the note below for additional details. SSH tunneling with PuTTY The output from your vncmgr command includes a line in this format. (Each x is a number.) ssh -l username -L xxxx:localhost4:xxxx casperxx.ucar.edu \"bash .vnctunnel-default\" Follow these steps to copy and paste the necessary information from the vncmgr command into the PuTTY interface for Windows. Load a PuTTY session with casperxx.ucar.edu as the hostname. Select Connection , then SSH . Enter the following in the Remote command field. bash .vnctunnel-default Under SSH , select Tunnels . Paste the first four digits from the ssh command above into the Source port field. Paste the localhost4:xxxx into the Destination field. Click Add , Click Open , then log in. Follow the instructions provided in your terminal window to start your VNC client. Starting SSH tunnel to the VNC server... Now load VNC on your local computer and connect to: localhost:xxxx VNC will ask for a one-time password. Use the following: xxxxxxxx This terminal session will hang until the tunnel is killed. To kill the tunnel, simply type C-c/Control-C.","title":"SSH tunneling with PuTTY"},{"location":"compute-systems/casper/remote-desktops/#using-fastx","text":"The FastX remote desktop service gives users access to Casper for performing lightweight tasks such as text editing, running programs such as xxdiff and ncview , or running analysis scripts that consume little in the way of graphics resources. A user can log out of a FastX remote desktop and return to it later. This service will remain available while NCAR and UCAR building closures are in effect. FastX sessions that consume excessive resources are subject to being killed. For resource-intensive workloads that do not require GPU-based rendering, consider starting a Casper job instead by running an execcasper command from a FastX terminal window. For more resource-intensive work with high-end, GPU-accelerated graphics, consider using VNC as described above rather than FastX. FastX can be accessed through a web browser or a desktop client. How to use both of these options is described below.","title":"Using FastX"},{"location":"compute-systems/casper/remote-desktops/#fastx-via-web-browser-with-vpn-access","text":"To use FastX without installing any software, connect to the NCAR VPN and use an updated version of any common browser. See the following section for an alternative to using the VPN. Connect to the NCAR VPN . Go to https://fastx.ucar.edu:3300 . Authenticate with your username and token response. Click the + button in the upper-left corner of the FastX window. In the next window, click the KDE button and then Launch . The KDE desktop will open in a new tab or a new browser window. Right-click on the desktop to start a Kconsole terminal window. Missing Modules under FastX? Some users will find that their terminal environment configuration is not complete (module commands will not be available, for example). To initialize your environment, run the following command once you open the Kconsole: bash/zsh tcsh # source bash/zsh login environment definition scripts: source /etc/profile # source tcsh login environment definition scripts: source /etc/csh.login","title":"FastX via web browser with VPN access"},{"location":"compute-systems/casper/remote-desktops/#reconnecting-to-your-fastx-session","text":"If you want to retain your session to return to it later, just log out or kill the browser window. When you log in again, select the icon to re-open the session.","title":"Reconnecting to your FastX session"},{"location":"compute-systems/casper/remote-desktops/#terminating-your-fastx-session","text":"If you do not want to retain your session, terminate it as shown here before logging out:","title":"Terminating your FastX session"},{"location":"compute-systems/casper/remote-desktops/#fastx-via-web-browser-and-ssh-tunnel","text":"If you are not connected to or do not want to connect to the NCAR VPN, you can still use FastX via web browser by creating an ssh tunnel from your laptop or desktop to FastX. Start by running the following on your command line, inserting your own username. ssh -L 3300:fastx.ucar.edu:3300 username@fastx.ucar.edu Authenticate as usual and you will be in a terminal session on a Casper node. Leave that terminal session running, open your browser, and go to https://localhost:3300/ . You may see a warning about the site being unsafe, but you may ignore the warning and continue. Authenticate with your username and token response. Continue as described in the previous section to launch the KDE desktop.","title":"FastX via web browser and ssh tunnel"},{"location":"compute-systems/casper/remote-desktops/#alternatives-for-creating-ssh-tunnel","text":"To create a tunnel using PuTTY or SecureCRT, following the examples in these video demonstrations: PuTTY ssh tunnel SecureCRT ssh tunnel After creating the tunnel, proceed as described in the previous section.","title":"Alternatives for creating ssh tunnel"},{"location":"compute-systems/casper/remote-desktops/#fastx-via-desktop-client","text":"If you\u2019re not connected to the NCAR VPN or prefer to use a faster, more robust remote desktop service, consider downloading and installing the FastX desktop client . FastX desktop client for Windows users If you do not have admin privileges to install the client on your machine, choose the \u201cWindows Nonroot\u201d client. Start the FastX client. Click the + button in the upper-left corner. Fill in the fields of the pop-up boxes as follows: Host: fastx.ucar.edu User: Enter your username Port: 22 Name: CISL remote desktop ( or any string ) Run (advanced tab): /ncar/opt/fastx/latest/bin/fastx-protocol Click OK . Select the remote desktop collection (double-click or press Enter ). Authenticate with your username and token response. Click the + button in the upper-left corner of the next window. Select the KDE icon that displays startplasma-x11 in the required command field. Double-click the icon to start the KDE desktop. You can then right-click on the desktop to get a Kconsole menu, OR Double-click the xterm icon instead to get a single xterm.","title":"FastX via desktop client"},{"location":"compute-systems/casper/compiling-code-on-casper/","text":"Compiling code on NCAR systems \u00b6 Compilers available on Casper \u00b6 Several C/C++ and Fortran compilers are available on all NCAR HPC systems. The information on this page applies to all of those systems except where noted. Compiler Language Commands for serial programs Commands for programs using MPI Flags to enable OpenMP Intel (Classic/OneAPI) * Fortran ifort / ifx mpif90 -qopenmp C icc / icx mpicc C++ icpc / icpx mpicxx NVIDIA HPC SDK Fortran nvfortran mpif90 -mp C nvc mpicc C++ nvc++ mpicxx GNU Compiler Collection (GCC) Fortran gfortran mpif90 -fopenmp C gcc mpicc C++ g++ mpicxx * Intel OneAPI is a cross-platform toolkit that supports C, C++, Fortran, and Python programming languages and replaces Intel Parallel Studio . Casper supports both Intel OneAPI and Intel Classic Compilers. Intel is planning to retire the Intel Classic compilers and is moving toward Intel OneAPI. Intel Classic Compiler commands (ifort, icc, and icpc) will be replaced by the Intel OneAPI compilers (ifx, icx, and icpx). Compiler Commands \u00b6 All supported compilers are available via the module utility. After loading the compiler module you want to use, refer to the table above to identify and run the appropriate compilation wrapper command. If your script already includes one of the following generic MPI commands, there is no need to change it: mpif90 , mpif77 mpicc mpiCC Build any libraries that you need to support an application with the same compiler, compiler version, and compatible flags used to compile the other parts of the application, including the main executable(s). Also, before you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will help you avoid job failures that can result from missing MPI launchers and library routines. Compiler man pages \u00b6 To refer to the man page for a compiler, log in to the system where you intend to use it, load the module, then execute man for the compiler. For example: module load nvhpc man nvfortran You can also use -help flags for a description of the command-line options for each compiler. Follow this example: ifort -help nvfortran -help [=option] Tip Use compiler diagnostic flags to identify potential problems while compiling the code. Changing compilers \u00b6 To change from one compiler to another, use module swap . In this example, you are switching from Intel to NVIDIA: module swap intel nvhpc When you load a compiler module or change to a different compiler, the system makes other compatible modules available. This helps you establish a working environment and avoid conflicts. If you need to link your program with a library, use module load to load the library as in this example: module load netcdf Then you can invoke the desired compilation command, including any library linking options such as -lnetcdf . Here's an example: mpif90 -o foo.exe foo.f90 -lnetcdf Compiling CPU code \u00b6 Optimizing code for multiple types of CPUs Be aware that compiling CPU code on Casper can be complicated by the heterogeneous nature of the nodes. (Casper nodes contain a mixture of Intel Skylake, Intel Cascade Lake, and AMD Milan CPUs.) In general users will want to compile binaries that can execute on any of the CPU types. This can be accomplished by manually specifying the target CPU architecture: Intel Compilers GCC Compilers NVHPC Compilers -march=core-avx2 -march=core-avx2 -tp=zen3 If your application fails to run with an illegal instruction message, this indicates the compiled binary contains instructions incompatible with the current CPU. Try compiling with flags as indicated above, or reach out to consulting for help. Using the default Intel compiler collection \u00b6 The Intel compiler suite is available via the intel module, which is loaded by default. It includes compilers for C, C++, and Fortran codes. To see which versions are available, use the module avail command. module avail intel To load the default Intel compiler, use module load without specifying a version. module load intel To load a different version, specify the version number when loading the module. Similarly, you can swap your current compiler module to Intel by using the module swap command. module swap gcc intel Extensive documentation for using the Intel compilers is available online here . To review the manual page for a compiler, run the man command for it as in this example: man ifort What's the difference between the intel , intel-oneapi , intel-classic modules? Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system. Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family. During this process both sets of compilers are available, but through different commands under different module selections: Module Fortran C C++ intel-classic ifort icc icpc intel-oneapi ifx icx icpx intel (default) ifort icx icpx The intel-classic module makes the familiar ifort/icc/icpc compilers available, however it is expected these will be deprecated during Casper's lifetime. At this stage we expect to keep existing compiler versions available, however there will be no further updates. The intel-oneapi module uses the new ifx/icx/icpx compilers. The default intel module presently uses the older ifort Fortran compiler along with the newer icx/icpx C/C++ compilers. This choice is intentional as the newer ifx does not reliably match the performance of ifort in all cases. We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future. Optimizing your code with Intel compilers \u00b6 Intel compilers provide several different optimization and vectorization options. By default, they use the -O2 option, which includes some optimizations. Using -O3 instead will provide more aggressive optimizations that may not improve the performance of some programs, while -O1 enables minimal optimization. A higher level of optimization might increase your compile time significantly. You can also disable any optimization by using -O0 . Examples \u00b6 To compile and link a single Fortran program and create an executable, follow this example: ifort filename.f90 -o filename.exe To enable multi-threaded parallelization (OpenMP), include the -qopenmp flag as shown here: ifort -qopenmp filename.f90 -o filename.exe Other compilers \u00b6 These additional compilers are available on Casper. NVIDIA\u2019s HPC SDK the GNU Compiler Collection (GCC) Compiling GPU code \u00b6 On Casper, GPU applications should be built with either the NVIDIA HPC SDK compilers and libraries, or with two-stage linking. In the following examples, we demonstrate the use of NVIDIA\u2019s tools. To compile CUDA code to run on the Casper data analysis and visualization nodes, use the appropriate NVIDIA compiler command: nvc \u2013 NVIDIA C compiler nvcc \u2013 NVIDIA CUDA compiler (Using nvcc requires a C compiler to be present in the background; nvc , icc , or gcc , for example.) nvfortran \u2013 CUDA Fortran Additional compilation flags for GPU code will depend in large part on which GPU-programming paradigm is being used (e.g., OpenACC, OpenMP, CUDA) and which compiler collection you have loaded. The following examples show basic usage, but note that many customizations and optimizations are possible. You are encouraged to read the relevant man page for the compiler you choose. OpenACC \u00b6 To compile with OpenACC directives, simply add the -acc flag to your invocation of nvc, nvc++, or nvfortan. A Fortran example: nvfortran -o acc_bin -acc acc_code.f90 You can gather more insight into GPU acceleration decisions made by the compiler by adding -Minfo=accel to your invocation. Using compiler options, you can also specify which GPU architecture to target. This example will request compilation for both V100 and A100 GPUs: nvfortran -o acc_bin -acc -gpu = cc70,cc80 acc_code.f90 Specifying multiple acceleration targets will increase the size of the binary and the time it takes to compile the code. OpenMP \u00b6 Using OpenMP to offload code to the GPU is similar to using OpenACC. To compile a code with OpenMP offloading, use the -mp=gpu flag. The aforementioned diagnostic and target flags also apply to OpenMP offloading. nvfortran -o omp_gpu -mp = gpu omp.f90 CUDA \u00b6 The process for compiling CUDA code depends on whether you are using C++ or Fortran. For C++, the process often involves multiple stages in which you first use nvcc , the NVIDIA CUDA compiler, and then your C++ compiler of choice. nvcc -c -arch = sm_80 cuda_code.cu g++ -o cuda_bin -lcuda -lcudart main.cpp cuda_code.o Using the nvcc compiler driver with a non-NVIDIA C++ compiler requires loading a cuda environment module in addition to the compiler of choice. The compiler handles CUDA code directly, so the compiler you use must support CUDA. This means you should use nvfortran . If your source code file ends with the .cuf extension, nvfortran will enable CUDA automatically. Otherwise, you can specify the -Mcuda flag to the compiler. nvfortran -Mcuda -o cf_bin cf_code.f90 The sample below demonstrates how to compile CUDA C code on casper. hello_world.cu on Casper with CUDA hello_world.cu source file: /* hello_world.cu * --------------------------------------------------- * A Hello World example in CUDA * --------------------------------------------------- * This is a short program which uses multiple CUDA * threads to calculate a \"Hello World\" message which * is then printed to the screen. It's intended to * demonstrate the execution of a CUDA kernel. * --------------------------------------------------- */ #define SIZE 12 #include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h> /* CUDA kernel used to calculate hello world message */ __global__ void hello_world ( char * a , int N ); int main ( int argc , char ** argv ) { /* data that will live on host */ char * data ; /* data that will live in device memory */ char * d_data ; /* allocate and initialize data array */ data = ( char * ) malloc ( SIZE * sizeof ( char )); data [ 0 ] = 72 ; data [ 1 ] = 100 ; data [ 2 ] = 106 ; data [ 3 ] = 105 ; data [ 4 ] = 107 ; data [ 5 ] = 27 ; data [ 6 ] = 81 ; data [ 7 ] = 104 ; data [ 8 ] = 106 ; data [ 9 ] = 99 ; data [ 10 ] = 90 ; data [ 11 ] = 22 ; /* print data before kernel call */ printf ( \"Contents of data before kernel call: %s \\n \" , data ); /* allocate memory on device */ cudaMalloc ( & d_data , SIZE * sizeof ( char )); /* copy memory to device array */ cudaMemcpy ( d_data , data , SIZE , cudaMemcpyHostToDevice ); /* call kernel */ hello_world <<< 4 , 3 >>> ( d_data , SIZE ); /* copy data back to host */ cudaMemcpy ( data , d_data , SIZE , cudaMemcpyDeviceToHost ); /* print contents of array */ printf ( \"Contents of data after kernel call: %s \\n \" , data ); /* clean up memory on host and device */ cudaFree ( d_data ); free ( data ); return ( 0 ); } /* hello_world * Each thread increments an element of the input * array by its global thread id */ __global__ void hello_world ( char * a , int N ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < N ) a [ i ] = a [ i ] + i ; } Log into a GPU-Enabled node Log in to either Casper or Derecho and run execcasper with a GPU resource request to start an interactive job on a GPU-accelerated Casper node: execcasper -l gpu_type = gp100 --ngpus = 1 Compile hello_world.cu NVHPC GCC module reset module load nvhpc cuda nvcc -o hello hello_world.cu module reset module load gnu cuda nvcc -c hello_world.cu gcc -o hello hello_world.o Run the program ./hello Contents of data before kernel call: HdjikhjcZ Contents of data after kernel call: Hello World! Native Compiler Commands \u00b6 We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly without the ncarcompilers wrappers, see this note: Native Compiler Commands We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly, unload the NCAR default compiler wrapper environment by entering this on your command line: module unload ncarcompilers You can still use the environment variables that are set by the modules that remain loaded, as shown in the following examples of invoking compilers directly to compile a Fortran program. Intel compiler NVIDIA HPC compiler GNU compiler collection (GCC) ifort -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> nvfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> gfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> Multiple Compiler Versions and User Applications \u00b6 In addition to multiple compilers, CISL keeps available multiple versions of libraries to accommodate a wide range of users' needs. Rather than rely on the environment variable LD_LIBRARY_PATH to find the correct libraries dynamically, we encode library paths within the binaries when you build Executable and Linkable Format (ELF) executables. To do this, we use RPATH rather than LD_LIBRARY_PATH to set the necessary paths to shared libraries. This enables your executable to work regardless of updates to new default versions of the various libraries; it doesn't have to search dynamically at run time to load them. It also means you don't need to worry about setting the variable or loading another module, greatly reducing the likelihood of runtime errors. Common Compiler Options and Diagnostic Flags \u00b6 Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable. Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags. Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly. The following options can be helpful as you compile code to run in the HPC environment that CISL manages. Compiler Flag Effect Intel Intel C++ diagnostic options -debug all provides complete debugging information. -g places symbolic debugging information in the executable program. -check all performs all runtime checks (includes bounds checking). -warn all enables all warnings. -stand f08 warns of usage that does not conform to the Fortran 2008 standard. -traceback enables stack trace if the program crashes. GCC GCC diagnostic warning ptions -ggdb places symbolic debugging information in the executable program for use by GDB. -fcheck=all performs all runtime checks (includes bounds checking). -Wall enables all warnings. -std=f2008 warns of usage that does not conform to the Fortran 2008 standard. NVIDIA HPC SDK NVIDIA HPC SDK documentation . -g Include symbolic debugging information in the object modules with optimization disabled ( -O0 ). -gopt Include symbolic debugging information in the object modules without affecting any optimizations. -C or -Mbounds Add array bounds checking. -Mchkptr Check for unintended de-referencing of NULL pointers. -Minform=inform Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase.","title":"Compiling Code on Casper"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiling-code-on-ncar-systems","text":"","title":"Compiling code on NCAR systems"},{"location":"compute-systems/casper/compiling-code-on-casper/#compilers-available-on-casper","text":"Several C/C++ and Fortran compilers are available on all NCAR HPC systems. The information on this page applies to all of those systems except where noted. Compiler Language Commands for serial programs Commands for programs using MPI Flags to enable OpenMP Intel (Classic/OneAPI) * Fortran ifort / ifx mpif90 -qopenmp C icc / icx mpicc C++ icpc / icpx mpicxx NVIDIA HPC SDK Fortran nvfortran mpif90 -mp C nvc mpicc C++ nvc++ mpicxx GNU Compiler Collection (GCC) Fortran gfortran mpif90 -fopenmp C gcc mpicc C++ g++ mpicxx * Intel OneAPI is a cross-platform toolkit that supports C, C++, Fortran, and Python programming languages and replaces Intel Parallel Studio . Casper supports both Intel OneAPI and Intel Classic Compilers. Intel is planning to retire the Intel Classic compilers and is moving toward Intel OneAPI. Intel Classic Compiler commands (ifort, icc, and icpc) will be replaced by the Intel OneAPI compilers (ifx, icx, and icpx).","title":"Compilers available on Casper"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiler-commands","text":"All supported compilers are available via the module utility. After loading the compiler module you want to use, refer to the table above to identify and run the appropriate compilation wrapper command. If your script already includes one of the following generic MPI commands, there is no need to change it: mpif90 , mpif77 mpicc mpiCC Build any libraries that you need to support an application with the same compiler, compiler version, and compatible flags used to compile the other parts of the application, including the main executable(s). Also, before you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will help you avoid job failures that can result from missing MPI launchers and library routines.","title":"Compiler Commands"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiler-man-pages","text":"To refer to the man page for a compiler, log in to the system where you intend to use it, load the module, then execute man for the compiler. For example: module load nvhpc man nvfortran You can also use -help flags for a description of the command-line options for each compiler. Follow this example: ifort -help nvfortran -help [=option] Tip Use compiler diagnostic flags to identify potential problems while compiling the code.","title":"Compiler man pages"},{"location":"compute-systems/casper/compiling-code-on-casper/#changing-compilers","text":"To change from one compiler to another, use module swap . In this example, you are switching from Intel to NVIDIA: module swap intel nvhpc When you load a compiler module or change to a different compiler, the system makes other compatible modules available. This helps you establish a working environment and avoid conflicts. If you need to link your program with a library, use module load to load the library as in this example: module load netcdf Then you can invoke the desired compilation command, including any library linking options such as -lnetcdf . Here's an example: mpif90 -o foo.exe foo.f90 -lnetcdf","title":"Changing compilers"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiling-cpu-code","text":"Optimizing code for multiple types of CPUs Be aware that compiling CPU code on Casper can be complicated by the heterogeneous nature of the nodes. (Casper nodes contain a mixture of Intel Skylake, Intel Cascade Lake, and AMD Milan CPUs.) In general users will want to compile binaries that can execute on any of the CPU types. This can be accomplished by manually specifying the target CPU architecture: Intel Compilers GCC Compilers NVHPC Compilers -march=core-avx2 -march=core-avx2 -tp=zen3 If your application fails to run with an illegal instruction message, this indicates the compiled binary contains instructions incompatible with the current CPU. Try compiling with flags as indicated above, or reach out to consulting for help.","title":"Compiling CPU code"},{"location":"compute-systems/casper/compiling-code-on-casper/#using-the-default-intel-compiler-collection","text":"The Intel compiler suite is available via the intel module, which is loaded by default. It includes compilers for C, C++, and Fortran codes. To see which versions are available, use the module avail command. module avail intel To load the default Intel compiler, use module load without specifying a version. module load intel To load a different version, specify the version number when loading the module. Similarly, you can swap your current compiler module to Intel by using the module swap command. module swap gcc intel Extensive documentation for using the Intel compilers is available online here . To review the manual page for a compiler, run the man command for it as in this example: man ifort What's the difference between the intel , intel-oneapi , intel-classic modules? Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system. Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family. During this process both sets of compilers are available, but through different commands under different module selections: Module Fortran C C++ intel-classic ifort icc icpc intel-oneapi ifx icx icpx intel (default) ifort icx icpx The intel-classic module makes the familiar ifort/icc/icpc compilers available, however it is expected these will be deprecated during Casper's lifetime. At this stage we expect to keep existing compiler versions available, however there will be no further updates. The intel-oneapi module uses the new ifx/icx/icpx compilers. The default intel module presently uses the older ifort Fortran compiler along with the newer icx/icpx C/C++ compilers. This choice is intentional as the newer ifx does not reliably match the performance of ifort in all cases. We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future.","title":"Using the default Intel compiler collection"},{"location":"compute-systems/casper/compiling-code-on-casper/#optimizing-your-code-with-intel-compilers","text":"Intel compilers provide several different optimization and vectorization options. By default, they use the -O2 option, which includes some optimizations. Using -O3 instead will provide more aggressive optimizations that may not improve the performance of some programs, while -O1 enables minimal optimization. A higher level of optimization might increase your compile time significantly. You can also disable any optimization by using -O0 .","title":"Optimizing your code with Intel compilers"},{"location":"compute-systems/casper/compiling-code-on-casper/#examples","text":"To compile and link a single Fortran program and create an executable, follow this example: ifort filename.f90 -o filename.exe To enable multi-threaded parallelization (OpenMP), include the -qopenmp flag as shown here: ifort -qopenmp filename.f90 -o filename.exe","title":"Examples"},{"location":"compute-systems/casper/compiling-code-on-casper/#other-compilers","text":"These additional compilers are available on Casper. NVIDIA\u2019s HPC SDK the GNU Compiler Collection (GCC)","title":"Other compilers"},{"location":"compute-systems/casper/compiling-code-on-casper/#compiling-gpu-code","text":"On Casper, GPU applications should be built with either the NVIDIA HPC SDK compilers and libraries, or with two-stage linking. In the following examples, we demonstrate the use of NVIDIA\u2019s tools. To compile CUDA code to run on the Casper data analysis and visualization nodes, use the appropriate NVIDIA compiler command: nvc \u2013 NVIDIA C compiler nvcc \u2013 NVIDIA CUDA compiler (Using nvcc requires a C compiler to be present in the background; nvc , icc , or gcc , for example.) nvfortran \u2013 CUDA Fortran Additional compilation flags for GPU code will depend in large part on which GPU-programming paradigm is being used (e.g., OpenACC, OpenMP, CUDA) and which compiler collection you have loaded. The following examples show basic usage, but note that many customizations and optimizations are possible. You are encouraged to read the relevant man page for the compiler you choose.","title":"Compiling GPU code"},{"location":"compute-systems/casper/compiling-code-on-casper/#openacc","text":"To compile with OpenACC directives, simply add the -acc flag to your invocation of nvc, nvc++, or nvfortan. A Fortran example: nvfortran -o acc_bin -acc acc_code.f90 You can gather more insight into GPU acceleration decisions made by the compiler by adding -Minfo=accel to your invocation. Using compiler options, you can also specify which GPU architecture to target. This example will request compilation for both V100 and A100 GPUs: nvfortran -o acc_bin -acc -gpu = cc70,cc80 acc_code.f90 Specifying multiple acceleration targets will increase the size of the binary and the time it takes to compile the code.","title":"OpenACC"},{"location":"compute-systems/casper/compiling-code-on-casper/#openmp","text":"Using OpenMP to offload code to the GPU is similar to using OpenACC. To compile a code with OpenMP offloading, use the -mp=gpu flag. The aforementioned diagnostic and target flags also apply to OpenMP offloading. nvfortran -o omp_gpu -mp = gpu omp.f90","title":"OpenMP"},{"location":"compute-systems/casper/compiling-code-on-casper/#cuda","text":"The process for compiling CUDA code depends on whether you are using C++ or Fortran. For C++, the process often involves multiple stages in which you first use nvcc , the NVIDIA CUDA compiler, and then your C++ compiler of choice. nvcc -c -arch = sm_80 cuda_code.cu g++ -o cuda_bin -lcuda -lcudart main.cpp cuda_code.o Using the nvcc compiler driver with a non-NVIDIA C++ compiler requires loading a cuda environment module in addition to the compiler of choice. The compiler handles CUDA code directly, so the compiler you use must support CUDA. This means you should use nvfortran . If your source code file ends with the .cuf extension, nvfortran will enable CUDA automatically. Otherwise, you can specify the -Mcuda flag to the compiler. nvfortran -Mcuda -o cf_bin cf_code.f90 The sample below demonstrates how to compile CUDA C code on casper. hello_world.cu on Casper with CUDA hello_world.cu source file: /* hello_world.cu * --------------------------------------------------- * A Hello World example in CUDA * --------------------------------------------------- * This is a short program which uses multiple CUDA * threads to calculate a \"Hello World\" message which * is then printed to the screen. It's intended to * demonstrate the execution of a CUDA kernel. * --------------------------------------------------- */ #define SIZE 12 #include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h> /* CUDA kernel used to calculate hello world message */ __global__ void hello_world ( char * a , int N ); int main ( int argc , char ** argv ) { /* data that will live on host */ char * data ; /* data that will live in device memory */ char * d_data ; /* allocate and initialize data array */ data = ( char * ) malloc ( SIZE * sizeof ( char )); data [ 0 ] = 72 ; data [ 1 ] = 100 ; data [ 2 ] = 106 ; data [ 3 ] = 105 ; data [ 4 ] = 107 ; data [ 5 ] = 27 ; data [ 6 ] = 81 ; data [ 7 ] = 104 ; data [ 8 ] = 106 ; data [ 9 ] = 99 ; data [ 10 ] = 90 ; data [ 11 ] = 22 ; /* print data before kernel call */ printf ( \"Contents of data before kernel call: %s \\n \" , data ); /* allocate memory on device */ cudaMalloc ( & d_data , SIZE * sizeof ( char )); /* copy memory to device array */ cudaMemcpy ( d_data , data , SIZE , cudaMemcpyHostToDevice ); /* call kernel */ hello_world <<< 4 , 3 >>> ( d_data , SIZE ); /* copy data back to host */ cudaMemcpy ( data , d_data , SIZE , cudaMemcpyDeviceToHost ); /* print contents of array */ printf ( \"Contents of data after kernel call: %s \\n \" , data ); /* clean up memory on host and device */ cudaFree ( d_data ); free ( data ); return ( 0 ); } /* hello_world * Each thread increments an element of the input * array by its global thread id */ __global__ void hello_world ( char * a , int N ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < N ) a [ i ] = a [ i ] + i ; } Log into a GPU-Enabled node Log in to either Casper or Derecho and run execcasper with a GPU resource request to start an interactive job on a GPU-accelerated Casper node: execcasper -l gpu_type = gp100 --ngpus = 1 Compile hello_world.cu NVHPC GCC module reset module load nvhpc cuda nvcc -o hello hello_world.cu module reset module load gnu cuda nvcc -c hello_world.cu gcc -o hello hello_world.o Run the program ./hello Contents of data before kernel call: HdjikhjcZ Contents of data after kernel call: Hello World!","title":"CUDA"},{"location":"compute-systems/casper/compiling-code-on-casper/#native-compiler-commands","text":"We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly without the ncarcompilers wrappers, see this note: Native Compiler Commands We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly, unload the NCAR default compiler wrapper environment by entering this on your command line: module unload ncarcompilers You can still use the environment variables that are set by the modules that remain loaded, as shown in the following examples of invoking compilers directly to compile a Fortran program. Intel compiler NVIDIA HPC compiler GNU compiler collection (GCC) ifort -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> nvfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> gfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library>","title":"Native Compiler Commands"},{"location":"compute-systems/casper/compiling-code-on-casper/#multiple-compiler-versions-and-user-applications","text":"In addition to multiple compilers, CISL keeps available multiple versions of libraries to accommodate a wide range of users' needs. Rather than rely on the environment variable LD_LIBRARY_PATH to find the correct libraries dynamically, we encode library paths within the binaries when you build Executable and Linkable Format (ELF) executables. To do this, we use RPATH rather than LD_LIBRARY_PATH to set the necessary paths to shared libraries. This enables your executable to work regardless of updates to new default versions of the various libraries; it doesn't have to search dynamically at run time to load them. It also means you don't need to worry about setting the variable or loading another module, greatly reducing the likelihood of runtime errors.","title":"Multiple Compiler Versions and User Applications"},{"location":"compute-systems/casper/compiling-code-on-casper/#common-compiler-options-and-diagnostic-flags","text":"Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable. Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags. Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly. The following options can be helpful as you compile code to run in the HPC environment that CISL manages. Compiler Flag Effect Intel Intel C++ diagnostic options -debug all provides complete debugging information. -g places symbolic debugging information in the executable program. -check all performs all runtime checks (includes bounds checking). -warn all enables all warnings. -stand f08 warns of usage that does not conform to the Fortran 2008 standard. -traceback enables stack trace if the program crashes. GCC GCC diagnostic warning ptions -ggdb places symbolic debugging information in the executable program for use by GDB. -fcheck=all performs all runtime checks (includes bounds checking). -Wall enables all warnings. -std=f2008 warns of usage that does not conform to the Fortran 2008 standard. NVIDIA HPC SDK NVIDIA HPC SDK documentation . -g Include symbolic debugging information in the object modules with optimization disabled ( -O0 ). -gopt Include symbolic debugging information in the object modules without affecting any optimizations. -C or -Mbounds Add array bounds checking. -Mchkptr Check for unintended de-referencing of NULL pointers. -Minform=inform Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase.","title":"Common Compiler Options and Diagnostic Flags"},{"location":"compute-systems/casper/compiling-code-on-casper/compiling-multigpu-multicuda-casper/","text":"","title":"Compiling multigpu multicuda casper"},{"location":"compute-systems/casper/starting-casper-jobs/","text":"Starting Casper jobs \u00b6 This page describes how to use PBS Pro to submit jobs to run on nodes in the Casper cluster. Unless GPUs are required, run jobs that require the use of more than one compute node on Derecho. Procedures for starting both interactive jobs and batch jobs on Casper are described below. Also: Compile your code on Casper nodes if you will run it on Casper. See Calculating charges to learn how core-hours charges are calculated for jobs that run on Casper. Begin by logging in on Casper or Derecho . Casper Wall-clock limits The wall-clock limit on the Casper cluster is 24 hours except as noted below. Specify the hours your job needs as in the examples below. Use either the hours:minutes:seconds format or minutes:seconds . Interactive jobs \u00b6 Starting a remote command shell with execcasper \u00b6 Run the execcasper command to start an interactive job. Invoking it without an argument will start an interactive shell on the first available HTC node . The default wall-clock time is 6 hours. To use another type of node, include a select statement specifying the resources you need. The execcasper command accepts all PBS flags and resource specifications as detailed by man qsub . If you do not include a resource specification by using either a select statement or convenience flags, you will be assigned 1 CPU with 10 GB of memory and no GPUs. If no project is assigned with either the -A option or the DAV_PROJECT environment variable, any valid project listed for your username will be chosen at random. Starting a virtual desktop with vncmgr \u00b6 If your work with complex programs such as MATLAB and VAPOR requires the use of virtual network computing (VNC) server and client software, use vncmgr instead of execcasper . Using vncmgr simplifies configuring and running a VNC session in a Casper batch job. How to do that is documented here . Batch jobs \u00b6 Prepare a batch script by following one of the examples here . Most Casper batch jobs use the casper submission queue. The exception is for GPU development jobs, which are submitted to the gpudev submission queue. Be aware that the system does not import your login environment by default, so make sure your script loads the software modules that you will need to run the job. Caution: Avoid using the PBS -V option with cross submission Avoid using the PBS -V option to propagate your environment settings to the batch job; it can cause odd behaviors and job failures when used in submissions to Casper from Derecho . If you need to forward certain environment variables to your job, use the lower-case -v option to specify them. (See man qsub for details.) When your job script is ready, use qsub to submit it from the Casper login nodes. GPU development jobs \u00b6 A submission queue called gpudev is available between 8 a.m. and 5:30 p.m. Mountain time Monday to Friday to support application development and debugging efforts on general purpose and ML/AI GPU applications. This queue provides rapid access to up to 4 V100 GPUs, avoiding the sometimes lengthy queue wait times in the gpgpu execution queue. Job submissions to this queue are limited to 30 minutes walltime instead of the 24-hour wallclock limit for all other submissions. All jobs submitted to the queue must request one or more V100 GPUs (up to four) in their resource directives. Node memory can be specified explicitly as usual, but by default jobs will be assigned N/4 of the total memory on a node, where N is the number of V100 GPUs requested. Concurrent resource limits \u00b6 Job limits are in place to ensure short dispatch times and a fair distribution of system resources. The specific limits that apply to your submission depend on the resources requested by your job. Based on your request, your submission will be classified as shown in the table. Submission queue Job category (execution queue) Job resource requests Limits casper 24-hour wallclock limit largemem mem>361 GB ncpus < =36 ngpus=0 Up to 5 jobs eligible for execution at any one time (more can be queued) htc mem < =361 GB ncpus < =36 ngpus=0 Up to 468 CPUs in use per user at any one time. Up to 4680 GB memory per user at any one time (across all jobs in category) vis gpu_type=gp100 Up to 2 GPUs in use per user at any one time Individual jobs are limited to a single gp100 (no multi-GPU jobs) gpgpu gpu_type=v100|a100 Up to 32 GPUs in use per user at any one time; users may submit jobs requesting more than 32 GPUs for execution on weekends. gpudev 30-minute wallclock limit ncpus < =36 1 < =ngpus < =4 Queue is only operational from 8 a.m. to 5:30 p.m. Mountain time, Monday to Friday. Users may have only one active job in the queue at any time. NVMe node-local storage \u00b6 Casper nodes each have 2 TB of local NVMe solid-state disk (SSD) storage. Some is used to augment memory to reduce the likelihood of jobs failing because of excessive memory use. NVMe storage can also be used while a job is running . (Recommended only for I/O-intensive jobs.) Data stored in /local_scratch/pbs.$PBS_JOBID are deleted when the job ends. To use this disk space while your job is running, include the following in your batch script after customizing as needed. ### Copy input data to NVMe (can check that it fits first using \"df -h\") cp -r /glade/scratch/$USER/input_data /local_scratch/pbs.$PBS_JOBID ### Run script to process data (NCL example takes input and output paths as command line arguments) ncl proc_data.ncl /local_scratch/pbs.$PBS_JOBID/input_data /local_scratch/pbs.$PBS_JOBID/output_data ### Move output data before the job ends and your output is deleted mv /local_scratch/pbs.$PBS_JOBID/output_data ${SCRATCH} Script examples \u00b6 See this page for many Casper PBS job script examples: Casper job script examples When your script is ready, submit your batch job for scheduling as shown here .","title":"Starting Casper jobs"},{"location":"compute-systems/casper/starting-casper-jobs/#starting-casper-jobs","text":"This page describes how to use PBS Pro to submit jobs to run on nodes in the Casper cluster. Unless GPUs are required, run jobs that require the use of more than one compute node on Derecho. Procedures for starting both interactive jobs and batch jobs on Casper are described below. Also: Compile your code on Casper nodes if you will run it on Casper. See Calculating charges to learn how core-hours charges are calculated for jobs that run on Casper. Begin by logging in on Casper or Derecho . Casper Wall-clock limits The wall-clock limit on the Casper cluster is 24 hours except as noted below. Specify the hours your job needs as in the examples below. Use either the hours:minutes:seconds format or minutes:seconds .","title":"Starting Casper jobs"},{"location":"compute-systems/casper/starting-casper-jobs/#interactive-jobs","text":"","title":"Interactive jobs"},{"location":"compute-systems/casper/starting-casper-jobs/#starting-a-remote-command-shell-with-execcasper","text":"Run the execcasper command to start an interactive job. Invoking it without an argument will start an interactive shell on the first available HTC node . The default wall-clock time is 6 hours. To use another type of node, include a select statement specifying the resources you need. The execcasper command accepts all PBS flags and resource specifications as detailed by man qsub . If you do not include a resource specification by using either a select statement or convenience flags, you will be assigned 1 CPU with 10 GB of memory and no GPUs. If no project is assigned with either the -A option or the DAV_PROJECT environment variable, any valid project listed for your username will be chosen at random.","title":"Starting a remote command shell with execcasper"},{"location":"compute-systems/casper/starting-casper-jobs/#starting-a-virtual-desktop-with-vncmgr","text":"If your work with complex programs such as MATLAB and VAPOR requires the use of virtual network computing (VNC) server and client software, use vncmgr instead of execcasper . Using vncmgr simplifies configuring and running a VNC session in a Casper batch job. How to do that is documented here .","title":"Starting a virtual desktop with vncmgr"},{"location":"compute-systems/casper/starting-casper-jobs/#batch-jobs","text":"Prepare a batch script by following one of the examples here . Most Casper batch jobs use the casper submission queue. The exception is for GPU development jobs, which are submitted to the gpudev submission queue. Be aware that the system does not import your login environment by default, so make sure your script loads the software modules that you will need to run the job. Caution: Avoid using the PBS -V option with cross submission Avoid using the PBS -V option to propagate your environment settings to the batch job; it can cause odd behaviors and job failures when used in submissions to Casper from Derecho . If you need to forward certain environment variables to your job, use the lower-case -v option to specify them. (See man qsub for details.) When your job script is ready, use qsub to submit it from the Casper login nodes.","title":"Batch jobs"},{"location":"compute-systems/casper/starting-casper-jobs/#gpu-development-jobs","text":"A submission queue called gpudev is available between 8 a.m. and 5:30 p.m. Mountain time Monday to Friday to support application development and debugging efforts on general purpose and ML/AI GPU applications. This queue provides rapid access to up to 4 V100 GPUs, avoiding the sometimes lengthy queue wait times in the gpgpu execution queue. Job submissions to this queue are limited to 30 minutes walltime instead of the 24-hour wallclock limit for all other submissions. All jobs submitted to the queue must request one or more V100 GPUs (up to four) in their resource directives. Node memory can be specified explicitly as usual, but by default jobs will be assigned N/4 of the total memory on a node, where N is the number of V100 GPUs requested.","title":"GPU development jobs"},{"location":"compute-systems/casper/starting-casper-jobs/#concurrent-resource-limits","text":"Job limits are in place to ensure short dispatch times and a fair distribution of system resources. The specific limits that apply to your submission depend on the resources requested by your job. Based on your request, your submission will be classified as shown in the table. Submission queue Job category (execution queue) Job resource requests Limits casper 24-hour wallclock limit largemem mem>361 GB ncpus < =36 ngpus=0 Up to 5 jobs eligible for execution at any one time (more can be queued) htc mem < =361 GB ncpus < =36 ngpus=0 Up to 468 CPUs in use per user at any one time. Up to 4680 GB memory per user at any one time (across all jobs in category) vis gpu_type=gp100 Up to 2 GPUs in use per user at any one time Individual jobs are limited to a single gp100 (no multi-GPU jobs) gpgpu gpu_type=v100|a100 Up to 32 GPUs in use per user at any one time; users may submit jobs requesting more than 32 GPUs for execution on weekends. gpudev 30-minute wallclock limit ncpus < =36 1 < =ngpus < =4 Queue is only operational from 8 a.m. to 5:30 p.m. Mountain time, Monday to Friday. Users may have only one active job in the queue at any time.","title":"Concurrent resource limits"},{"location":"compute-systems/casper/starting-casper-jobs/#nvme-node-local-storage","text":"Casper nodes each have 2 TB of local NVMe solid-state disk (SSD) storage. Some is used to augment memory to reduce the likelihood of jobs failing because of excessive memory use. NVMe storage can also be used while a job is running . (Recommended only for I/O-intensive jobs.) Data stored in /local_scratch/pbs.$PBS_JOBID are deleted when the job ends. To use this disk space while your job is running, include the following in your batch script after customizing as needed. ### Copy input data to NVMe (can check that it fits first using \"df -h\") cp -r /glade/scratch/$USER/input_data /local_scratch/pbs.$PBS_JOBID ### Run script to process data (NCL example takes input and output paths as command line arguments) ncl proc_data.ncl /local_scratch/pbs.$PBS_JOBID/input_data /local_scratch/pbs.$PBS_JOBID/output_data ### Move output data before the job ends and your output is deleted mv /local_scratch/pbs.$PBS_JOBID/output_data ${SCRATCH}","title":"NVMe node-local storage"},{"location":"compute-systems/casper/starting-casper-jobs/#script-examples","text":"See this page for many Casper PBS job script examples: Casper job script examples When your script is ready, submit your batch job for scheduling as shown here .","title":"Script examples"},{"location":"compute-systems/casper/starting-casper-jobs/casper-job-script-examples-content/","text":"Batch script to run a high-throughput computing (HTC) job on Casper This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU. bash tcsh #!/bin/bash -l ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat #!/bin/tcsh ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat Batch script to run an MPI GPU job on Casper bash tcsh #!/bin/bash -l #PBS -N mpi_job #PBS -A <project_code> #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB #PBS -l gpu_type=v100 #PBS -l walltime=01:00:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name #!/bin/tcsh ### Job Name #PBS -N mpi_gpu_job ### Charging account #PBS -A <project_code> ### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB ### Specify that the GPUs will be V100s #PBS -l gpu_type=v100 ### Allow job to run up to 1 hour #PBS -l walltime=01:00:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name Batch script to run a pure OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name #!/bin/tcsh #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name Batch script to run a hybrid MPI/OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name #!/bin/tcsh #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name Batch script to run a job array on Casper Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. This example uses environment variable PBS_ARRAY_INDEX as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the #PBS -J array directive. bash tcsh #!/bin/bash -l #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=00:10:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX #!/bin/tcsh #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=01:00:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX If you need to include a job ID in a subsequent qsub command, be sure to use quotation marks to preserve the [] brackets, as in this example: qsub -W \"depend=afterok:317485[]\" postprocess.pbs Using NVIDIA MPS in Casper GPU jobs Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper. Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU. The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk: Launch the MPS control daemon on each job node. Start the MPS server on each node. Run your GPU application. Terminate the MPS server and daemon. To enable MPS on job hosts, add mps=1 to your select statement chunks as follows: #PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1 On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time. Jobs may not request MPS activation on nodes with GP100 GPUs. In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU. bash #!/bin/bash #PBS -A <project_code> #PBS -N gpu_mps_job #PBS -q casper@casper-pbs #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1 #PBS -l gpu_type=v100 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4 # Run application using Open MPI mpirun ./executable_name","title":"Casper job script examples content"},{"location":"compute-systems/casper/starting-casper-jobs/casper-job-script-examples/","text":"These PBS batch script examples work for executables generated by any compiler and MPI library installed on Casper . (The defaults are Intel and OpenMPI, respectively.) Remember to substitute your own job name and project code, and customize the other directives and commands as necessary. The examples are similar to PBS examples for running jobs on Derecho . For help with any of them, contact the NCAR Research Computing help desk . When your script is ready, submit your batch job from a Casper login node by using the qsub command followed by the name of your script file. qsub script_name Batch script to run a high-throughput computing (HTC) job on Casper This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU. bash tcsh #!/bin/bash -l ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat #!/bin/tcsh ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat Batch script to run an MPI GPU job on Casper bash tcsh #!/bin/bash -l #PBS -N mpi_job #PBS -A <project_code> #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB #PBS -l gpu_type=v100 #PBS -l walltime=01:00:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name #!/bin/tcsh ### Job Name #PBS -N mpi_gpu_job ### Charging account #PBS -A <project_code> ### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB ### Specify that the GPUs will be V100s #PBS -l gpu_type=v100 ### Allow job to run up to 1 hour #PBS -l walltime=01:00:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name Batch script to run a pure OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name #!/bin/tcsh #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name Batch script to run a hybrid MPI/OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name #!/bin/tcsh #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name Batch script to run a job array on Casper Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. This example uses environment variable PBS_ARRAY_INDEX as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the #PBS -J array directive. bash tcsh #!/bin/bash -l #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=00:10:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX #!/bin/tcsh #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=01:00:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX If you need to include a job ID in a subsequent qsub command, be sure to use quotation marks to preserve the [] brackets, as in this example: qsub -W \"depend=afterok:317485[]\" postprocess.pbs Using NVIDIA MPS in Casper GPU jobs Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper. Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU. The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk: Launch the MPS control daemon on each job node. Start the MPS server on each node. Run your GPU application. Terminate the MPS server and daemon. To enable MPS on job hosts, add mps=1 to your select statement chunks as follows: #PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1 On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time. Jobs may not request MPS activation on nodes with GP100 GPUs. In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU. bash #!/bin/bash #PBS -A <project_code> #PBS -N gpu_mps_job #PBS -q casper@casper-pbs #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1 #PBS -l gpu_type=v100 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4 # Run application using Open MPI mpirun ./executable_name","title":"Casper job Script Examples"},{"location":"compute-systems/cheyenne/","text":"Cheyenne supercomputer \u00b6 Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne Logging in \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Hardware \u00b6 145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504) Estimating core-hours needed \u00b6 Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Cheyenne supercomputer"},{"location":"compute-systems/cheyenne/#cheyenne-supercomputer","text":"Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne","title":"Cheyenne supercomputer"},{"location":"compute-systems/cheyenne/#logging-in","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in"},{"location":"compute-systems/cheyenne/#hardware","text":"145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504)","title":"Hardware"},{"location":"compute-systems/cheyenne/#estimating-core-hours-needed","text":"Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Estimating core-hours needed"},{"location":"compute-systems/cheyenne/cheyenne/","text":"Cheyenne supercomputer \u00b6 Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne Logging in \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Hardware \u00b6 145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504) Estimating core-hours needed \u00b6 Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Cheyenne supercomputer"},{"location":"compute-systems/cheyenne/cheyenne/#cheyenne-supercomputer","text":"Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne","title":"Cheyenne supercomputer"},{"location":"compute-systems/cheyenne/cheyenne/#logging-in","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in"},{"location":"compute-systems/cheyenne/cheyenne/#hardware","text":"145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504)","title":"Hardware"},{"location":"compute-systems/cheyenne/cheyenne/#estimating-core-hours-needed","text":"Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Estimating core-hours needed"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/","text":"Cheyenne job script examples \u00b6 When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here: Storing temporary files with TMPDIR . Specify an ompthreads value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases: If your code was compiled with the GNU or Intel compiler using the -f openmp option. If your code was compiled with a PGI compiler with option -mp . Tip For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify ompthreads=1 in your PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel mpt If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/19.1.1 mpt/2.25 These examples are based on the following assumptions: You are using HPE's Message Passing Toolkit (MPT) MPI library. The programs being run were compiled with Intel 16.0.3 or a later version. Contact the NCAR Research Computing help desk for assistance with adapting them for other cases. When your script is ready, submit your batch job for scheduling as shown here . Page contents \u00b6 Batch script to run an MPI job Batch script to run a pure OpenMP job Batch script to run a hybrid MPI/OpenMP job Batch script to run a job array Batch script to run a command file (MPMD) job Pinning tasks/threads to CPUs Dependent jobs Batch script to run an MPI job \u00b6 tcsh bash #!/bin/tcsh ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe #!/bin/bash ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe Batch script to run a pure OpenMP job \u00b6 To run a pure OpenMP job, specify the number of CPUs you want from the node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name #!/bin/bash #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name Batch script to run a hybrid MPI/OpenMP job \u00b6 If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher. Specify the number of CPUs you want from each node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name Batch script to run a job array \u00b6 Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\" Before submitting this batch script: Place files input.1 through input.18 in the same directory where you have the sequential cmd command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array. tcsh bash #!/bin/tcsh ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX #!/bin/bash ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX Tip If you need to include a job ID in a subsequent qsub command as in the following example, be sure to use quotation marks to preserve the [] brackets: `` qsub -W \"depend=afterok:317485[]\" postprocess.pbs ``` Batch script to run a command file (MPMD) job \u00b6 Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file ( cmdfile ) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts. The command file used in the example job scripts has these four lines. ./cmd1.exe < input1 > output1 ./cmd2.exe < input2 > output2 ./cmd3.exe < input3 > output3 ./cmd4.exe < input4 > output4 The job will produce output files that reside in the directory in which the job was submitted. In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others. Tip If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. setenv MPI_SHEPHERD true mpiexec_mpt launch_cf.sh cmdfile #!/bin/bash #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. export MPI_SHEPHERD = true mpiexec_mpt launch_cf.sh cmdfile Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486","title":"Cheyenne job script examples"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#cheyenne-job-script-examples","text":"When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here: Storing temporary files with TMPDIR . Specify an ompthreads value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases: If your code was compiled with the GNU or Intel compiler using the -f openmp option. If your code was compiled with a PGI compiler with option -mp . Tip For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify ompthreads=1 in your PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel mpt If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/19.1.1 mpt/2.25 These examples are based on the following assumptions: You are using HPE's Message Passing Toolkit (MPT) MPI library. The programs being run were compiled with Intel 16.0.3 or a later version. Contact the NCAR Research Computing help desk for assistance with adapting them for other cases. When your script is ready, submit your batch job for scheduling as shown here .","title":"Cheyenne job script examples"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#page-contents","text":"Batch script to run an MPI job Batch script to run a pure OpenMP job Batch script to run a hybrid MPI/OpenMP job Batch script to run a job array Batch script to run a command file (MPMD) job Pinning tasks/threads to CPUs Dependent jobs","title":"Page contents"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-an-mpi-job","text":"tcsh bash #!/bin/tcsh ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe #!/bin/bash ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe","title":"Batch script to run an MPI job"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-pure-openmp-job","text":"To run a pure OpenMP job, specify the number of CPUs you want from the node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name #!/bin/bash #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name","title":"Batch script to run a pure OpenMP job"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-hybrid-mpiopenmp-job","text":"If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher. Specify the number of CPUs you want from each node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name","title":"Batch script to run a hybrid MPI/OpenMP job"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-job-array","text":"Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\" Before submitting this batch script: Place files input.1 through input.18 in the same directory where you have the sequential cmd command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array. tcsh bash #!/bin/tcsh ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX #!/bin/bash ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX Tip If you need to include a job ID in a subsequent qsub command as in the following example, be sure to use quotation marks to preserve the [] brackets: `` qsub -W \"depend=afterok:317485[]\" postprocess.pbs ```","title":"Batch script to run a job array"},{"location":"compute-systems/cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-command-file-mpmd-job","text":"Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file ( cmdfile ) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts. The command file used in the example job scripts has these four lines. ./cmd1.exe < input1 > output1 ./cmd2.exe < input2 > output2 ./cmd3.exe < input3 > output3 ./cmd4.exe < input4 > output4 The job will produce output files that reside in the directory in which the job was submitted. In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others. Tip If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. setenv MPI_SHEPHERD true mpiexec_mpt launch_cf.sh cmdfile #!/bin/bash #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. export MPI_SHEPHERD = true mpiexec_mpt launch_cf.sh cmdfile Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486","title":"Batch script to run a command file (MPMD) job"},{"location":"compute-systems/derecho/","text":"Derecho \u00b6 Installed in 2023, Derecho is NCAR's latest supercomputer. Derecho features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below. Estimating Derecho Allocation Needs Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems. Quick Start \u00b6 Logging in \u00b6 Once you have an account , have reviewed the Derecho Use Policies , and have a Derecho resource allocation you can log in and run jobs on the Derecho data analysis and visualization cluster. To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@derecho.hpc.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can omit username in the command above if your Derecho username is the same as your username on your local computer. After running the ssh command, you will be asked to authenticate to finish logging in. Derecho has full access to NCAR storage resources , including GLADE . Users can transfer data to and from Derecho. To run data analysis and visualization jobs on the Derecho system's nodes, follow the procedures described here . There is no need to transfer output files from Derecho for this since Derecho and Casper mount the same GLADE file systems. Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators. Environment \u00b6 The Derecho HPC system uses a Cray variant of SUSE Enterprise Linux and supports widely used shells on its login and compute nodes. Users also have several compiler and MPI library choices. Shells \u00b6 The default login shell for new Derecho users is bash . You can change the default after logging in to the Systems Accounting Manager (SAM) . It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering echo $SHELL on your Derecho command line. Environment modules \u00b6 The Derecho module utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. See the Environment modules page for a general discussion of module usage. Derecho's default module environment is listed here . Accessing software and compiling code \u00b6 Derecho users have access to Intel, NVIDIA, and GNU compilers. The Intel compiler and OpenMPI modules are loaded by default and provide access to pre-compiled HPC Software and Data Analysis and Visualization Resources . See this page for a full discussion of compiling on Derecho . Many Derecho data analysis and AI/ML workflows benefit instead from using Conda , especially NCAR's Python Library (NPL) or to gain access to several Machine Learning Frameworks . Running jobs on Derecho \u00b6 Users can run a variety of types of jobs on Derecho, including both traditional batch jobs submitted through PBS and also interactive and/or graphics-intensive analysis, often through remote desktops on Derecho. Job scripts \u00b6 Job scripts are discussed broadly here . Users already familiar with PBS and batch submission may find Derecho-specific PBS job scripts helpful in porting their work. Derecho Hardware \u00b6 323,712 processor cores 3 rd Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket 256 GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket 512 GB DDR4 memory per node 4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU 600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs 64 cores per socket 512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs 32 cores per socket 2 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes 42 TB DDR4 memory on 82 GPU nodes 13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology, 200 Gb/sec per port per direction 1.7-2.6 usec MPI latency CPU-only nodes - one Slingshot injection port GPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's High Performance Computing Benchmarks run on each system. > 3.5 times Cheyenne peak performance 19.87 peak petaflops (vs 5.34) Status \u00b6 Nodes \u00b6 Queues \u00b6","title":"Derecho"},{"location":"compute-systems/derecho/#derecho","text":"Installed in 2023, Derecho is NCAR's latest supercomputer. Derecho features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below. Estimating Derecho Allocation Needs Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems.","title":"Derecho"},{"location":"compute-systems/derecho/#quick-start","text":"","title":"Quick Start"},{"location":"compute-systems/derecho/#logging-in","text":"Once you have an account , have reviewed the Derecho Use Policies , and have a Derecho resource allocation you can log in and run jobs on the Derecho data analysis and visualization cluster. To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@derecho.hpc.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can omit username in the command above if your Derecho username is the same as your username on your local computer. After running the ssh command, you will be asked to authenticate to finish logging in. Derecho has full access to NCAR storage resources , including GLADE . Users can transfer data to and from Derecho. To run data analysis and visualization jobs on the Derecho system's nodes, follow the procedures described here . There is no need to transfer output files from Derecho for this since Derecho and Casper mount the same GLADE file systems. Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.","title":"Logging in"},{"location":"compute-systems/derecho/#environment","text":"The Derecho HPC system uses a Cray variant of SUSE Enterprise Linux and supports widely used shells on its login and compute nodes. Users also have several compiler and MPI library choices.","title":"Environment"},{"location":"compute-systems/derecho/#shells","text":"The default login shell for new Derecho users is bash . You can change the default after logging in to the Systems Accounting Manager (SAM) . It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering echo $SHELL on your Derecho command line.","title":"Shells"},{"location":"compute-systems/derecho/#environment-modules","text":"The Derecho module utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. See the Environment modules page for a general discussion of module usage. Derecho's default module environment is listed here .","title":"Environment modules"},{"location":"compute-systems/derecho/#accessing-software-and-compiling-code","text":"Derecho users have access to Intel, NVIDIA, and GNU compilers. The Intel compiler and OpenMPI modules are loaded by default and provide access to pre-compiled HPC Software and Data Analysis and Visualization Resources . See this page for a full discussion of compiling on Derecho . Many Derecho data analysis and AI/ML workflows benefit instead from using Conda , especially NCAR's Python Library (NPL) or to gain access to several Machine Learning Frameworks .","title":"Accessing software and compiling code"},{"location":"compute-systems/derecho/#running-jobs-on-derecho","text":"Users can run a variety of types of jobs on Derecho, including both traditional batch jobs submitted through PBS and also interactive and/or graphics-intensive analysis, often through remote desktops on Derecho.","title":"Running jobs on Derecho"},{"location":"compute-systems/derecho/#job-scripts","text":"Job scripts are discussed broadly here . Users already familiar with PBS and batch submission may find Derecho-specific PBS job scripts helpful in porting their work.","title":"Job scripts"},{"location":"compute-systems/derecho/#derecho-hardware","text":"323,712 processor cores 3 rd Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket 256 GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket 512 GB DDR4 memory per node 4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU 600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs 64 cores per socket 512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs 32 cores per socket 2 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes 42 TB DDR4 memory on 82 GPU nodes 13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology, 200 Gb/sec per port per direction 1.7-2.6 usec MPI latency CPU-only nodes - one Slingshot injection port GPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's High Performance Computing Benchmarks run on each system. > 3.5 times Cheyenne peak performance 19.87 peak petaflops (vs 5.34)","title":"Derecho Hardware"},{"location":"compute-systems/derecho/#status","text":"","title":"Status"},{"location":"compute-systems/derecho/#nodes","text":"","title":"Nodes"},{"location":"compute-systems/derecho/#queues","text":"","title":"Queues"},{"location":"compute-systems/derecho/derecho-modules-list/","text":"Derecho default module list \u00b6 -------------------------- Module Stack Environments --------------------------- ncarenv/23.06 (S,L,D) ncarenv/23.09 (S) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.7 intel-classic/2023.0.0 arm-forge/22.1.3 intel-oneapi/2023.0.0 atp/3.14.18 intel/2023.0.0 (L) atp/3.15.0 (D) linaro-forge/23.0 cce/15.0.1 matlab/R2023a cdo/2.1.1 nccmp/1.9.0.1 charliecloud/0.32 ncl/6.6.2 cmake/3.26.3 nco/5.1.4 conda/latest ncview/2.1.8 cray-ccdb/4.12.13 ncvis/2022.08.28 cray-dyninst/12.1.1 nvhpc/21.3 cray-mrnet/5.0.4 nvhpc/23.1 (D) cray-stat/4.11.13 nvhpc/23.5 craype/2.7.20 (L) papi/7.0.0.1 cuda/11.7.1 pcre/8.45 cudnn/8.5.0.96-11.7 peak-memusage/3.0.1 cutensor/1.7.0.1 perftools-base/23.03.0 darshan-util/3.4.2 perl/5.36.0 gcc/12.2.0 podman/4.3.1 gdb4hpc/4.14.7 sanitizers4hpc/1.0.4 go/1.20.3 texlive/20220321 grads/2.2.1 valgrind4hpc/2.12.11 idl/8.9.0 vtune/2023.0.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- cray-libsci/23.02.1.1 intel-mpi/2021.8.0 cray-mpich/8.1.25 (L) mkl/2023.0.0 eccodes/2.25.0 mpi-serial/2.3.0 fftw/3.3.10 ncarcompilers/1.0.0 (L) geos/3.9.1 netcdf/4.9.2 (L) hdf/4.2.15 proj/8.2.1 hdf5/1.12.2 (L) udunits/2.2.28 ---------------- MPI-dependent Software - [oneapi + cray-mpich] ---------------- darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.0 esmf/8.4.2 netcdf-mpi/4.9.2 parallelio/2.6.1 esmf/8.5.0 (D) parallel-netcdf/1.12.3 parallelio/2.6.2 (D) fftw-mpi/3.3.10 parallelio/1.10.1 gptl/8.1.1 parallelio/2.5.10 Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Derecho complete module listing \u00b6 ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- apptainer: apptainer/1.1.7, apptainer/1.1.9 arm-forge: arm-forge/22.1.3 atp: atp/3.14.18, atp/3.15.0 cce: cce/15.0.1 cdo: cdo/2.1.1, cdo/2.2.2 charliecloud: charliecloud/0.32, charliecloud/0.33 cmake: cmake/3.26.3 conda: conda/latest cp2k: cp2k/2023.2 cray-ccdb: cray-ccdb/4.12.13 cray-dyninst: cray-dyninst/12.1.1 cray-libsci: cray-libsci/23.02.1.1 cray-mpich: cray-mpich/8.1.25 cray-mrnet: cray-mrnet/5.0.4 cray-stat: cray-stat/4.11.13 craype: craype/2.7.20 cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1 cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8 cutensor: cutensor/1.7.0.1 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 eccodes: eccodes/2.25.0 ecflow: ecflow/5.8.3 esmf: esmf/8.4.2, esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.6.4, gdal/3.7.1 gdb4hpc: gdb4hpc/4.14.7 geos: geos/3.9.1 go: go/1.20.3, go/1.20.6 gptl: gptl/8.1.1 grads: grads/2.2.1, grads/2.2.3 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 idl: idl/8.9.0 intel: intel/2023.0.0, intel/2023.2.1 intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1 intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0 intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 matlab: matlab/R2023a mkl: mkl/2023.0.0, mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 mpifileutils: mpifileutils/0.11.1 mvapich: mvapich/3.0b ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.06, ncarenv/23.09 nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.4, nco/5.1.6 ncview: ncview/2.1.8, ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7 osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ... papi: papi/7.0.0.1 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ... pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perftools: perftools perftools-base: perftools-base/23.03.0 perftools-lite: perftools-lite perftools-lite-events: perftools-lite-events perftools-lite-gpu: perftools-lite-gpu perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload perl: perl/5.36.0, perl/5.38.0 podman: podman/4.3.1, podman/4.5.1 proj: proj/8.2.1 rstudio: rstudio/2023.09.0 sanitizers4hpc: sanitizers4hpc/1.0.4 superlu: superlu/5.3.0 superlu-dist: superlu-dist/8.1.2 texlive: texlive/20220321 udunits: udunits/2.2.28 valgrind4hpc: valgrind4hpc/2.12.11 vtune: vtune/2023.0.0, vtune/2023.2.0 ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Derecho modules list"},{"location":"compute-systems/derecho/derecho-modules-list/#derecho-default-module-list","text":"-------------------------- Module Stack Environments --------------------------- ncarenv/23.06 (S,L,D) ncarenv/23.09 (S) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.7 intel-classic/2023.0.0 arm-forge/22.1.3 intel-oneapi/2023.0.0 atp/3.14.18 intel/2023.0.0 (L) atp/3.15.0 (D) linaro-forge/23.0 cce/15.0.1 matlab/R2023a cdo/2.1.1 nccmp/1.9.0.1 charliecloud/0.32 ncl/6.6.2 cmake/3.26.3 nco/5.1.4 conda/latest ncview/2.1.8 cray-ccdb/4.12.13 ncvis/2022.08.28 cray-dyninst/12.1.1 nvhpc/21.3 cray-mrnet/5.0.4 nvhpc/23.1 (D) cray-stat/4.11.13 nvhpc/23.5 craype/2.7.20 (L) papi/7.0.0.1 cuda/11.7.1 pcre/8.45 cudnn/8.5.0.96-11.7 peak-memusage/3.0.1 cutensor/1.7.0.1 perftools-base/23.03.0 darshan-util/3.4.2 perl/5.36.0 gcc/12.2.0 podman/4.3.1 gdb4hpc/4.14.7 sanitizers4hpc/1.0.4 go/1.20.3 texlive/20220321 grads/2.2.1 valgrind4hpc/2.12.11 idl/8.9.0 vtune/2023.0.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- cray-libsci/23.02.1.1 intel-mpi/2021.8.0 cray-mpich/8.1.25 (L) mkl/2023.0.0 eccodes/2.25.0 mpi-serial/2.3.0 fftw/3.3.10 ncarcompilers/1.0.0 (L) geos/3.9.1 netcdf/4.9.2 (L) hdf/4.2.15 proj/8.2.1 hdf5/1.12.2 (L) udunits/2.2.28 ---------------- MPI-dependent Software - [oneapi + cray-mpich] ---------------- darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.0 esmf/8.4.2 netcdf-mpi/4.9.2 parallelio/2.6.1 esmf/8.5.0 (D) parallel-netcdf/1.12.3 parallelio/2.6.2 (D) fftw-mpi/3.3.10 parallelio/1.10.1 gptl/8.1.1 parallelio/2.5.10 Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"Derecho default module list"},{"location":"compute-systems/derecho/derecho-modules-list/#derecho-complete-module-listing","text":"---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- apptainer: apptainer/1.1.7, apptainer/1.1.9 arm-forge: arm-forge/22.1.3 atp: atp/3.14.18, atp/3.15.0 cce: cce/15.0.1 cdo: cdo/2.1.1, cdo/2.2.2 charliecloud: charliecloud/0.32, charliecloud/0.33 cmake: cmake/3.26.3 conda: conda/latest cp2k: cp2k/2023.2 cray-ccdb: cray-ccdb/4.12.13 cray-dyninst: cray-dyninst/12.1.1 cray-libsci: cray-libsci/23.02.1.1 cray-mpich: cray-mpich/8.1.25 cray-mrnet: cray-mrnet/5.0.4 cray-stat: cray-stat/4.11.13 craype: craype/2.7.20 cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1 cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8 cutensor: cutensor/1.7.0.1 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 eccodes: eccodes/2.25.0 ecflow: ecflow/5.8.3 esmf: esmf/8.4.2, esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.6.4, gdal/3.7.1 gdb4hpc: gdb4hpc/4.14.7 geos: geos/3.9.1 go: go/1.20.3, go/1.20.6 gptl: gptl/8.1.1 grads: grads/2.2.1, grads/2.2.3 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 idl: idl/8.9.0 intel: intel/2023.0.0, intel/2023.2.1 intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1 intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0 intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 matlab: matlab/R2023a mkl: mkl/2023.0.0, mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 mpifileutils: mpifileutils/0.11.1 mvapich: mvapich/3.0b ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.06, ncarenv/23.09 nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.4, nco/5.1.6 ncview: ncview/2.1.8, ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7 osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ... papi: papi/7.0.0.1 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ... pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perftools: perftools perftools-base: perftools-base/23.03.0 perftools-lite: perftools-lite perftools-lite-events: perftools-lite-events perftools-lite-gpu: perftools-lite-gpu perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload perl: perl/5.36.0, perl/5.38.0 podman: podman/4.3.1, podman/4.5.1 proj: proj/8.2.1 rstudio: rstudio/2023.09.0 sanitizers4hpc: sanitizers4hpc/1.0.4 superlu: superlu/5.3.0 superlu-dist: superlu-dist/8.1.2 texlive: texlive/20220321 udunits: udunits/2.2.28 valgrind4hpc: valgrind4hpc/2.12.11 vtune: vtune/2023.0.0, vtune/2023.2.0 ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Derecho complete module listing"},{"location":"compute-systems/derecho/derecho-modules/","text":"Derecho Modules \u00b6 Derecho default module list \u00b6 -------------------------- Module Stack Environments --------------------------- ncarenv/23.06 (S,L,D) ncarenv/23.09 (S) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.7 intel-classic/2023.0.0 arm-forge/22.1.3 intel-oneapi/2023.0.0 atp/3.14.18 intel/2023.0.0 (L) atp/3.15.0 (D) linaro-forge/23.0 cce/15.0.1 matlab/R2023a cdo/2.1.1 nccmp/1.9.0.1 charliecloud/0.32 ncl/6.6.2 cmake/3.26.3 nco/5.1.4 conda/latest ncview/2.1.8 cray-ccdb/4.12.13 ncvis/2022.08.28 cray-dyninst/12.1.1 nvhpc/21.3 cray-mrnet/5.0.4 nvhpc/23.1 (D) cray-stat/4.11.13 nvhpc/23.5 craype/2.7.20 (L) papi/7.0.0.1 cuda/11.7.1 pcre/8.45 cudnn/8.5.0.96-11.7 peak-memusage/3.0.1 cutensor/1.7.0.1 perftools-base/23.03.0 darshan-util/3.4.2 perl/5.36.0 gcc/12.2.0 podman/4.3.1 gdb4hpc/4.14.7 sanitizers4hpc/1.0.4 go/1.20.3 texlive/20220321 grads/2.2.1 valgrind4hpc/2.12.11 idl/8.9.0 vtune/2023.0.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- cray-libsci/23.02.1.1 intel-mpi/2021.8.0 cray-mpich/8.1.25 (L) mkl/2023.0.0 eccodes/2.25.0 mpi-serial/2.3.0 fftw/3.3.10 ncarcompilers/1.0.0 (L) geos/3.9.1 netcdf/4.9.2 (L) hdf/4.2.15 proj/8.2.1 hdf5/1.12.2 (L) udunits/2.2.28 ---------------- MPI-dependent Software - [oneapi + cray-mpich] ---------------- darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.0 esmf/8.4.2 netcdf-mpi/4.9.2 parallelio/2.6.1 esmf/8.5.0 (D) parallel-netcdf/1.12.3 parallelio/2.6.2 (D) fftw-mpi/3.3.10 parallelio/1.10.1 gptl/8.1.1 parallelio/2.5.10 Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Derecho complete module listing \u00b6 ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- apptainer: apptainer/1.1.7, apptainer/1.1.9 arm-forge: arm-forge/22.1.3 atp: atp/3.14.18, atp/3.15.0 cce: cce/15.0.1 cdo: cdo/2.1.1, cdo/2.2.2 charliecloud: charliecloud/0.32, charliecloud/0.33 cmake: cmake/3.26.3 conda: conda/latest cp2k: cp2k/2023.2 cray-ccdb: cray-ccdb/4.12.13 cray-dyninst: cray-dyninst/12.1.1 cray-libsci: cray-libsci/23.02.1.1 cray-mpich: cray-mpich/8.1.25 cray-mrnet: cray-mrnet/5.0.4 cray-stat: cray-stat/4.11.13 craype: craype/2.7.20 cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1 cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8 cutensor: cutensor/1.7.0.1 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 eccodes: eccodes/2.25.0 ecflow: ecflow/5.8.3 esmf: esmf/8.4.2, esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.6.4, gdal/3.7.1 gdb4hpc: gdb4hpc/4.14.7 geos: geos/3.9.1 go: go/1.20.3, go/1.20.6 gptl: gptl/8.1.1 grads: grads/2.2.1, grads/2.2.3 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 idl: idl/8.9.0 intel: intel/2023.0.0, intel/2023.2.1 intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1 intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0 intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 matlab: matlab/R2023a mkl: mkl/2023.0.0, mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 mpifileutils: mpifileutils/0.11.1 mvapich: mvapich/3.0b ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.06, ncarenv/23.09 nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.4, nco/5.1.6 ncview: ncview/2.1.8, ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7 osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ... papi: papi/7.0.0.1 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ... pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perftools: perftools perftools-base: perftools-base/23.03.0 perftools-lite: perftools-lite perftools-lite-events: perftools-lite-events perftools-lite-gpu: perftools-lite-gpu perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload perl: perl/5.36.0, perl/5.38.0 podman: podman/4.3.1, podman/4.5.1 proj: proj/8.2.1 rstudio: rstudio/2023.09.0 sanitizers4hpc: sanitizers4hpc/1.0.4 superlu: superlu/5.3.0 superlu-dist: superlu-dist/8.1.2 texlive: texlive/20220321 udunits: udunits/2.2.28 valgrind4hpc: valgrind4hpc/2.12.11 vtune: vtune/2023.0.0, vtune/2023.2.0 ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Derecho Software"},{"location":"compute-systems/derecho/derecho-modules/#derecho-modules","text":"","title":"Derecho Modules"},{"location":"compute-systems/derecho/derecho-modules/#derecho-default-module-list","text":"-------------------------- Module Stack Environments --------------------------- ncarenv/23.06 (S,L,D) ncarenv/23.09 (S) ------------------------- Compilers and Core Software -------------------------- apptainer/1.1.7 intel-classic/2023.0.0 arm-forge/22.1.3 intel-oneapi/2023.0.0 atp/3.14.18 intel/2023.0.0 (L) atp/3.15.0 (D) linaro-forge/23.0 cce/15.0.1 matlab/R2023a cdo/2.1.1 nccmp/1.9.0.1 charliecloud/0.32 ncl/6.6.2 cmake/3.26.3 nco/5.1.4 conda/latest ncview/2.1.8 cray-ccdb/4.12.13 ncvis/2022.08.28 cray-dyninst/12.1.1 nvhpc/21.3 cray-mrnet/5.0.4 nvhpc/23.1 (D) cray-stat/4.11.13 nvhpc/23.5 craype/2.7.20 (L) papi/7.0.0.1 cuda/11.7.1 pcre/8.45 cudnn/8.5.0.96-11.7 peak-memusage/3.0.1 cutensor/1.7.0.1 perftools-base/23.03.0 darshan-util/3.4.2 perl/5.36.0 gcc/12.2.0 podman/4.3.1 gdb4hpc/4.14.7 sanitizers4hpc/1.0.4 go/1.20.3 texlive/20220321 grads/2.2.1 valgrind4hpc/2.12.11 idl/8.9.0 vtune/2023.0.0 -------------------- Compiler-dependent Software - [oneapi] -------------------- cray-libsci/23.02.1.1 intel-mpi/2021.8.0 cray-mpich/8.1.25 (L) mkl/2023.0.0 eccodes/2.25.0 mpi-serial/2.3.0 fftw/3.3.10 ncarcompilers/1.0.0 (L) geos/3.9.1 netcdf/4.9.2 (L) hdf/4.2.15 proj/8.2.1 hdf5/1.12.2 (L) udunits/2.2.28 ---------------- MPI-dependent Software - [oneapi + cray-mpich] ---------------- darshan-runtime/3.4.2 hdf5-mpi/1.12.2 parallelio/2.6.0 esmf/8.4.2 netcdf-mpi/4.9.2 parallelio/2.6.1 esmf/8.5.0 (D) parallel-netcdf/1.12.3 parallelio/2.6.2 (D) fftw-mpi/3.3.10 parallelio/1.10.1 gptl/8.1.1 parallelio/2.5.10 Where: D: Default Module L: Module is loaded S: Module is Sticky, requires --force to unload or purge If the avail list is too long consider trying: \"module --default avail\" or \"ml -d av\" to just list the default modules. \"module overview\" or \"ml ov\" to display the number of modules for each name. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"Derecho default module list"},{"location":"compute-systems/derecho/derecho-modules/#derecho-complete-module-listing","text":"---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- apptainer: apptainer/1.1.7, apptainer/1.1.9 arm-forge: arm-forge/22.1.3 atp: atp/3.14.18, atp/3.15.0 cce: cce/15.0.1 cdo: cdo/2.1.1, cdo/2.2.2 charliecloud: charliecloud/0.32, charliecloud/0.33 cmake: cmake/3.26.3 conda: conda/latest cp2k: cp2k/2023.2 cray-ccdb: cray-ccdb/4.12.13 cray-dyninst: cray-dyninst/12.1.1 cray-libsci: cray-libsci/23.02.1.1 cray-mpich: cray-mpich/8.1.25 cray-mrnet: cray-mrnet/5.0.4 cray-stat: cray-stat/4.11.13 craype: craype/2.7.20 cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1 cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8 cutensor: cutensor/1.7.0.1 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 eccodes: eccodes/2.25.0 ecflow: ecflow/5.8.3 esmf: esmf/8.4.2, esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.6.4, gdal/3.7.1 gdb4hpc: gdb4hpc/4.14.7 geos: geos/3.9.1 go: go/1.20.3, go/1.20.6 gptl: gptl/8.1.1 grads: grads/2.2.1, grads/2.2.3 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 idl: idl/8.9.0 intel: intel/2023.0.0, intel/2023.2.1 intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1 intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0 intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 matlab: matlab/R2023a mkl: mkl/2023.0.0, mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 mpifileutils: mpifileutils/0.11.1 mvapich: mvapich/3.0b ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.06, ncarenv/23.09 nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.4, nco/5.1.6 ncview: ncview/2.1.8, ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7 osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ... papi: papi/7.0.0.1 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ... pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perftools: perftools perftools-base: perftools-base/23.03.0 perftools-lite: perftools-lite perftools-lite-events: perftools-lite-events perftools-lite-gpu: perftools-lite-gpu perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload perl: perl/5.36.0, perl/5.38.0 podman: podman/4.3.1, podman/4.5.1 proj: proj/8.2.1 rstudio: rstudio/2023.09.0 sanitizers4hpc: sanitizers4hpc/1.0.4 superlu: superlu/5.3.0 superlu-dist: superlu-dist/8.1.2 texlive: texlive/20220321 udunits: udunits/2.2.28 valgrind4hpc: valgrind4hpc/2.12.11 vtune: vtune/2023.0.0, vtune/2023.2.0 ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ----------------------------------------------------------------------------","title":"Derecho complete module listing"},{"location":"compute-systems/derecho/derecho-use-policies/","text":"System use policies \u00b6 Appropriate use of login nodes \u00b6 Users may run short, non-memory-intensive processes interactively on the Derecho system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster. Fair share policy \u00b6 CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, the Antarctic Mesoscale Prediction System (AMPS) community, and the Wyoming community. The fair-share policy takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions. Job scheduling priorities \u00b6 The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following: Job priority (user selectable) Fair share factor Eligible time in queue Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: The necessary resources are not yet available. The system has been reserved for a scheduled outage. The job has been placed on hold. You have reached your concurrent core-usage limit when using the develop queue. A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job. PBS sorting factors \u00b6 Stakeholder shares and fair-share factor \u00b6 CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half. Job priority \u00b6 Users can set job priority to one of three values. Jobs with higher priority are charged against the user's allocation at higher rates than others. Job priority Priority order Priority factor Description premium 1 1.5 Jobs are charged at 150% of the regular rate. regular 2 1 All production jobs default to this priority. economy 3 0.7 Production batch jobs are charged at 70% of regular rate. preempt 4 0.2 Automatically selected when job is submitted to preempt queue. Job size \u00b6 Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start. GPU usage \u00b6 In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected. Backfilling \u00b6 When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs. Preemption \u00b6 Derecho has a preemption routing queue that can be used to submit jobs that will run when the required resources are not in use for higher-priority work in the main or develop execution queues. In order to take advantage of preemption, submit your job to the preempt routing queue and it job will run when the necessary resources are available. When resources for any preempt jobs are needed by higher-priority work, the scheduler sends a SIGTERM signal that can be detected by your job. After the SIGTERM signal is sent to the job, there is a five-minute window in which the job has a chance to checkpoint or save any work that was accomplished. After the five-minute window, the job will be killed by the scheduler and deleted. See this page for additional preemption details.","title":"Derecho Use Policies"},{"location":"compute-systems/derecho/derecho-use-policies/#system-use-policies","text":"","title":"System use policies"},{"location":"compute-systems/derecho/derecho-use-policies/#appropriate-use-of-login-nodes","text":"Users may run short, non-memory-intensive processes interactively on the Derecho system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster.","title":"Appropriate use of login nodes"},{"location":"compute-systems/derecho/derecho-use-policies/#fair-share-policy","text":"CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, the Antarctic Mesoscale Prediction System (AMPS) community, and the Wyoming community. The fair-share policy takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions.","title":"Fair share policy"},{"location":"compute-systems/derecho/derecho-use-policies/#job-scheduling-priorities","text":"The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following: Job priority (user selectable) Fair share factor Eligible time in queue Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: The necessary resources are not yet available. The system has been reserved for a scheduled outage. The job has been placed on hold. You have reached your concurrent core-usage limit when using the develop queue. A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.","title":"Job scheduling priorities"},{"location":"compute-systems/derecho/derecho-use-policies/#pbs-sorting-factors","text":"","title":"PBS sorting factors"},{"location":"compute-systems/derecho/derecho-use-policies/#stakeholder-shares-and-fair-share-factor","text":"CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half.","title":"Stakeholder shares and fair-share factor"},{"location":"compute-systems/derecho/derecho-use-policies/#job-priority","text":"Users can set job priority to one of three values. Jobs with higher priority are charged against the user's allocation at higher rates than others. Job priority Priority order Priority factor Description premium 1 1.5 Jobs are charged at 150% of the regular rate. regular 2 1 All production jobs default to this priority. economy 3 0.7 Production batch jobs are charged at 70% of regular rate. preempt 4 0.2 Automatically selected when job is submitted to preempt queue.","title":"Job priority"},{"location":"compute-systems/derecho/derecho-use-policies/#job-size","text":"Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start.","title":"Job size"},{"location":"compute-systems/derecho/derecho-use-policies/#gpu-usage","text":"In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected.","title":"GPU usage"},{"location":"compute-systems/derecho/derecho-use-policies/#backfilling","text":"When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.","title":"Backfilling"},{"location":"compute-systems/derecho/derecho-use-policies/#preemption","text":"Derecho has a preemption routing queue that can be used to submit jobs that will run when the required resources are not in use for higher-priority work in the main or develop execution queues. In order to take advantage of preemption, submit your job to the preempt routing queue and it job will run when the necessary resources are available. When resources for any preempt jobs are needed by higher-priority work, the scheduler sends a SIGTERM signal that can be detected by your job. After the SIGTERM signal is sent to the job, there is a five-minute window in which the job has a chance to checkpoint or save any work that was accomplished. After the five-minute window, the job will be killed by the scheduler and deleted. See this page for additional preemption details.","title":"Preemption"},{"location":"compute-systems/derecho/moving-from-cheyenne/","text":"This page is intended to provide a high-level comparison of Derecho to Cheyenne, particularly for users comfortable with Cheyenne operations and looking for a reference guide for transitioning workflows. Processes & Procedures for Derecho vs. Cheyenne \u00b6 Allocations : Users must have a project allocation to use Derecho. Derecho CPU and GPU resources are allocated as separate entities. Logging in : ssh is used similarly to access both systems. Please note for Derecho the host name is derecho .hpc. ucar.edu ssh access to Derecho vs. Cheyenne Derecho Cheyenne The fully-qualified domain name (FQDN) for Derecho is derecho.hpc.ucar.edu . ssh username@derecho.hpc.ucar.edu The fully-qualified domain name (FQDN) for Cheyenne is cheyenne.ucar.edu . ssh username@cheyenne.ucar.edu Home and Work file spaces : Derecho and Cheyenne share the work spaces /glade/u/home/${USER} and /glade/work/${USER} . No action is necessary to migrate or synchronize these file spaces. Scratch file space : Users are allocated 30TB data / 12 Million files of scratch quota at /glade/derecho/scratch/${USER} . Moving Files from Cheyenne : Derecho and Cheyenne have distinct scratch files systems. Users may access their Cheyenne scratch contents on Derecho from /glade/cheyenne/scratch/${USER} . No attempt is made to automatically transfer contents between the systems, as scratch storage is intended to be temporary and tied to specific job workflows. (Users interested in duplicating contents between systems on their own may follow instructions here .) Project file space : The Campaign Storage file system is mounted across all of Derecho and Cheyenne, and should be used for shared project storage. The Cheyenne project spaces /glade/p/ and /glade/collections/ are deprecated and are being migrated into /glade/campaign/ as part of the Cheyenne decommissioning process. NCAR Labs are responsible for moving their contents from these deprecated spaces, while migration for active University projects will be performed automatically. Compiling : Derecho maintains the intel compilers as default. Additional compilers are available through the module system, occasionally with different naming conventions. In general the compilers on Derecho are much newer than their counterparts on Cheyenne, for example the default version of the GNU Compiler Collection on Derecho is accessible via the gcc/12.2.0 module, whereas Cheyenne's default is gnu/10.1.0 . Check CPU architecture flags!! When recompiling users should pay special attention to any architecture-specific CPU optimization flags. Derecho uses AMD Milan processors, which perform well with code compiled with -march=core-avx2 under Intel and GCC. DO use on Derecho : -march=core-avx2 Be especially careful not to use some other flags commonly used on Cheyenne, as these will create sub-optimal code that will run slower than otherwise possible, or may fail to execute altogether. Do NOT use on Derecho : -xHost , -axHost , -xCORE-AVX2 , -axCORE-AVX2 What's the difference between the intel , intel-oneapi , intel-classic modules? Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system. Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family. During this process both sets of compilers are available, but through different commands under different module selections: Module Fortran C C++ intel-classic ifort icc icpc intel-oneapi ifx icx icpx intel (default) ifort icx icpx The intel-classic module makes the familiar ifort/icc/icpc compilers available, however it is expected these will be deprecated during Casper's lifetime. At this stage we expect to keep existing compiler versions available, however there will be no further updates. The intel-oneapi module uses the new ifx/icx/icpx compilers. The default intel module presently uses the older ifort Fortran compiler along with the newer icx/icpx C/C++ compilers. This choice is intentional as the newer ifx does not reliably match the performance of ifort in all cases. We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future. Users migrating particularly old code bases from Cheyenne may wish to try the intel-classic compilers if intel causes excessive warnings that are not easily resolved. The intel-classic compilers are less stringent with certain standards compliance checks. Note however this is not a permanent solution, as Intel will deprecate the Classic series at some point in Derecho's lifespan. ncarcompilers and linking with libraries : We continue to recommend the use of the ncarcompilers module to facilitate compilation, particularly when using 3 rd -party libraries through the module system. One notable change on Derecho is that ncarcompilers no longer assumes which combination of -l<package_libraries> the user desires, necessitating manual selection. For example, to link a simple Fortran application with NetCDF, the process is slightly changed on Derecho vs. Cheyenne: ncarcompilers on Derecho requires specifying particular package libraries The ncarcompilers module greatly facilitates linking to 3 rd party libraries by injecting module -specific linker library search paths into executables. This results in executables that can find their dependencies without relying on environment variables at execution time. Derecho Cheyenne Compiling a simple Fortran program that uses NetCDF requires specifying which -l<package_libs> are required: module load netcdf ifort -o foo.exe foo.f90 -lnetcdf This change allows for broader support of packages that introduce multiple libraries, resolving ambiguity in the users' intent. Compiling a simple Fortran program that uses NetCDF on Cheyenne assumes which -l<package_libs> are required, and this detail is hidden from the user: module load netcdf ifort -o foo.exe foo.f90 PBS Job Submission : PBS is use to launch jobs on Derecho similar to Cheyenne. Job submission and monitoring via qsub , qstat , qdel etc... are similar between the systems. Queues : On Derecho the default PBS queue main takes the place of the three queues regular , premium , and economy on Cheyenne. Job Priority : On Derecho users request a specific job priority via the #PBS -l job_priority=<regular|premium|economy> resource directive, instead of through distinct queues. select statements for CPU jobs : Derecho CPU nodes have 128 cores. A typical PBS resource selection statement is #PBS -l select=10:ncpus=128:mpiprocs=128:ompthreads=1 . Update your select statements! Derecho CPU nodes have 128 cores, and are generally assigned exclusively. This means you will be charged for all 128 cores on the node, regardless of how many you use. Do not simply copy your old select statements from Cheyenne - doing so will under-utilize the CPU nodes, and you will be charged for the full resource regardless of your usage! Memory : Each CPU node has a maximum of 235GB of RAM available for user jobs. Derecho CPU nodes are all identical - there is no notion of largemem or smallmem CPU nodes. Users requiring more memory per core than the 235GB/128 default configuration allows will need to under-subscribe CPU nodes, that is, leave some cores idle in order to increase the effective memory-per-utilized-core. MPI Environment : Derecho and Cheyenne differ significantly in their default MPI configurations. Derecho uses cray-mpich by default, vs. Cheyenne's Message Passing Toolkit (MPT) implementation. Launching jobs with mpiexec : Derecho uses the mpiexec launcher specified by the MPI standard. (There is no mpiexec_mpt on Derecho.) Additional command-line arguments are required to specify processor layout for mpiexec : mpiexec -n <# Total Ranks> -ppn <# Ranks Per Node > ./executable see man mpiexec on Derecho for additional details. MPI Environment Variables : Any users leveraging MPT-specific environment variables in their job scripts to change default behavior should first test their application with default configurations to determine if such approaches are still necessary, and if so will need to find cray-mpich equivalents - see man intro_mpi on Derecho or reach out to consulting for assistance. Process binding : Derecho does not use the dmplace or omplace utilities found on Cheyenne for process binding, requiring instead binding selections to be specified through mpiexec . For additional details and examples see Derecho PBS Script Examples and the discussion of the --cpu-bind option in the mpiexec manual page ( man mpiexec on Derecho). cron automation : Some users leverage cron on Cheyenne to automate workflows. NCAR/CISL has deployed a new cron service independent of the HPC systems. This separated, high-availability solution allows us to perform maintenance on the HPC resources while not interrupting cron workflows that can tolerate the downtime. Additional details are here . Going Further \u00b6 Much more information on Derecho hardware, software, and general user environment can be found in the Introduction to the Derecho Supercomputer training slides. Comparison References \u00b6 User Environment Comparison \u00b6 Click on the image below for a detailed comparison of Cheyenne and Derecho key user environment differences. Hardware Comparison \u00b6 Click on the image below for a detailed comparison of Cheyenne and Derecho hardware.","title":"Moving to Derecho from Cheyenne"},{"location":"compute-systems/derecho/moving-from-cheyenne/#processes-procedures-for-derecho-vs-cheyenne","text":"Allocations : Users must have a project allocation to use Derecho. Derecho CPU and GPU resources are allocated as separate entities. Logging in : ssh is used similarly to access both systems. Please note for Derecho the host name is derecho .hpc. ucar.edu ssh access to Derecho vs. Cheyenne Derecho Cheyenne The fully-qualified domain name (FQDN) for Derecho is derecho.hpc.ucar.edu . ssh username@derecho.hpc.ucar.edu The fully-qualified domain name (FQDN) for Cheyenne is cheyenne.ucar.edu . ssh username@cheyenne.ucar.edu Home and Work file spaces : Derecho and Cheyenne share the work spaces /glade/u/home/${USER} and /glade/work/${USER} . No action is necessary to migrate or synchronize these file spaces. Scratch file space : Users are allocated 30TB data / 12 Million files of scratch quota at /glade/derecho/scratch/${USER} . Moving Files from Cheyenne : Derecho and Cheyenne have distinct scratch files systems. Users may access their Cheyenne scratch contents on Derecho from /glade/cheyenne/scratch/${USER} . No attempt is made to automatically transfer contents between the systems, as scratch storage is intended to be temporary and tied to specific job workflows. (Users interested in duplicating contents between systems on their own may follow instructions here .) Project file space : The Campaign Storage file system is mounted across all of Derecho and Cheyenne, and should be used for shared project storage. The Cheyenne project spaces /glade/p/ and /glade/collections/ are deprecated and are being migrated into /glade/campaign/ as part of the Cheyenne decommissioning process. NCAR Labs are responsible for moving their contents from these deprecated spaces, while migration for active University projects will be performed automatically. Compiling : Derecho maintains the intel compilers as default. Additional compilers are available through the module system, occasionally with different naming conventions. In general the compilers on Derecho are much newer than their counterparts on Cheyenne, for example the default version of the GNU Compiler Collection on Derecho is accessible via the gcc/12.2.0 module, whereas Cheyenne's default is gnu/10.1.0 . Check CPU architecture flags!! When recompiling users should pay special attention to any architecture-specific CPU optimization flags. Derecho uses AMD Milan processors, which perform well with code compiled with -march=core-avx2 under Intel and GCC. DO use on Derecho : -march=core-avx2 Be especially careful not to use some other flags commonly used on Cheyenne, as these will create sub-optimal code that will run slower than otherwise possible, or may fail to execute altogether. Do NOT use on Derecho : -xHost , -axHost , -xCORE-AVX2 , -axCORE-AVX2 What's the difference between the intel , intel-oneapi , intel-classic modules? Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system. Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family. During this process both sets of compilers are available, but through different commands under different module selections: Module Fortran C C++ intel-classic ifort icc icpc intel-oneapi ifx icx icpx intel (default) ifort icx icpx The intel-classic module makes the familiar ifort/icc/icpc compilers available, however it is expected these will be deprecated during Casper's lifetime. At this stage we expect to keep existing compiler versions available, however there will be no further updates. The intel-oneapi module uses the new ifx/icx/icpx compilers. The default intel module presently uses the older ifort Fortran compiler along with the newer icx/icpx C/C++ compilers. This choice is intentional as the newer ifx does not reliably match the performance of ifort in all cases. We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future. Users migrating particularly old code bases from Cheyenne may wish to try the intel-classic compilers if intel causes excessive warnings that are not easily resolved. The intel-classic compilers are less stringent with certain standards compliance checks. Note however this is not a permanent solution, as Intel will deprecate the Classic series at some point in Derecho's lifespan. ncarcompilers and linking with libraries : We continue to recommend the use of the ncarcompilers module to facilitate compilation, particularly when using 3 rd -party libraries through the module system. One notable change on Derecho is that ncarcompilers no longer assumes which combination of -l<package_libraries> the user desires, necessitating manual selection. For example, to link a simple Fortran application with NetCDF, the process is slightly changed on Derecho vs. Cheyenne: ncarcompilers on Derecho requires specifying particular package libraries The ncarcompilers module greatly facilitates linking to 3 rd party libraries by injecting module -specific linker library search paths into executables. This results in executables that can find their dependencies without relying on environment variables at execution time. Derecho Cheyenne Compiling a simple Fortran program that uses NetCDF requires specifying which -l<package_libs> are required: module load netcdf ifort -o foo.exe foo.f90 -lnetcdf This change allows for broader support of packages that introduce multiple libraries, resolving ambiguity in the users' intent. Compiling a simple Fortran program that uses NetCDF on Cheyenne assumes which -l<package_libs> are required, and this detail is hidden from the user: module load netcdf ifort -o foo.exe foo.f90 PBS Job Submission : PBS is use to launch jobs on Derecho similar to Cheyenne. Job submission and monitoring via qsub , qstat , qdel etc... are similar between the systems. Queues : On Derecho the default PBS queue main takes the place of the three queues regular , premium , and economy on Cheyenne. Job Priority : On Derecho users request a specific job priority via the #PBS -l job_priority=<regular|premium|economy> resource directive, instead of through distinct queues. select statements for CPU jobs : Derecho CPU nodes have 128 cores. A typical PBS resource selection statement is #PBS -l select=10:ncpus=128:mpiprocs=128:ompthreads=1 . Update your select statements! Derecho CPU nodes have 128 cores, and are generally assigned exclusively. This means you will be charged for all 128 cores on the node, regardless of how many you use. Do not simply copy your old select statements from Cheyenne - doing so will under-utilize the CPU nodes, and you will be charged for the full resource regardless of your usage! Memory : Each CPU node has a maximum of 235GB of RAM available for user jobs. Derecho CPU nodes are all identical - there is no notion of largemem or smallmem CPU nodes. Users requiring more memory per core than the 235GB/128 default configuration allows will need to under-subscribe CPU nodes, that is, leave some cores idle in order to increase the effective memory-per-utilized-core. MPI Environment : Derecho and Cheyenne differ significantly in their default MPI configurations. Derecho uses cray-mpich by default, vs. Cheyenne's Message Passing Toolkit (MPT) implementation. Launching jobs with mpiexec : Derecho uses the mpiexec launcher specified by the MPI standard. (There is no mpiexec_mpt on Derecho.) Additional command-line arguments are required to specify processor layout for mpiexec : mpiexec -n <# Total Ranks> -ppn <# Ranks Per Node > ./executable see man mpiexec on Derecho for additional details. MPI Environment Variables : Any users leveraging MPT-specific environment variables in their job scripts to change default behavior should first test their application with default configurations to determine if such approaches are still necessary, and if so will need to find cray-mpich equivalents - see man intro_mpi on Derecho or reach out to consulting for assistance. Process binding : Derecho does not use the dmplace or omplace utilities found on Cheyenne for process binding, requiring instead binding selections to be specified through mpiexec . For additional details and examples see Derecho PBS Script Examples and the discussion of the --cpu-bind option in the mpiexec manual page ( man mpiexec on Derecho). cron automation : Some users leverage cron on Cheyenne to automate workflows. NCAR/CISL has deployed a new cron service independent of the HPC systems. This separated, high-availability solution allows us to perform maintenance on the HPC resources while not interrupting cron workflows that can tolerate the downtime. Additional details are here .","title":"Processes &amp; Procedures for Derecho vs. Cheyenne"},{"location":"compute-systems/derecho/moving-from-cheyenne/#going-further","text":"Much more information on Derecho hardware, software, and general user environment can be found in the Introduction to the Derecho Supercomputer training slides.","title":"Going Further"},{"location":"compute-systems/derecho/moving-from-cheyenne/#comparison-references","text":"","title":"Comparison References"},{"location":"compute-systems/derecho/moving-from-cheyenne/#user-environment-comparison","text":"Click on the image below for a detailed comparison of Cheyenne and Derecho key user environment differences.","title":"User Environment Comparison"},{"location":"compute-systems/derecho/moving-from-cheyenne/#hardware-comparison","text":"Click on the image below for a detailed comparison of Cheyenne and Derecho hardware.","title":"Hardware Comparison"},{"location":"compute-systems/derecho/compiling-code-on-derecho/","text":"Compiling code on Derecho \u00b6 Compilers available on Derecho \u00b6 Several C/C++ and Fortran compilers are available on all NCAR HPC systems. The information on this page applies to all of those systems except where noted. Compiler Language Commands for serial programs Commands for programs using MPI Flags to enable OpenMP Intel (Classic/OneAPI)* Fortran ifort / ifx mpif90 -qopenmp C icc / icx mpicc C++ icpc / icpx mpicxx NVIDIA HPC SDK Fortran nvfortran mpif90 -mp C nvc mpicc C++ nvc++ mpicxx GNU Compiler Collection (GCC) Fortran gfortran mpif90 -fopenmp C gcc mpicc C++ g++ mpicxx Cray Compiler (Derecho only)** Fortran ftn mpif90 -fopenmp C cc mpicc C++ CC mpicxx * Intel OneAPI is a cross-platform toolkit that supports C, C++, Fortran, and Python programming languages and replaces Intel Parallel Studio . Derecho supports both Intel OneAPI and Intel Classic Compilers. Intel is planning to retire the Intel Classic compilers and is moving toward Intel OneAPI. Intel Classic Compiler commands (ifort, icc, and icpc) will be replaced by the Intel OneAPI compilers (ifx, icx, and icpx). ** Please note that mpi wrappers are not available by default using Cray compilers but the ncarcompilers module will translate a call to \u201cmpicc\u201d to \u201ccc\u201d (and likewise for the other languages) as a convenience. Compiler Commands \u00b6 All supported compilers are available via the module utility. After loading the compiler module you want to use, refer to the table above to identify and run the appropriate compilation wrapper command. If your script already includes one of the following generic MPI commands, there is no need to change it: mpif90 , mpif77 , ftn * mpicc , cc * mpiCC , CC * *The wrappers ftn , cc , and CC are Cray-specific only available on Derecho. Build any libraries that you need to support an application with the same compiler, compiler version, and compatible flags used to compile the other parts of the application, including the main executable(s). Also, before you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will help you avoid job failures that can result from missing MPI launchers and library routines. Compiler man pages \u00b6 To refer to the man page for a compiler, log in to the system where you intend to use it, load the module, then execute man for the compiler. For example: module load nvhpc man nvfortran You can also use -help flags for a description of the command-line options for each compiler. Follow this example: ifort -help nvfortran -help [=option] Tip Use compiler diagnostic flags to identify potential problems while compiling the code. Changing compilers \u00b6 To change from one compiler to another, use module swap . In this example, you are switching from Intel to NVIDIA: module swap intel nvhpc When you load a compiler module or change to a different compiler, the system makes other compatible modules available. This helps you establish a working environment and avoid conflicts. If you need to link your program with a library, use module load to load the library as in this example: module load netcdf Then you can invoke the desired compilation command, including any library linking options such as -lnetcdf . Here's an example: mpif90-o foo.exe foo.f90 -lnetcdf Compiling CPU code \u00b6 Using the Cray compiler collection \u00b6 Derecho users have access to the Cray Compiling Environment (CCE) using the cce module. The compiler collection includes cc , CC , and ftn for compiling C, C++, and Fortran codes. To see which versions of the compiler are available, use the module avail command: module avail cce CCE base compilers are available by default in the ncarcompilers module. Loading the ncarcompilers module simplifies building code with dependencies such as netCDF. For example, compiling a simple Fortran code using netCDF is as follows with the compiler wrappers: ftn -o mybin -lnetcdff mycode.f90 Meanwhile, if you did not have the ncarcompilers module loaded, you would need to run the following command instead, with the linker flags and include-paths: ftn -I/path/to/netcdf/include -L/path/to/netcdf/lib -lnetcdff -o mybin mycode.f90 Using Cray MPICH MPI \u00b6 Unlike other MPI libraries, Cray MPICH does NOT provide MPI wrapper commands like mpicc , mpicxx , and mpif90 . Rather, use the same cc , CC , and ftn commands you use to compile a serial code. The Cray Programming Environment (CPE) will add MPI build flags to your commands whenever you have the cray-mpich module loaded. As many application build systems expect the MPI wrappers, our ncarcompilers module will translate a call to mpicc to cc (and likewise for the other languages) as a convenience, typically eliminating the need to alter pre-existing build scripts. Cray MPICH also supports GPU devices. If you are using an MPI application compiled with GPU support, enable CUDA functionality by loading a cuda module and setting or exporting this environment variable before calling the MPI launcher in your job by including this in your script: MPICH_GPU_SUPPORT_ENABLED = 1 Also, if your GPU-enabled MPI application makes use of managed memory, you also need to set this environment variable: MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 At runtime, you will also need to pass information about job parallelism to the mpiexec (or mpirun / aprun ) launcher because this information is not automatically taken from the PBS job script. You can pass this information by setting environment variables or by using mpiexec options. Full details of runtime settings for launching parallel programs can be found by running man mpiexec . The primary settings you will need are: the number of mpi ranks ( -n / PALS_NRANKS ) the number of ranks per node ( -ppn / PALS_PPN ) the number of OpenMP threads or CPUs to associate with each rank ( -d / PALS_DEPTH ) binding options ( --cpu-bind / PALS_CPU_BIND ) Tip Cray MPICH has many tunable parameters you can set through environment variables. Run man mpi for a complete listing of these environment variables. Example PBS select statements and corresponding MPI launch options are shown below for binding a hybrid MPI + OpenMP application (144 MPI ranks, and 4 OpenMP threads per MPI rank, which requires 5 nodes but does not fully subscribe the last node). Examples of both methods \u2013 setting environment variables and passing options to mpiexec \u2013 are provided. Environment variable example \u00b6 bash tcsh #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 export PALS_NRANKS = 144 export PALS_PPN = 32 export PALS_DEPTH = 4 export PALS_CPU_BIND = depth mpiexec ./a.out #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 setenv PALS_NRANKS 144 setenv PALS_PPN 32 setenv PALS_DEPTH 4 setenv PALS_CPU_BIND depth mpiexec ./a.out mpiexec options example \u00b6 bash tcsh #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 mpiexec --cpu-bind depth -n 144 -ppn 32 -d 4 ./a.out #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 mpiexec --cpu-bind depth -n 144 -ppn 32 -d 4 ./a.out Other compilers \u00b6 These additional compilers are available on Derecho. NVIDIA\u2019s HPC SDK Intel compilers the GNU Compiler Collection (GCC) When using non-Cray compilers, you can use either the compiler collection\u2019s own commands (e.g., ifort , nvfortran ) or the equivalent CPE command (e.g., ftn ) as long as you have loaded your desired compiler module. If you do not have the ncarcompilers module loaded and you are using the cray-mpich MPI, you will need to use a CPE command. Using Intel compilers \u00b6 The Intel compiler suite is available via the intel module. It includes compilers for C, C++, and Fortran codes. by default. To see which versions are available, use the module avail command. module avail intel To load the default Intel compiler, use module load without specifying a version. module load intel To load a different version, specify the version number when loading the module. Similarly, you can swap your current compiler module to Intel by using the module swap command. module swap cce/14.0.3 intel Extensive documentation for using the Intel compilers is available online here . To review the manual page for a compiler, run the man command for it as in this example: man ifort What's the difference between the intel , intel-oneapi , intel-classic modules? Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system. Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family. During this process both sets of compilers are available, but through different commands under different module selections: Module Fortran C C++ intel-classic ifort icc icpc intel-oneapi ifx icx icpx intel (default) ifort icx icpx The intel-classic module makes the familiar ifort/icc/icpc compilers available, however it is expected these will be deprecated during Casper's lifetime. At this stage we expect to keep existing compiler versions available, however there will be no further updates. The intel-oneapi module uses the new ifx/icx/icpx compilers. The default intel module presently uses the older ifort Fortran compiler along with the newer icx/icpx C/C++ compilers. This choice is intentional as the newer ifx does not reliably match the performance of ifort in all cases. We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future. Optimizing your code with Intel compilers \u00b6 Intel compilers provide several different optimization and vectorization options. By default, they use the -O2 option, which includes some optimizations. Using -O3 instead will provide more aggressive optimizations that may not improve the performance of some programs, while -O1 enables minimal optimization. A higher level of optimization might increase your compile time significantly. You can also disable any optimization by using -O0 . Be aware that compiling CPU code with the Intel compiler on Derecho is significantly different from using the Intel compiler on the Cheyenne system. Flags that are commonly used on Cheyenne might cause Derecho jobs to fail or run much more slowly than otherwise possible. CPU Architecture Flags to use on Derecho DO use on Derecho: -march=core-avx2 Do NOT use on Derecho: Do NOT use on Derecho: -xHost , -axHost , -xCORE-AVX2 , -axCORE-AVX2 These flags will generate code that may not run, or will run suboptimally on Derecho. Examples \u00b6 To compile and link a single Fortran program and create an executable, follow this example: ifort filename.f90 -o filename.exe To enable multi-threaded parallelization (OpenMP), include the -qopenmp flag as shown here: ifort -qopenmp filename.f90 -o filename.exe Compiling GPU code \u00b6 On Derecho, GPU applications should be built with either the Cray compilers or the NVIDIA HPC SDK compilers and libraries. In the following examples, we demonstrate the use of NVIDIA\u2019s tools. Additional compilation flags for GPU code will depend in large part on which GPU-programming paradigm is being used (e.g., OpenACC, OpenMP, CUDA) and which compiler collection you have loaded. The following examples show basic usage, but note that many customizations and optimizations are possible. You are encouraged to read the relevant man page for the compiler you choose. OpenACC \u00b6 To compile with OpenACC directives, simply add the -acc flag to your invocation of nvc, nvc++, or nvfortan. A Fortran example: nvfortran -o acc_bin -acc acc_code.f90 You can gather more insight into GPU acceleration decisions made by the compiler by adding -Minfo=accel to your invocation. Using compiler options, you can also specify which GPU architecture to target. This example will request compilation for both V100 (as on Casper) and A100 GPUs (as on Derecho): nvfortran -o acc_bin -acc -gpu = cc70,cc80 acc_code.f90 Specifying multiple acceleration targets will increase the size of the binary and the time it takes to compile the code. OpenMP \u00b6 Using OpenMP to offload code to the GPU is similar to using OpenACC. To compile a code with OpenMP offloading, use the -mp=gpu flag. The aforementioned diagnostic and target flags also apply to OpenMP offloading. nvfortran -o omp_gpu -mp = gpu omp.f90 CUDA \u00b6 The process for compiling CUDA code depends on whether you are using C++ or Fortran. For C++, the process often involves multiple stages in which you first use nvcc``, the NVIDIA CUDA compiler, and then your C++ compiler of choice. nvcc -c -arch = sm_80 cuda_code.cu g++ -o cuda_bin -lcuda -lcudart main.cpp cuda_code.o Using the nvcc` compiler driver with a non-NVIDIA C++ compiler requires loading a cuda environment module in addition to the compiler of choice. The compiler handles CUDA code directly, so the compiler you use must support CUDA. This means you should use nvfortran . If your source code file ends with the .cuf extension, nvfortran will enable CUDA automatically. Otherwise, you can specify the -Mcuda flag to the compiler. nvfortran -Mcuda -o cf_bin cf_code.f90 Native Compiler Commands \u00b6 We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly without the ncarcompilers wrappers, see this note: Native Compiler Commands We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly, unload the NCAR default compiler wrapper environment by entering this on your command line: module unload ncarcompilers You can still use the environment variables that are set by the modules that remain loaded, as shown in the following examples of invoking compilers directly to compile a Fortran program. Intel compiler NVIDIA HPC compiler GNU compiler collection (GCC) Cray Compilers (CPE) ifort -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> nvfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> gfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> ftn -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> Multiple Compiler Versions and User Applications \u00b6 In addition to multiple compilers, CISL keeps available multiple versions of libraries to accommodate a wide range of users' needs. Rather than rely on the environment variable LD_LIBRARY_PATH to find the correct libraries dynamically, we encode library paths within the binaries when you build Executable and Linkable Format (ELF) executables. To do this, we use RPATH rather than LD_LIBRARY_PATH to set the necessary paths to shared libraries. This enables your executable to work regardless of updates to new default versions of the various libraries; it doesn't have to search dynamically at run time to load them. It also means you don't need to worry about setting the variable or loading another module, greatly reducing the likelihood of runtime errors. Common Compiler Options and Diagnostic Flags \u00b6 Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable. Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags. Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly. The following options can be helpful as you compile code to run in the HPC environment that CISL manages. Compiler Flag Effect Cray Cray C/C++ debug options -G0 provide complete debugging information with optimizations disabled (i.e. -O0 , -O ipa0 , -O scale0 , -O vector0 ). Breakpoints can be set at different sections of the code for easier debugging. -G01 generate debugging report with partial optimization. -G02 generate debugging report with full optimization. -g generate debugging report (equivalent to -G0 ). -h bounds Enables checking of array bounds, pointer and array references at runtime. Intel Intel C++ diagnostic options -debug all provides complete debugging information. -g places symbolic debugging information in the executable program. -check all performs all runtime checks (includes bounds checking). -warn all enables all warnings. -stand f08 warns of usage that does not conform to the Fortran 2008 standard. -traceback enables stack trace if the program crashes. GCC GCC diagnostic warning ptions -ggdb places symbolic debugging information in the executable program for use by GDB. -fcheck=all performs all runtime checks (includes bounds checking). -Wall enables all warnings. -std=f2008 warns of usage that does not conform to the Fortran 2008 standard. NVIDIA HPC SDK NVIDIA HPC SDK documentation . -g Include symbolic debugging information in the object modules with optimization disabled ( -O0 ). -gopt Include symbolic debugging information in the object modules without affecting any optimizations. -C or -Mbounds Add array bounds checking. -Mchkptr Check for unintended de-referencing of NULL pointers. -Minform=inform Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase.","title":"Compiling Code on Derecho"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiling-code-on-derecho","text":"","title":"Compiling code on Derecho"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compilers-available-on-derecho","text":"Several C/C++ and Fortran compilers are available on all NCAR HPC systems. The information on this page applies to all of those systems except where noted. Compiler Language Commands for serial programs Commands for programs using MPI Flags to enable OpenMP Intel (Classic/OneAPI)* Fortran ifort / ifx mpif90 -qopenmp C icc / icx mpicc C++ icpc / icpx mpicxx NVIDIA HPC SDK Fortran nvfortran mpif90 -mp C nvc mpicc C++ nvc++ mpicxx GNU Compiler Collection (GCC) Fortran gfortran mpif90 -fopenmp C gcc mpicc C++ g++ mpicxx Cray Compiler (Derecho only)** Fortran ftn mpif90 -fopenmp C cc mpicc C++ CC mpicxx * Intel OneAPI is a cross-platform toolkit that supports C, C++, Fortran, and Python programming languages and replaces Intel Parallel Studio . Derecho supports both Intel OneAPI and Intel Classic Compilers. Intel is planning to retire the Intel Classic compilers and is moving toward Intel OneAPI. Intel Classic Compiler commands (ifort, icc, and icpc) will be replaced by the Intel OneAPI compilers (ifx, icx, and icpx). ** Please note that mpi wrappers are not available by default using Cray compilers but the ncarcompilers module will translate a call to \u201cmpicc\u201d to \u201ccc\u201d (and likewise for the other languages) as a convenience.","title":"Compilers available on Derecho"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiler-commands","text":"All supported compilers are available via the module utility. After loading the compiler module you want to use, refer to the table above to identify and run the appropriate compilation wrapper command. If your script already includes one of the following generic MPI commands, there is no need to change it: mpif90 , mpif77 , ftn * mpicc , cc * mpiCC , CC * *The wrappers ftn , cc , and CC are Cray-specific only available on Derecho. Build any libraries that you need to support an application with the same compiler, compiler version, and compatible flags used to compile the other parts of the application, including the main executable(s). Also, before you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will help you avoid job failures that can result from missing MPI launchers and library routines.","title":"Compiler Commands"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiler-man-pages","text":"To refer to the man page for a compiler, log in to the system where you intend to use it, load the module, then execute man for the compiler. For example: module load nvhpc man nvfortran You can also use -help flags for a description of the command-line options for each compiler. Follow this example: ifort -help nvfortran -help [=option] Tip Use compiler diagnostic flags to identify potential problems while compiling the code.","title":"Compiler man pages"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#changing-compilers","text":"To change from one compiler to another, use module swap . In this example, you are switching from Intel to NVIDIA: module swap intel nvhpc When you load a compiler module or change to a different compiler, the system makes other compatible modules available. This helps you establish a working environment and avoid conflicts. If you need to link your program with a library, use module load to load the library as in this example: module load netcdf Then you can invoke the desired compilation command, including any library linking options such as -lnetcdf . Here's an example: mpif90-o foo.exe foo.f90 -lnetcdf","title":"Changing compilers"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiling-cpu-code","text":"","title":"Compiling CPU code"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#using-the-cray-compiler-collection","text":"Derecho users have access to the Cray Compiling Environment (CCE) using the cce module. The compiler collection includes cc , CC , and ftn for compiling C, C++, and Fortran codes. To see which versions of the compiler are available, use the module avail command: module avail cce CCE base compilers are available by default in the ncarcompilers module. Loading the ncarcompilers module simplifies building code with dependencies such as netCDF. For example, compiling a simple Fortran code using netCDF is as follows with the compiler wrappers: ftn -o mybin -lnetcdff mycode.f90 Meanwhile, if you did not have the ncarcompilers module loaded, you would need to run the following command instead, with the linker flags and include-paths: ftn -I/path/to/netcdf/include -L/path/to/netcdf/lib -lnetcdff -o mybin mycode.f90","title":"Using the Cray compiler collection"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#using-cray-mpich-mpi","text":"Unlike other MPI libraries, Cray MPICH does NOT provide MPI wrapper commands like mpicc , mpicxx , and mpif90 . Rather, use the same cc , CC , and ftn commands you use to compile a serial code. The Cray Programming Environment (CPE) will add MPI build flags to your commands whenever you have the cray-mpich module loaded. As many application build systems expect the MPI wrappers, our ncarcompilers module will translate a call to mpicc to cc (and likewise for the other languages) as a convenience, typically eliminating the need to alter pre-existing build scripts. Cray MPICH also supports GPU devices. If you are using an MPI application compiled with GPU support, enable CUDA functionality by loading a cuda module and setting or exporting this environment variable before calling the MPI launcher in your job by including this in your script: MPICH_GPU_SUPPORT_ENABLED = 1 Also, if your GPU-enabled MPI application makes use of managed memory, you also need to set this environment variable: MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 At runtime, you will also need to pass information about job parallelism to the mpiexec (or mpirun / aprun ) launcher because this information is not automatically taken from the PBS job script. You can pass this information by setting environment variables or by using mpiexec options. Full details of runtime settings for launching parallel programs can be found by running man mpiexec . The primary settings you will need are: the number of mpi ranks ( -n / PALS_NRANKS ) the number of ranks per node ( -ppn / PALS_PPN ) the number of OpenMP threads or CPUs to associate with each rank ( -d / PALS_DEPTH ) binding options ( --cpu-bind / PALS_CPU_BIND ) Tip Cray MPICH has many tunable parameters you can set through environment variables. Run man mpi for a complete listing of these environment variables. Example PBS select statements and corresponding MPI launch options are shown below for binding a hybrid MPI + OpenMP application (144 MPI ranks, and 4 OpenMP threads per MPI rank, which requires 5 nodes but does not fully subscribe the last node). Examples of both methods \u2013 setting environment variables and passing options to mpiexec \u2013 are provided.","title":"Using Cray MPICH MPI"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#environment-variable-example","text":"bash tcsh #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 export PALS_NRANKS = 144 export PALS_PPN = 32 export PALS_DEPTH = 4 export PALS_CPU_BIND = depth mpiexec ./a.out #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 setenv PALS_NRANKS 144 setenv PALS_PPN 32 setenv PALS_DEPTH 4 setenv PALS_CPU_BIND depth mpiexec ./a.out","title":"Environment variable example"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#mpiexec-options-example","text":"bash tcsh #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 mpiexec --cpu-bind depth -n 144 -ppn 32 -d 4 ./a.out #PBS -l select=5:ncpus=128:mpiprocs=32:ompthreads=4 mpiexec --cpu-bind depth -n 144 -ppn 32 -d 4 ./a.out","title":"mpiexec options example"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#other-compilers","text":"These additional compilers are available on Derecho. NVIDIA\u2019s HPC SDK Intel compilers the GNU Compiler Collection (GCC) When using non-Cray compilers, you can use either the compiler collection\u2019s own commands (e.g., ifort , nvfortran ) or the equivalent CPE command (e.g., ftn ) as long as you have loaded your desired compiler module. If you do not have the ncarcompilers module loaded and you are using the cray-mpich MPI, you will need to use a CPE command.","title":"Other compilers"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#using-intel-compilers","text":"The Intel compiler suite is available via the intel module. It includes compilers for C, C++, and Fortran codes. by default. To see which versions are available, use the module avail command. module avail intel To load the default Intel compiler, use module load without specifying a version. module load intel To load a different version, specify the version number when loading the module. Similarly, you can swap your current compiler module to Intel by using the module swap command. module swap cce/14.0.3 intel Extensive documentation for using the Intel compilers is available online here . To review the manual page for a compiler, run the man command for it as in this example: man ifort What's the difference between the intel , intel-oneapi , intel-classic modules? Users migrating from Cheyenne and previous Casper deployments may note there are several \"flavors\" of the Intel compiler available through the module system. Intel is currently moving from their \"classic\" compiler suite to the new \"OneAPI\" family. During this process both sets of compilers are available, but through different commands under different module selections: Module Fortran C C++ intel-classic ifort icc icpc intel-oneapi ifx icx icpx intel (default) ifort icx icpx The intel-classic module makes the familiar ifort/icc/icpc compilers available, however it is expected these will be deprecated during Casper's lifetime. At this stage we expect to keep existing compiler versions available, however there will be no further updates. The intel-oneapi module uses the new ifx/icx/icpx compilers. The default intel module presently uses the older ifort Fortran compiler along with the newer icx/icpx C/C++ compilers. This choice is intentional as the newer ifx does not reliably match the performance of ifort in all cases. We will continue to monitor the progress of the OneAPI compilers and will change this behavior in the future.","title":"Using Intel compilers"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#optimizing-your-code-with-intel-compilers","text":"Intel compilers provide several different optimization and vectorization options. By default, they use the -O2 option, which includes some optimizations. Using -O3 instead will provide more aggressive optimizations that may not improve the performance of some programs, while -O1 enables minimal optimization. A higher level of optimization might increase your compile time significantly. You can also disable any optimization by using -O0 . Be aware that compiling CPU code with the Intel compiler on Derecho is significantly different from using the Intel compiler on the Cheyenne system. Flags that are commonly used on Cheyenne might cause Derecho jobs to fail or run much more slowly than otherwise possible. CPU Architecture Flags to use on Derecho DO use on Derecho: -march=core-avx2 Do NOT use on Derecho: Do NOT use on Derecho: -xHost , -axHost , -xCORE-AVX2 , -axCORE-AVX2 These flags will generate code that may not run, or will run suboptimally on Derecho.","title":"Optimizing your code with Intel compilers"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#examples","text":"To compile and link a single Fortran program and create an executable, follow this example: ifort filename.f90 -o filename.exe To enable multi-threaded parallelization (OpenMP), include the -qopenmp flag as shown here: ifort -qopenmp filename.f90 -o filename.exe","title":"Examples"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#compiling-gpu-code","text":"On Derecho, GPU applications should be built with either the Cray compilers or the NVIDIA HPC SDK compilers and libraries. In the following examples, we demonstrate the use of NVIDIA\u2019s tools. Additional compilation flags for GPU code will depend in large part on which GPU-programming paradigm is being used (e.g., OpenACC, OpenMP, CUDA) and which compiler collection you have loaded. The following examples show basic usage, but note that many customizations and optimizations are possible. You are encouraged to read the relevant man page for the compiler you choose.","title":"Compiling GPU code"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#openacc","text":"To compile with OpenACC directives, simply add the -acc flag to your invocation of nvc, nvc++, or nvfortan. A Fortran example: nvfortran -o acc_bin -acc acc_code.f90 You can gather more insight into GPU acceleration decisions made by the compiler by adding -Minfo=accel to your invocation. Using compiler options, you can also specify which GPU architecture to target. This example will request compilation for both V100 (as on Casper) and A100 GPUs (as on Derecho): nvfortran -o acc_bin -acc -gpu = cc70,cc80 acc_code.f90 Specifying multiple acceleration targets will increase the size of the binary and the time it takes to compile the code.","title":"OpenACC"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#openmp","text":"Using OpenMP to offload code to the GPU is similar to using OpenACC. To compile a code with OpenMP offloading, use the -mp=gpu flag. The aforementioned diagnostic and target flags also apply to OpenMP offloading. nvfortran -o omp_gpu -mp = gpu omp.f90","title":"OpenMP"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#cuda","text":"The process for compiling CUDA code depends on whether you are using C++ or Fortran. For C++, the process often involves multiple stages in which you first use nvcc``, the NVIDIA CUDA compiler, and then your C++ compiler of choice. nvcc -c -arch = sm_80 cuda_code.cu g++ -o cuda_bin -lcuda -lcudart main.cpp cuda_code.o Using the nvcc` compiler driver with a non-NVIDIA C++ compiler requires loading a cuda environment module in addition to the compiler of choice. The compiler handles CUDA code directly, so the compiler you use must support CUDA. This means you should use nvfortran . If your source code file ends with the .cuf extension, nvfortran will enable CUDA automatically. Otherwise, you can specify the -Mcuda flag to the compiler. nvfortran -Mcuda -o cf_bin cf_code.f90","title":"CUDA"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#native-compiler-commands","text":"We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly without the ncarcompilers wrappers, see this note: Native Compiler Commands We recommend using the module wrapper commands described above. However, if you prefer to invoke the compilers directly, unload the NCAR default compiler wrapper environment by entering this on your command line: module unload ncarcompilers You can still use the environment variables that are set by the modules that remain loaded, as shown in the following examples of invoking compilers directly to compile a Fortran program. Intel compiler NVIDIA HPC compiler GNU compiler collection (GCC) Cray Compilers (CPE) ifort -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> nvfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> gfortran -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library> ftn -o a.out $NCAR_INC_ <PACKAGE> program_name.f $NCAR_LDFLAGS_ <PACKAGE> -l<package_library>","title":"Native Compiler Commands"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#multiple-compiler-versions-and-user-applications","text":"In addition to multiple compilers, CISL keeps available multiple versions of libraries to accommodate a wide range of users' needs. Rather than rely on the environment variable LD_LIBRARY_PATH to find the correct libraries dynamically, we encode library paths within the binaries when you build Executable and Linkable Format (ELF) executables. To do this, we use RPATH rather than LD_LIBRARY_PATH to set the necessary paths to shared libraries. This enables your executable to work regardless of updates to new default versions of the various libraries; it doesn't have to search dynamically at run time to load them. It also means you don't need to worry about setting the variable or loading another module, greatly reducing the likelihood of runtime errors.","title":"Multiple Compiler Versions and User Applications"},{"location":"compute-systems/derecho/compiling-code-on-derecho/#common-compiler-options-and-diagnostic-flags","text":"Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable. Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags. Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly. The following options can be helpful as you compile code to run in the HPC environment that CISL manages. Compiler Flag Effect Cray Cray C/C++ debug options -G0 provide complete debugging information with optimizations disabled (i.e. -O0 , -O ipa0 , -O scale0 , -O vector0 ). Breakpoints can be set at different sections of the code for easier debugging. -G01 generate debugging report with partial optimization. -G02 generate debugging report with full optimization. -g generate debugging report (equivalent to -G0 ). -h bounds Enables checking of array bounds, pointer and array references at runtime. Intel Intel C++ diagnostic options -debug all provides complete debugging information. -g places symbolic debugging information in the executable program. -check all performs all runtime checks (includes bounds checking). -warn all enables all warnings. -stand f08 warns of usage that does not conform to the Fortran 2008 standard. -traceback enables stack trace if the program crashes. GCC GCC diagnostic warning ptions -ggdb places symbolic debugging information in the executable program for use by GDB. -fcheck=all performs all runtime checks (includes bounds checking). -Wall enables all warnings. -std=f2008 warns of usage that does not conform to the Fortran 2008 standard. NVIDIA HPC SDK NVIDIA HPC SDK documentation . -g Include symbolic debugging information in the object modules with optimization disabled ( -O0 ). -gopt Include symbolic debugging information in the object modules without affecting any optimizations. -C or -Mbounds Add array bounds checking. -Mchkptr Check for unintended de-referencing of NULL pointers. -Minform=inform Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase.","title":"Common Compiler Options and Diagnostic Flags"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/","text":"Compiler diagnostic flags \u00b6 Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable. Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags. Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly. The following options can be helpful as you compile code to run in the HPC environment that CISL manages. Cray \u00b6 The following compiler flags may be helpful for debugging your code: -G0 \u2013 provide complete debugging information with optimizations disabled (i.e. -O0 , -O ipa0 , -O scale0 , -O vector0 ). Breakpoints can be set at different sections of the code for easier debugging. -G01 \u2013 generate debugging report with partial optimization. -G02 \u2013 generate debugging report with full optimization. -g \u2013 generate debugging report (equivalent to -G0 ). -h bounds - Enables checking of array bounds, pointer and array references at runtime. Also see Cray C/C++ debug options . Intel \u00b6 -debug all \u2013 provides complete debugging information. -g \u2013 places symbolic debugging information in the executable program. -check all \u2013 performs all runtime checks (includes bounds checking). -warn all \u2013 enables all warnings. -stand f08 \u2013 warns of usage that does not conform to the Fortran 2008 standard. -traceback \u2013 enables stack trace if the program crashes. Also see Intel C++ diagnostic options . GNU \u00b6 -ggdb \u2013 places symbolic debugging information in the executable program for use by GDB. -fcheck=all \u2013 performs all runtime checks (includes bounds checking). -Wall \u2013 enables all warnings. -std=f2008 \u2013 warns of usage that does not conform to the Fortran 2008 standard. Also see GCC diagnostic warning options . NVIDIA HPC SDK \u00b6 The following compiler flags may be helpful for debugging your code using NVIDIA HPC SDK. -g \u2013 Include symbolic debugging information in the object modules with optimization disabled ( -O0 ). -gopt \u2013 Include symbolic debugging information in the object modules without affecting any optimizations. -C or -Mbounds \u2013 Add array bounds checking. -Mchkptr \u2013 Check for unintended de-referencing of NULL pointers. -Minform=inform - Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase. Also see NVIDIA HPC SDK documentation .","title":"Compiler diagnostic flags"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#compiler-diagnostic-flags","text":"Portability and correctness both are important goals when developing code. Non-standard code may not be portable, and its execution may be unpredictable. Using diagnostic options when you compile your code can help you find potential problems. Since the compiler is going to analyze your code anyway, it pays to take advantage of the diagnostic options to learn as much as you can from the analysis. Please note that some compilers disable the default optimization when you switch on certain debugging flags. Because of differences in compilers, it also is good practice to compile your code with each compiler that is available on the system, note any diagnostic messages you get, and revise your code accordingly. The following options can be helpful as you compile code to run in the HPC environment that CISL manages.","title":"Compiler diagnostic flags"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#cray","text":"The following compiler flags may be helpful for debugging your code: -G0 \u2013 provide complete debugging information with optimizations disabled (i.e. -O0 , -O ipa0 , -O scale0 , -O vector0 ). Breakpoints can be set at different sections of the code for easier debugging. -G01 \u2013 generate debugging report with partial optimization. -G02 \u2013 generate debugging report with full optimization. -g \u2013 generate debugging report (equivalent to -G0 ). -h bounds - Enables checking of array bounds, pointer and array references at runtime. Also see Cray C/C++ debug options .","title":"Cray"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#intel","text":"-debug all \u2013 provides complete debugging information. -g \u2013 places symbolic debugging information in the executable program. -check all \u2013 performs all runtime checks (includes bounds checking). -warn all \u2013 enables all warnings. -stand f08 \u2013 warns of usage that does not conform to the Fortran 2008 standard. -traceback \u2013 enables stack trace if the program crashes. Also see Intel C++ diagnostic options .","title":"Intel"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#gnu","text":"-ggdb \u2013 places symbolic debugging information in the executable program for use by GDB. -fcheck=all \u2013 performs all runtime checks (includes bounds checking). -Wall \u2013 enables all warnings. -std=f2008 \u2013 warns of usage that does not conform to the Fortran 2008 standard. Also see GCC diagnostic warning options .","title":"GNU"},{"location":"compute-systems/derecho/compiling-code-on-derecho/compiler-diagnostic-flags/#nvidia-hpc-sdk","text":"The following compiler flags may be helpful for debugging your code using NVIDIA HPC SDK. -g \u2013 Include symbolic debugging information in the object modules with optimization disabled ( -O0 ). -gopt \u2013 Include symbolic debugging information in the object modules without affecting any optimizations. -C or -Mbounds \u2013 Add array bounds checking. -Mchkptr \u2013 Check for unintended de-referencing of NULL pointers. -Minform=inform - Display all the error messages of any severity (inform, warn, severe and fatal) during compilation phase. Also see NVIDIA HPC SDK documentation .","title":"NVIDIA HPC SDK"},{"location":"compute-systems/derecho/starting-derecho-jobs/","text":"Starting Derecho jobs \u00b6 When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel cray-mpich If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/2023.0.0 cray-mpich/8.1.25 Script examples \u00b6 See this page for many Derecho PBS job script examples: Derecho job script examples When your script is ready, submit your batch job for scheduling as shown here .","title":"Starting Derecho jobs"},{"location":"compute-systems/derecho/starting-derecho-jobs/#starting-derecho-jobs","text":"When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel cray-mpich If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/2023.0.0 cray-mpich/8.1.25","title":"Starting Derecho jobs"},{"location":"compute-systems/derecho/starting-derecho-jobs/#script-examples","text":"See this page for many Derecho PBS job script examples: Derecho job script examples When your script is ready, submit your batch job for scheduling as shown here .","title":"Script examples"},{"location":"compute-systems/derecho/starting-derecho-jobs/derecho-job-script-examples-content/","text":"Running a hybrid CPU program with MPI and OpenMP on Derecho In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A <project_code> #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name Running an MPI-enabled GPU application on Derecho In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A <project_code> #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018set_gpu_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The set_gpu_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name Binding MPI ranks to CPU cores and GPU devices on Derecho For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: set_gpu_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name Running a containerized application under MPI on GPUs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" See here for a more complete discussion of the nuances of containerized applications on Derecho.","title":"Derecho job script examples content"},{"location":"compute-systems/derecho/starting-derecho-jobs/derecho-job-script-examples/","text":"Derecho batch job script examples \u00b6 When using these examples to create your own job scripts to run on Derecho, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. Running a hybrid CPU program with MPI and OpenMP on Derecho In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A <project_code> #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name Running an MPI-enabled GPU application on Derecho In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A <project_code> #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018set_gpu_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The set_gpu_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name Binding MPI ranks to CPU cores and GPU devices on Derecho For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: set_gpu_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name Running a containerized application under MPI on GPUs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" See here for a more complete discussion of the nuances of containerized applications on Derecho.","title":"Derecho batch job script examples"},{"location":"compute-systems/derecho/starting-derecho-jobs/derecho-job-script-examples/#derecho-batch-job-script-examples","text":"When using these examples to create your own job scripts to run on Derecho, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. Running a hybrid CPU program with MPI and OpenMP on Derecho In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A <project_code> #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name Running an MPI-enabled GPU application on Derecho In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A <project_code> #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018set_gpu_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The set_gpu_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name Binding MPI ranks to CPU cores and GPU devices on Derecho For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: set_gpu_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name Running a containerized application under MPI on GPUs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" See here for a more complete discussion of the nuances of containerized applications on Derecho.","title":"Derecho batch job script examples"},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/","text":"Process binding \u00b6 How to pin MPI processes to CPU cores depends on if the programs are pure MPI programs, hybrid MPI, or OpenMP programs. Pure MPI programs (MPI only, no threads) \u00b6 Hybrid MPI + OpenMP programs \u00b6","title":"Process Binding"},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/#process-binding","text":"How to pin MPI processes to CPU cores depends on if the programs are pure MPI programs, hybrid MPI, or OpenMP programs.","title":"Process binding"},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/#pure-mpi-programs-mpi-only-no-threads","text":"","title":"Pure MPI programs (MPI only, no threads)"},{"location":"compute-systems/derecho/starting-derecho-jobs/process-binding/#hybrid-mpi-openmp-programs","text":"","title":"Hybrid MPI + OpenMP programs"},{"location":"compute-systems/jupyterhub/","text":"JupyterHub at NCAR \u00b6 The JupyterHub deployment that CISL manages allows \"push-button\" access to NCAR's Cheyenne supercomputing resource and the Casper cluster of nodes used for data analysis and visualization, machine learning, and deep learning. It gives users the ability to create, save, and share Jupyter Notebooks through the JupyterLab interface and to run interactive, web-based analysis, visualization and compute jobs on Cheyenne and Casper. JupyterHub is an alternative to X11 access for interacting with those resources to run jobs as well as for using web-based interactive shell functionality without the need to install or use software such as SSH or PuTTY. Getting started \u00b6 Use your web browser to go to jupyterhub.hpc.ucar.edu . Chrome and Firefox are recommended for all users. Select Production . Log in with your NCAR username and Duo two-factor authentication, just as you would when logging directly in to either system. After you authenticate, you will be able to start a new default server or create a named server. (See following image.) You can have up to four named servers to use for accessing different compute resources. Do not start a new server simply to run additional notebooks! Do not start a new server simply to run additional notebooks; a single server can support multiple notebooks at once. However, executing the same notebook file in multiple servers concurrently can lead to kernel failures and other errors. After starting a server, select the cluster you want to use. You can choose to work on a login node or a batch node on either Casper or Cheyenne. If you choose a login node , launching the server will take you to the web interface. If you choose a batch node , use the form provided (images below) to specify your project code, set the necessary PBS job options, and launch the appropriate server. The name of your batch job will be STDIN . For more information about the options, see: Submitting jobs with PBS . Launch your job when ready. This job only gives you access to the JupyterLab instance. If you need more resources, you can launch another job or jobs from within JupyterLab. After launching the job, you will have access to multiple kernels in the web interface (image below) for working with various languages and applications. Where are my files? Note that the \u201cFile browser\u201d icon (upper-left of following image) allows you to explore your home directory only . To change to your scratch or work space, create soft links in your home directory to those locations. Python environments and kernels \u00b6 The JupyterLab dashboard provides access to Notebook and Console kernels, which are programming language interpreters. Available kernels, which change periodically as new releases are installed, include: Multiple Python 3 interpreters with varying package support including a basic install (Python 3), the Pangeo stack installed with conda (Pangeo), and the NCAR Python Library (NPL) that is also provided at the command-line by the conda environment module. R MATLAB Julia A Bash interpreter that provides a shell environment Related documentation \u00b6 See these related CISL documentation pages for additional support: Using Conda and Python","title":"JupyterHub at NCAR"},{"location":"compute-systems/jupyterhub/#jupyterhub-at-ncar","text":"The JupyterHub deployment that CISL manages allows \"push-button\" access to NCAR's Cheyenne supercomputing resource and the Casper cluster of nodes used for data analysis and visualization, machine learning, and deep learning. It gives users the ability to create, save, and share Jupyter Notebooks through the JupyterLab interface and to run interactive, web-based analysis, visualization and compute jobs on Cheyenne and Casper. JupyterHub is an alternative to X11 access for interacting with those resources to run jobs as well as for using web-based interactive shell functionality without the need to install or use software such as SSH or PuTTY.","title":"JupyterHub at NCAR"},{"location":"compute-systems/jupyterhub/#getting-started","text":"Use your web browser to go to jupyterhub.hpc.ucar.edu . Chrome and Firefox are recommended for all users. Select Production . Log in with your NCAR username and Duo two-factor authentication, just as you would when logging directly in to either system. After you authenticate, you will be able to start a new default server or create a named server. (See following image.) You can have up to four named servers to use for accessing different compute resources. Do not start a new server simply to run additional notebooks! Do not start a new server simply to run additional notebooks; a single server can support multiple notebooks at once. However, executing the same notebook file in multiple servers concurrently can lead to kernel failures and other errors. After starting a server, select the cluster you want to use. You can choose to work on a login node or a batch node on either Casper or Cheyenne. If you choose a login node , launching the server will take you to the web interface. If you choose a batch node , use the form provided (images below) to specify your project code, set the necessary PBS job options, and launch the appropriate server. The name of your batch job will be STDIN . For more information about the options, see: Submitting jobs with PBS . Launch your job when ready. This job only gives you access to the JupyterLab instance. If you need more resources, you can launch another job or jobs from within JupyterLab. After launching the job, you will have access to multiple kernels in the web interface (image below) for working with various languages and applications. Where are my files? Note that the \u201cFile browser\u201d icon (upper-left of following image) allows you to explore your home directory only . To change to your scratch or work space, create soft links in your home directory to those locations.","title":"Getting started"},{"location":"compute-systems/jupyterhub/#python-environments-and-kernels","text":"The JupyterLab dashboard provides access to Notebook and Console kernels, which are programming language interpreters. Available kernels, which change periodically as new releases are installed, include: Multiple Python 3 interpreters with varying package support including a basic install (Python 3), the Pangeo stack installed with conda (Pangeo), and the NCAR Python Library (NPL) that is also provided at the command-line by the conda environment module. R MATLAB Julia A Bash interpreter that provides a shell environment","title":"Python environments and kernels"},{"location":"compute-systems/jupyterhub/#related-documentation","text":"See these related CISL documentation pages for additional support: Using Conda and Python","title":"Related documentation"},{"location":"environment-and-software/","text":"Overview \u00b6 CISL deploys a wide range of preconfigured software packages for general use through several deployment mechanisms, accessible as described in the User Environment section. Specific tools and processes are further documented in the HPC Software section of this guide, with explicit instructions for using particular tools such as Matlab, Profilers & Debuggers, etc...","title":"Overview"},{"location":"environment-and-software/#overview","text":"CISL deploys a wide range of preconfigured software packages for general use through several deployment mechanisms, accessible as described in the User Environment section. Specific tools and processes are further documented in the HPC Software section of this guide, with explicit instructions for using particular tools such as Matlab, Profilers & Debuggers, etc...","title":"Overview"},{"location":"environment-and-software/community-models/","text":"Community models \u00b6 The models described below are available for use on NCAR computers that CISL manages. Please contact the NCAR Research Computing help desk if you need assistance. Community Earth System Model (CESM) \u2013 A fully coupled, global climate model developed at NCAR. CESM (formerly CCSM) provides state-of-the-art computer simulations of the Earth's past, present, and future climate states. The CESM simulation image above is from the CISL Visualization Gallery. Weather Research & Forecasting (WRF) model \u2013 A next-generation mesoscale, numerical weather-prediction system designed to serve both operational forecasting and atmospheric research needs. Whole Atmosphere Community Climate Model (WACCM) \u2013 A comprehensive numerical model spanning the range of altitude from the Earth's surface to the thermosphere, developed by a collaboration of NCAR\u2019s High Altitude Observatory, Atmospheric Chemistry Observations & Modeling, and Climate and Global Dynamics division. Atmospheric chemistry models \u2013 NCAR\u2019s Atmospheric Chemistry Observations & Modeling builds, critically evaluates, and applies process, regional- and global-scale models that address atmospheric chemistry research questions, with a focus on couplings between different components of the Earth system. Thermospheric General Circulation Models (TGCMs) \u2013 The High Altitude Observatory (HAO) at NCAR has developed a series of numeric simulation models \u2013 such as TIEGCM and TIME-GCM \u2013 of the Earth's upper atmosphere, including the upper stratosphere, mesosphere, and thermosphere. Data Assimilation Research Testbed (DART) \u2013 A community facility for ensemble data assimilation developed and maintained by NCAR\u2019s Data Assimilation Research Section ( DAReS ).","title":"Community Models"},{"location":"environment-and-software/community-models/#community-models","text":"The models described below are available for use on NCAR computers that CISL manages. Please contact the NCAR Research Computing help desk if you need assistance. Community Earth System Model (CESM) \u2013 A fully coupled, global climate model developed at NCAR. CESM (formerly CCSM) provides state-of-the-art computer simulations of the Earth's past, present, and future climate states. The CESM simulation image above is from the CISL Visualization Gallery. Weather Research & Forecasting (WRF) model \u2013 A next-generation mesoscale, numerical weather-prediction system designed to serve both operational forecasting and atmospheric research needs. Whole Atmosphere Community Climate Model (WACCM) \u2013 A comprehensive numerical model spanning the range of altitude from the Earth's surface to the thermosphere, developed by a collaboration of NCAR\u2019s High Altitude Observatory, Atmospheric Chemistry Observations & Modeling, and Climate and Global Dynamics division. Atmospheric chemistry models \u2013 NCAR\u2019s Atmospheric Chemistry Observations & Modeling builds, critically evaluates, and applies process, regional- and global-scale models that address atmospheric chemistry research questions, with a focus on couplings between different components of the Earth system. Thermospheric General Circulation Models (TGCMs) \u2013 The High Altitude Observatory (HAO) at NCAR has developed a series of numeric simulation models \u2013 such as TIEGCM and TIME-GCM \u2013 of the Earth's upper atmosphere, including the upper stratosphere, mesosphere, and thermosphere. Data Assimilation Research Testbed (DART) \u2013 A community facility for ensemble data assimilation developed and maintained by NCAR\u2019s Data Assimilation Research Section ( DAReS ).","title":"Community models"},{"location":"environment-and-software/data-analysis-and-visualization/","text":"Data analysis and visualization \u00b6 Many data analysis and visualization software packages are freely available for use on CISL-managed resources. These packages include some developed and supported by NCAR and CISL. Frequently used packages \u00b6 These are among the more frequently used data analysis and visualization packages available on NCAR systems. To request installation of other packages, contact the NCAR Research Computing help desk . GrADS \u00b6 The Grid Analysis and Display System ( GrADS ) is an interactive desktop tool for visualizing earth science data. IDL \u00b6 IDL is Interactive Data Language, which is used for data visualization and analysis. Documentation is available here . MATLAB \u00b6 Please follow these license use guidelines for Matlab The CISL user community shares a limited number of licenses for running MATLAB, MATLAB Toolboxes, and some other applications. Follow these guidelines to ensure fair access for all users: Avoid monopolizing these licenses. If you need to use multiple licenses at one time, be considerate of others and finish your session as quickly as possible. Close applications when you are done to free up licenses for others to use. CISL reserves the right to kill jobs/tasks of users who monopolize these licenses. To see how many licenses are being used, run licstatus at your command line. You will see columns showing how many licenses you're using, the total number of licenses in use, and the total number of licenses. licstatus This is a high-level language and interactive environment for data analysis, statistics, and image processing. Several MATLAB toolboxes are provided (list below). See the MathWorks web site for documentation and note the information just below about Octave, an alternative to MATLAB. Related: MATLAB Parallel Computing Toolbox MATLAB toolboxes Image Processing Toolbox Mapping Toolbox MATLAB Compiler Neural Network Toolbox Optimization Toolbox Parallel Computing Toolbox Signal Processing Toolbox Statistics Toolbox Wavelet Toolbox MATLAB alternative - Octave \u00b6 Many MATLAB codes run with very little or no modification under Octave , a free interactive data analysis software package with syntax and functionality that are similar to MATLAB's. Since using Octave is not constrained by license issues, we encourage MATLAB users to try it, particularly those who have long-running MATLAB jobs. Depending on compute intensity, Octave usually runs slower than MATLAB but it may be suitable for most data analysis work and you won't risk having jobs killed because of a lack of licenses. To use Octave interactively, start an interactive job and load the module. module load octave Run octave to start the command line interface, or run the following command to use the GUI. octave --force-gui NCL \u00b6 NCAR Command Language is an interpreted language that CISL designed for scientific data analysis and visualization. ParaView * \u00b6 This is an open-source application for building visualizations and analyzing data, either interactively in 3D or through batch processing. See ParaView.org for documentation. PyNGL and PyNIO \u00b6 Python packages that CISL developed for scientific visualization, file input/output, and data analysis. VAPOR * \u00b6 The Visualization and Analysis Platform for Ocean, Atmosphere, and Solar Researchers is a desktop platform that provides an interactive 3D visualization environment for exploring geosciences CFD data sets. See VAPOR . Many additional applications and tools that are commonly used by atmospheric and Earth system scientists are available on NCAR HPC resources and through the CISL Research Data Archive . These include Mathematica , Vis5d , and VTK . Those marked with an asterisk should be run only on the Casper nodes because of their graphics and GPU requirements. Others can be used on Cheyenne. Check the man pages for any program to get additional information. Advanced visualization support \u00b6 Researchers who need help visualizing data to demonstrate the results of their scientific computing can request expert assistance and collaboration from CISL. This service is available to researchers who want help using specialized applications on data analysis and visualization resources that CISL manages. CISL visualization staff have particular expertise in CISL-developed software such as VAPOR and NCL but are open to assisting with other applications. They have helped science teams produce numerous visualizations for conferences, publications, and scientific journals as well as science and broadcast news programs. See these sites for some examples: CISL Visualization Gallery NCAR VisLab YouTube Channel Requesting support \u00b6 To ask for assistance, please use this request form . Indicate clearly that the request is for \"Advanced Visualization Support\" and describe how the proposed visualization project meets the following criteria: The assistance will culminate in a visualization deliverable or deliverables within a defined time period. The deliverable(s) will help demonstrate science results and findings. A member or members of the science team will participate actively in the production of the deliverable(s). Data to be visualized must have been generated on a CISL high-performance computing system. CISL data analysis and visualization resources will be used in creating the deliverables. Requests for advanced visualization support are evaluated based on those criteria to ensure the most productive use of the limited available staff time. To further help us evaluate your request, please briefly address the following questions when submitting the request form: What does the scientist want to communicate about the data with a visualization? What type of visualization is needed (2D or 3D animation, still images)? What model or instrument produced the data? How big are the data? In what file format are the data stored? The CISL visualization staff will follow up with requesters directly if additional details are needed. Starting visualization applications \u00b6 CISL visualization experts can help as described above with using applications such as NCL and VAPOR. See the Casper documentation to learn how to submit the necessary jobs and start the applications on that data analysis and visualization cluster.","title":"Data Analysis and Visualization"},{"location":"environment-and-software/data-analysis-and-visualization/#data-analysis-and-visualization","text":"Many data analysis and visualization software packages are freely available for use on CISL-managed resources. These packages include some developed and supported by NCAR and CISL.","title":"Data analysis and visualization"},{"location":"environment-and-software/data-analysis-and-visualization/#frequently-used-packages","text":"These are among the more frequently used data analysis and visualization packages available on NCAR systems. To request installation of other packages, contact the NCAR Research Computing help desk .","title":"Frequently used packages"},{"location":"environment-and-software/data-analysis-and-visualization/#grads","text":"The Grid Analysis and Display System ( GrADS ) is an interactive desktop tool for visualizing earth science data.","title":"GrADS"},{"location":"environment-and-software/data-analysis-and-visualization/#idl","text":"IDL is Interactive Data Language, which is used for data visualization and analysis. Documentation is available here .","title":"IDL"},{"location":"environment-and-software/data-analysis-and-visualization/#matlab","text":"Please follow these license use guidelines for Matlab The CISL user community shares a limited number of licenses for running MATLAB, MATLAB Toolboxes, and some other applications. Follow these guidelines to ensure fair access for all users: Avoid monopolizing these licenses. If you need to use multiple licenses at one time, be considerate of others and finish your session as quickly as possible. Close applications when you are done to free up licenses for others to use. CISL reserves the right to kill jobs/tasks of users who monopolize these licenses. To see how many licenses are being used, run licstatus at your command line. You will see columns showing how many licenses you're using, the total number of licenses in use, and the total number of licenses. licstatus This is a high-level language and interactive environment for data analysis, statistics, and image processing. Several MATLAB toolboxes are provided (list below). See the MathWorks web site for documentation and note the information just below about Octave, an alternative to MATLAB. Related: MATLAB Parallel Computing Toolbox MATLAB toolboxes Image Processing Toolbox Mapping Toolbox MATLAB Compiler Neural Network Toolbox Optimization Toolbox Parallel Computing Toolbox Signal Processing Toolbox Statistics Toolbox Wavelet Toolbox","title":"MATLAB"},{"location":"environment-and-software/data-analysis-and-visualization/#matlab-alternative-octave","text":"Many MATLAB codes run with very little or no modification under Octave , a free interactive data analysis software package with syntax and functionality that are similar to MATLAB's. Since using Octave is not constrained by license issues, we encourage MATLAB users to try it, particularly those who have long-running MATLAB jobs. Depending on compute intensity, Octave usually runs slower than MATLAB but it may be suitable for most data analysis work and you won't risk having jobs killed because of a lack of licenses. To use Octave interactively, start an interactive job and load the module. module load octave Run octave to start the command line interface, or run the following command to use the GUI. octave --force-gui","title":"MATLAB alternative - Octave"},{"location":"environment-and-software/data-analysis-and-visualization/#ncl","text":"NCAR Command Language is an interpreted language that CISL designed for scientific data analysis and visualization.","title":"NCL"},{"location":"environment-and-software/data-analysis-and-visualization/#paraview","text":"This is an open-source application for building visualizations and analyzing data, either interactively in 3D or through batch processing. See ParaView.org for documentation.","title":"ParaView*"},{"location":"environment-and-software/data-analysis-and-visualization/#pyngl-and-pynio","text":"Python packages that CISL developed for scientific visualization, file input/output, and data analysis.","title":"PyNGL and PyNIO"},{"location":"environment-and-software/data-analysis-and-visualization/#vapor","text":"The Visualization and Analysis Platform for Ocean, Atmosphere, and Solar Researchers is a desktop platform that provides an interactive 3D visualization environment for exploring geosciences CFD data sets. See VAPOR . Many additional applications and tools that are commonly used by atmospheric and Earth system scientists are available on NCAR HPC resources and through the CISL Research Data Archive . These include Mathematica , Vis5d , and VTK . Those marked with an asterisk should be run only on the Casper nodes because of their graphics and GPU requirements. Others can be used on Cheyenne. Check the man pages for any program to get additional information.","title":"VAPOR*"},{"location":"environment-and-software/data-analysis-and-visualization/#advanced-visualization-support","text":"Researchers who need help visualizing data to demonstrate the results of their scientific computing can request expert assistance and collaboration from CISL. This service is available to researchers who want help using specialized applications on data analysis and visualization resources that CISL manages. CISL visualization staff have particular expertise in CISL-developed software such as VAPOR and NCL but are open to assisting with other applications. They have helped science teams produce numerous visualizations for conferences, publications, and scientific journals as well as science and broadcast news programs. See these sites for some examples: CISL Visualization Gallery NCAR VisLab YouTube Channel","title":"Advanced visualization support"},{"location":"environment-and-software/data-analysis-and-visualization/#requesting-support","text":"To ask for assistance, please use this request form . Indicate clearly that the request is for \"Advanced Visualization Support\" and describe how the proposed visualization project meets the following criteria: The assistance will culminate in a visualization deliverable or deliverables within a defined time period. The deliverable(s) will help demonstrate science results and findings. A member or members of the science team will participate actively in the production of the deliverable(s). Data to be visualized must have been generated on a CISL high-performance computing system. CISL data analysis and visualization resources will be used in creating the deliverables. Requests for advanced visualization support are evaluated based on those criteria to ensure the most productive use of the limited available staff time. To further help us evaluate your request, please briefly address the following questions when submitting the request form: What does the scientist want to communicate about the data with a visualization? What type of visualization is needed (2D or 3D animation, still images)? What model or instrument produced the data? How big are the data? In what file format are the data stored? The CISL visualization staff will follow up with requesters directly if additional details are needed.","title":"Requesting support"},{"location":"environment-and-software/data-analysis-and-visualization/#starting-visualization-applications","text":"CISL visualization experts can help as described above with using applications such as NCL and VAPOR. See the Casper documentation to learn how to submit the necessary jobs and start the applications on that data analysis and visualization cluster.","title":"Starting visualization applications"},{"location":"environment-and-software/machine-learning-and-deep-learning/","text":"Machine learning and deep learning \u00b6 CISL provides several libraries for users' machine learning and deep learning (ML/DL) work. These libraries have been compiled from source to use native CUDA (GPU) and MPI libraries, increasing the capabilities over downloadable distributions that are available online. The ML/DL library installations can be found in the NCAR Python Library . The libraries available are: TensorFlow machine learning library v2.3.1 PyTorch machine learning library v1.7.1 scikit-learn machine learning library v0.5.3 Horovod deep learning framework v0.21.0 Keras deep learning library v2.4.3","title":"Machine Learning and Deep Learning"},{"location":"environment-and-software/machine-learning-and-deep-learning/#machine-learning-and-deep-learning","text":"CISL provides several libraries for users' machine learning and deep learning (ML/DL) work. These libraries have been compiled from source to use native CUDA (GPU) and MPI libraries, increasing the capabilities over downloadable distributions that are available online. The ML/DL library installations can be found in the NCAR Python Library . The libraries available are: TensorFlow machine learning library v2.3.1 PyTorch machine learning library v1.7.1 scikit-learn machine learning library v0.5.3 Horovod deep learning framework v0.21.0 Keras deep learning library v2.4.3","title":"Machine learning and deep learning"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/","text":"MATLAB Parallel Computing Toolbox on Casper and Cheyenne \u00b6 The MATLAB Parallel Computing Toolbox (PCT) allows you to run MATLAB code in parallel across multiple workers, which are analogous to MPI tasks or OpenMP threads. Additionally, NCAR has a license for the MATLAB Parallel Server (MPS) \u2013 formerly the MATLAB Distributed Computing Server \u2013 which allows you to run a MATLAB script using workers from multiple compute nodes via a batch job you submit with PBS. Using MPS is critical for workflows that result in MATLAB processing running on multiple nodes. Here's why: Any MATLAB component \u2013 MATLAB itself or one of the many toolboxes \u2013 follows the same rules regarding license use. When you load MATLAB or activate a toolbox, you check out a single license. If you launch the same product on the same node , no additional licenses are used. However, if you run the same product on a different node , you consume another license. These rules mean that running multiple batch jobs, with each executing a MATLAB script, consumes many licenses if they are scheduled on different compute nodes. The MPS alleviates this issue. If you start a PBS job via the MPS, you consume only a single MATLAB license (and single licenses of any other toolboxes loaded including the PCT itself) no matter how many nodes you request. Instead, MATLAB will use MPS worker licenses for each worker requested. Therefore, if you have a workflow that will result in MATLAB processes running on multiple nodes, always look to use the MATLAB Parallel Server. In summary, here are basic guidelines for when you should use each product: MATLAB only - when you need only a single worker/task (e.g., running a script with only serial computation) PCT (local) - when you want to run a script or function using a parallel pool of workers on a single node MPS (distributed PCT) - when you need to run a script/function across multiple nodes of workers OR you want to run multiple scripts on multiple nodes (similar to a command-file PBS job ) Running a simple parallel code on one node using the toolbox \u00b6 fun_local In this example, a simple code uses multiple workers on a single node to compute a sum in parallel. Here is the sample code: # parallel_sum.m: function s = parallel_sum ( N ) s = 0 ; parfor i = 1 : N s = s + i ; end fprintf ( 'Sum of numbers from 1 to %d is %d.n' , N , s ); end This function is executed by MATLAB code that reads in a few parameters from your user environment. For single-node parallel jobs, this example uses the \u201clocal\u201d cluster profile that is available by default when using the toolbox. Warning Do not run the local toolbox profile on login nodes ; excessive CPU and/or memory use will result in your script being terminated. Instead, use a PBS batch job as in the following example to run a single-node cluster on a compute node. PBS Submission Script submit_local.pbs This PBS job also specifies the number of workers, which corresponds to the number of CPUs requested in the job script. #!/bin/bash #PBS -N matlab_pct #PBS -A <PROJECT> #PBS -l walltime=05:00 #PBS -q casper #PBS -j oe #PBS -o local.log #PBS -l select=1:ncpus=4:mpiprocs=4:mem=10GB # The following script is not distributed; it uses threads # and so this PBS job should only ever request a single node module load matlab # Derive the number of workers to use in the toolbox run script export NUMWORKERS = $( wc -l $PBS_NODEFILE | cut -d ' ' -f1 ) SECONDS = 0 matlab -nodesktop -nosplash << EOF % Start local cluster and submit job with custom number of workers c = parcluster('local') j = c.batch(@parallel_sum, 1, {100}, 'pool', $((NUMWORKERS - 1))); % Wait for the job to finish, then get output wait(j); diary(j); exit; EOF echo \"Time elapsed = $SECONDS s\" MPS cluster profiles \u00b6 When using the PCT, you are expected to create and use cluster profiles that manage either node-local tasks or batch-scheduler tasks. While there is a preconfigured profile for single-node use (local), you will need to do some setup before you can use the MPS. CISL provides a distributed cluster profile for PBS on both Casper and Cheyenne for all versions of MATLAB starting with R2020a. You can import an existing cluster profile using the wizard in the graphical interface, or you can do it programmatically as follows. If you use our sample distributed script provided in the following section, we include the MPS cluster profile setup for you, so you can skip the commands in this section. At the MATLAB command line, enter the following line to import the MPS profile: ncar_mps = parallel.importProfile('/glade/u/apps/opt/matlab/parallel/ncar_mps.mlsettings'); You need to import the profile only once; MATLAB will remember it in future sessions. If you anticipate using the parallel server profile frequently, you may want to make it your default parallel profile as shown here: parallel.defaultClusterProfile(ncar-mps); Using the MATLAB parallel server (MPS) to span multiple nodes \u00b6 The configuration above will limit your job to the number of CPUs on a single node; on Casper and Cheyenne this means 36 workers, or 72 if you use hyperthreads. However, you can use the parallel server to span multiple nodes. When using MPS, MATLAB itself will submit a job to the batch scheduler and use an internal MPI library to enable communication between remote workers. func_mps Using the MATLAB parallel server Here again, use a MATLAB script to set up your parallel cluster as in this example, which embeds MATLAB code into a driver script submit_server.sh : #!/bin/bash # This script doesn't need to run on a batch node... we can simply submit # the parallel job by running this script on the login node module rm ncarenv module load matlab mkdir -p output # Job parameters MPSNODES = 2 MPSTASKS = 4 MPSACCOUNT = <PROJECT> MPSQUEUE = casper@casper-pbs MPSWALLTIME = 300 SECONDS = 0 matlab -nodesktop -nosplash << EOF % Add cluster profile if not already present if ~any(strcmp(parallel.clusterProfiles, 'ncar_mps')) ncar_mps = parallel.importProfile('/glade/u/apps/opt/matlab/parallel/ncar_mps.mlsettings'); end % Start PBS cluster and submit job with custom number of workers c = parcluster('ncar_mps'); % Matlab workers will equal nodes * tasks-per-node - 1 jNodes = '$MPSNODES'; jTasks = '$MPSTASKS'; jWorkers = str2num(jNodes) * str2num(jTasks) - 1; c.ClusterMatlabRoot = getenv('NCAR_ROOT_MATLAB'); c.ResourceTemplate = append('-l select=', jNodes, ':ncpus=', jTasks, ':mpiprocs=', jTasks); c.SubmitArguments = append('-A $MPSACCOUNT -q $MPSQUEUE -l walltime=$MPSWALLTIME'); c.JobStorageLocation = append(getenv('PWD'), '/output'); % Output cluster settings c % Submit job to batch scheduler (PBS) j = batch(c, @parallel_sum, 1, {100}, 'pool', jWorkers); % Wait for job to finish and get output wait(j); diary(j); exit; EOF echo \"Time elapsed = $SECONDS s\" Sample PCT scripts \u00b6 Including the scripts shown above, there are four sets of example scripts that you can use, modify, and extend to fit your purposes. All four examples can be copied from /glade/u/apps/opt/matlab/parallel/examples . func_local - Run a specified function using a pool of local (single-node) workers. This is Example 1 above. func_mps - Run a specified function using a pool of workers distributed across multiple nodes. This is Example 2 above. multi_script_mps - Run a specified collection of MATLAB functions in separate script files using a pool of workers. Configured for MPS use but can be modified to use the local profile. spmd_mps - Run a single MATLAB function in a specified script using many input parameters. The last two examples are functionally similar to a command-file PBS job, but with the licensing benefits of using MPS.","title":"MATLAB Parallel Computing Toolbox"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#matlab-parallel-computing-toolbox-on-casper-and-cheyenne","text":"The MATLAB Parallel Computing Toolbox (PCT) allows you to run MATLAB code in parallel across multiple workers, which are analogous to MPI tasks or OpenMP threads. Additionally, NCAR has a license for the MATLAB Parallel Server (MPS) \u2013 formerly the MATLAB Distributed Computing Server \u2013 which allows you to run a MATLAB script using workers from multiple compute nodes via a batch job you submit with PBS. Using MPS is critical for workflows that result in MATLAB processing running on multiple nodes. Here's why: Any MATLAB component \u2013 MATLAB itself or one of the many toolboxes \u2013 follows the same rules regarding license use. When you load MATLAB or activate a toolbox, you check out a single license. If you launch the same product on the same node , no additional licenses are used. However, if you run the same product on a different node , you consume another license. These rules mean that running multiple batch jobs, with each executing a MATLAB script, consumes many licenses if they are scheduled on different compute nodes. The MPS alleviates this issue. If you start a PBS job via the MPS, you consume only a single MATLAB license (and single licenses of any other toolboxes loaded including the PCT itself) no matter how many nodes you request. Instead, MATLAB will use MPS worker licenses for each worker requested. Therefore, if you have a workflow that will result in MATLAB processes running on multiple nodes, always look to use the MATLAB Parallel Server. In summary, here are basic guidelines for when you should use each product: MATLAB only - when you need only a single worker/task (e.g., running a script with only serial computation) PCT (local) - when you want to run a script or function using a parallel pool of workers on a single node MPS (distributed PCT) - when you need to run a script/function across multiple nodes of workers OR you want to run multiple scripts on multiple nodes (similar to a command-file PBS job )","title":"MATLAB Parallel Computing Toolbox on Casper and Cheyenne"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#running-a-simple-parallel-code-on-one-node-using-the-toolbox","text":"fun_local In this example, a simple code uses multiple workers on a single node to compute a sum in parallel. Here is the sample code: # parallel_sum.m: function s = parallel_sum ( N ) s = 0 ; parfor i = 1 : N s = s + i ; end fprintf ( 'Sum of numbers from 1 to %d is %d.n' , N , s ); end This function is executed by MATLAB code that reads in a few parameters from your user environment. For single-node parallel jobs, this example uses the \u201clocal\u201d cluster profile that is available by default when using the toolbox. Warning Do not run the local toolbox profile on login nodes ; excessive CPU and/or memory use will result in your script being terminated. Instead, use a PBS batch job as in the following example to run a single-node cluster on a compute node. PBS Submission Script submit_local.pbs This PBS job also specifies the number of workers, which corresponds to the number of CPUs requested in the job script. #!/bin/bash #PBS -N matlab_pct #PBS -A <PROJECT> #PBS -l walltime=05:00 #PBS -q casper #PBS -j oe #PBS -o local.log #PBS -l select=1:ncpus=4:mpiprocs=4:mem=10GB # The following script is not distributed; it uses threads # and so this PBS job should only ever request a single node module load matlab # Derive the number of workers to use in the toolbox run script export NUMWORKERS = $( wc -l $PBS_NODEFILE | cut -d ' ' -f1 ) SECONDS = 0 matlab -nodesktop -nosplash << EOF % Start local cluster and submit job with custom number of workers c = parcluster('local') j = c.batch(@parallel_sum, 1, {100}, 'pool', $((NUMWORKERS - 1))); % Wait for the job to finish, then get output wait(j); diary(j); exit; EOF echo \"Time elapsed = $SECONDS s\"","title":"Running a simple parallel code on one node using the toolbox"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#mps-cluster-profiles","text":"When using the PCT, you are expected to create and use cluster profiles that manage either node-local tasks or batch-scheduler tasks. While there is a preconfigured profile for single-node use (local), you will need to do some setup before you can use the MPS. CISL provides a distributed cluster profile for PBS on both Casper and Cheyenne for all versions of MATLAB starting with R2020a. You can import an existing cluster profile using the wizard in the graphical interface, or you can do it programmatically as follows. If you use our sample distributed script provided in the following section, we include the MPS cluster profile setup for you, so you can skip the commands in this section. At the MATLAB command line, enter the following line to import the MPS profile: ncar_mps = parallel.importProfile('/glade/u/apps/opt/matlab/parallel/ncar_mps.mlsettings'); You need to import the profile only once; MATLAB will remember it in future sessions. If you anticipate using the parallel server profile frequently, you may want to make it your default parallel profile as shown here: parallel.defaultClusterProfile(ncar-mps);","title":"MPS cluster profiles"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#using-the-matlab-parallel-server-mps-to-span-multiple-nodes","text":"The configuration above will limit your job to the number of CPUs on a single node; on Casper and Cheyenne this means 36 workers, or 72 if you use hyperthreads. However, you can use the parallel server to span multiple nodes. When using MPS, MATLAB itself will submit a job to the batch scheduler and use an internal MPI library to enable communication between remote workers. func_mps Using the MATLAB parallel server Here again, use a MATLAB script to set up your parallel cluster as in this example, which embeds MATLAB code into a driver script submit_server.sh : #!/bin/bash # This script doesn't need to run on a batch node... we can simply submit # the parallel job by running this script on the login node module rm ncarenv module load matlab mkdir -p output # Job parameters MPSNODES = 2 MPSTASKS = 4 MPSACCOUNT = <PROJECT> MPSQUEUE = casper@casper-pbs MPSWALLTIME = 300 SECONDS = 0 matlab -nodesktop -nosplash << EOF % Add cluster profile if not already present if ~any(strcmp(parallel.clusterProfiles, 'ncar_mps')) ncar_mps = parallel.importProfile('/glade/u/apps/opt/matlab/parallel/ncar_mps.mlsettings'); end % Start PBS cluster and submit job with custom number of workers c = parcluster('ncar_mps'); % Matlab workers will equal nodes * tasks-per-node - 1 jNodes = '$MPSNODES'; jTasks = '$MPSTASKS'; jWorkers = str2num(jNodes) * str2num(jTasks) - 1; c.ClusterMatlabRoot = getenv('NCAR_ROOT_MATLAB'); c.ResourceTemplate = append('-l select=', jNodes, ':ncpus=', jTasks, ':mpiprocs=', jTasks); c.SubmitArguments = append('-A $MPSACCOUNT -q $MPSQUEUE -l walltime=$MPSWALLTIME'); c.JobStorageLocation = append(getenv('PWD'), '/output'); % Output cluster settings c % Submit job to batch scheduler (PBS) j = batch(c, @parallel_sum, 1, {100}, 'pool', jWorkers); % Wait for job to finish and get output wait(j); diary(j); exit; EOF echo \"Time elapsed = $SECONDS s\"","title":"Using the MATLAB parallel server (MPS) to span multiple nodes"},{"location":"environment-and-software/matlab-parallel-computing-toolbox/#sample-pct-scripts","text":"Including the scripts shown above, there are four sets of example scripts that you can use, modify, and extend to fit your purposes. All four examples can be copied from /glade/u/apps/opt/matlab/parallel/examples . func_local - Run a specified function using a pool of local (single-node) workers. This is Example 1 above. func_mps - Run a specified function using a pool of workers distributed across multiple nodes. This is Example 2 above. multi_script_mps - Run a specified collection of MATLAB functions in separate script files using a pool of workers. Configured for MPS use but can be modified to use the local profile. spmd_mps - Run a single MATLAB function in a specified script using many input parameters. The last two examples are functionally similar to a command-file PBS job, but with the licensing benefits of using MPS.","title":"Sample PCT scripts"},{"location":"environment-and-software/ncar-classic-libraries-for-geophysics/","text":"NCAR Classic Libraries for Geophysics \u00b6 Several mathematical libraries developed in the years 1970-1990 remain popular in the geophysics community. These libraries, listed below, are available for downloading here on GitHub: NCAR Classic Libraries for-Geophysics . FFTPACK : A library of fast Fourier transforms FISHPACK : Fortran subprograms for solving separable elliptic partial differential equations (PDEs) FISHPACK 90 : FISHPACK subprograms with a Fortran 90 interface MUDPACK : Multigrid Fortran subprograms for solving separable and non-separable elliptic PDEs SPHEREPACK : A Fortran library for modeling geophysical processes All of these library routines are written primarily in Fortran 77. Their internal implementation does not always conform to the Fortran Standard. FISHPACK90 provides a Fortran 90 interface to the FISHPACK routines. Only MUDPACK is written with parallelism in mind; it uses OpenMP directives for shared-memory parallelism. The other libraries were designed to run on a single processor. These libraries represent many person-years of development, and though they are no longer under development, NCAR continues to make them available to the public at no cost under a software licensing agreement. The libraries are best suited to Linux and UNIX environments and require a directory structure, tar , and gmake commands.","title":"NCAR Classic Libraries for Geophysics"},{"location":"environment-and-software/ncar-classic-libraries-for-geophysics/#ncar-classic-libraries-for-geophysics","text":"Several mathematical libraries developed in the years 1970-1990 remain popular in the geophysics community. These libraries, listed below, are available for downloading here on GitHub: NCAR Classic Libraries for-Geophysics . FFTPACK : A library of fast Fourier transforms FISHPACK : Fortran subprograms for solving separable elliptic partial differential equations (PDEs) FISHPACK 90 : FISHPACK subprograms with a Fortran 90 interface MUDPACK : Multigrid Fortran subprograms for solving separable and non-separable elliptic PDEs SPHEREPACK : A Fortran library for modeling geophysical processes All of these library routines are written primarily in Fortran 77. Their internal implementation does not always conform to the Fortran Standard. FISHPACK90 provides a Fortran 90 interface to the FISHPACK routines. Only MUDPACK is written with parallelism in mind; it uses OpenMP directives for shared-memory parallelism. The other libraries were designed to run on a single processor. These libraries represent many person-years of development, and though they are no longer under development, NCAR continues to make them available to the public at no cost under a software licensing agreement. The libraries are best suited to Linux and UNIX environments and require a directory structure, tar , and gmake commands.","title":"NCAR Classic Libraries for Geophysics"},{"location":"environment-and-software/ncl/","text":"Using NCL in the NCAR HPC environment \u00b6 The NCAR HPC environment supports the use of NCAR Command Language ( NCL ) both interactively and in batch mode to analyze and visualize data. As described below, to use NCL in the Cheyenne environment you will log in to Derecho or Casper, then: Start an interactive job on Casper and execute the NCL script from that window, or Submit a batch job to execute an NCL script. Follow the instructions below to get started, and customize the scripts and commands as necessary to work with your own data. Other resources \u00b6 See the NCL web site for complete documentation of the language's extensive analysis and visualization capabilities. See the NCL Applications page for links to hundreds of complete NCL scripts that you can download and modify as needed. Interactive use \u00b6 To start an interactive window from which to modify and execute NCL scripts, log in to Casper or Derecho. Start a job on Casper as described in this documentation . When your job starts, load the default module for NCL. module load ncl Modify your NCL script if necessary using a UNIX editor, and execute it as shown here, substituting the name of your own NCL script for script_name.ncl . ncl script_name.ncl Submitting a batch script \u00b6 If you expect running your NCL script to take longer than you would want to work interactively \u2014 overnight, for example \u2014 submit your NCL script in a batch job so it can run unattended. See Starting jobs on Casper nodes for batch job script examples and other details. Visualization examples \u00b6 Example 1 \u00b6 Make an NCL script file named contour_ts_line.ncl using the sample script below. When you run it on Casper, it will create a simple line contour plot using a sample CMIP5 NetCDF data file in the /glade/u/sampledata/ncl/CESM/CAM5 directory. The output to your working directory will be a graphic file called contour_ts_line.png . ;---------------------------------------------------------------------- ; This script creates a simple line contour plot of the first timestep ; of the \"ts\" variable on the given NetCDF file. ;---------------------------------------------------------------------- load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\" load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_csm.ncl\" begin ;---Open file and read data dir = \"/glade/u/sampledata/ncl/CESM/CAM5/\" filename = \"ts_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc\" a = addfile(dir+filename,\"r\") ts = a->ts(0,:,:) ; Read first time step ts = ts-273.15 ; convert from Kelvin->Celsius ts@units = \"degC\" ;---Look at the variable's metadata, if desired printVarSummary(ts) ;---Open file or window to send graphical output to. wks = gsn_open_wks(\"png\",\"contour_ts_line\") ; \"png\", \"ps\", \"pdf\", \"x11\" ;---Create a default line contour plot. res = True plot = gsn_csm_contour_map(wks,ts,res) end Example 2 \u00b6 Using a different script, you can create a more interesting visualization with the data that was used in the first example. Make an NCL script file named contour_ts_color.ncl using the sample script below. When you run it on Casper, the output to your working directory will be a color-filled contour called contour_ts_color.png . ;---------------------------------------------------------------------- ; This script creates filled contour plot of the first timestep of ; the \"ts\" variable on the given NetCDF file. ;---------------------------------------------------------------------- load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\" load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_csm.ncl\" begin ;---Open file and read data dir = \"/glade/u/sampledata/ncl/CESM/CAM5/\" filename = \"ts_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc\" a = addfile(dir+filename,\"r\") ts = a->ts(0,:,:) ; Read first time step. ts = ts-273.15 ; Convert from Kelvin -> Celsius. ts@units = \"degC\" ;---Look at the variable's metadata, if desired printVarSummary(ts) ;---Open file or window to send graphical output to. wks = gsn_open_wks(\"png\",\"contour_ts_color\") ; \"png\", \"ps\", \"pdf\", \"x11\" ;---Set some graphical resources to customize the contour plot. res = True res@gsnMaximize = True ; Maximize plot in frame res@cnFillOn = True ; Turn on contour fill res@cnLinesOn = False ; Turn off contour lines res@cnLineLabelsOn = False ; Turn off line labels res@tiMainString = filename ; Add a main title res@gsnAddCyclic = True ; Add longitude cyclic point ;--Set the contour levels using \"nice_mnmxintvl\" function. mnmxint = nice_mnmxintvl( min(ts), max(ts), 18, False) res@cnLevelSelectionMode = \"ManualLevels\" res@cnMinLevelValF = mnmxint(0) res@cnMaxLevelValF = mnmxint(1) res@cnLevelSpacingF = mnmxint(2)/4. ; Decrease spacing for more levels ;---Create and draw the plot. plot = gsn_csm_contour_map(wks,ts,res) end","title":"NCL"},{"location":"environment-and-software/ncl/#using-ncl-in-the-ncar-hpc-environment","text":"The NCAR HPC environment supports the use of NCAR Command Language ( NCL ) both interactively and in batch mode to analyze and visualize data. As described below, to use NCL in the Cheyenne environment you will log in to Derecho or Casper, then: Start an interactive job on Casper and execute the NCL script from that window, or Submit a batch job to execute an NCL script. Follow the instructions below to get started, and customize the scripts and commands as necessary to work with your own data.","title":"Using NCL in the NCAR HPC environment"},{"location":"environment-and-software/ncl/#other-resources","text":"See the NCL web site for complete documentation of the language's extensive analysis and visualization capabilities. See the NCL Applications page for links to hundreds of complete NCL scripts that you can download and modify as needed.","title":"Other resources"},{"location":"environment-and-software/ncl/#interactive-use","text":"To start an interactive window from which to modify and execute NCL scripts, log in to Casper or Derecho. Start a job on Casper as described in this documentation . When your job starts, load the default module for NCL. module load ncl Modify your NCL script if necessary using a UNIX editor, and execute it as shown here, substituting the name of your own NCL script for script_name.ncl . ncl script_name.ncl","title":"Interactive use"},{"location":"environment-and-software/ncl/#submitting-a-batch-script","text":"If you expect running your NCL script to take longer than you would want to work interactively \u2014 overnight, for example \u2014 submit your NCL script in a batch job so it can run unattended. See Starting jobs on Casper nodes for batch job script examples and other details.","title":"Submitting a batch script"},{"location":"environment-and-software/ncl/#visualization-examples","text":"","title":"Visualization examples"},{"location":"environment-and-software/ncl/#example-1","text":"Make an NCL script file named contour_ts_line.ncl using the sample script below. When you run it on Casper, it will create a simple line contour plot using a sample CMIP5 NetCDF data file in the /glade/u/sampledata/ncl/CESM/CAM5 directory. The output to your working directory will be a graphic file called contour_ts_line.png . ;---------------------------------------------------------------------- ; This script creates a simple line contour plot of the first timestep ; of the \"ts\" variable on the given NetCDF file. ;---------------------------------------------------------------------- load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\" load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_csm.ncl\" begin ;---Open file and read data dir = \"/glade/u/sampledata/ncl/CESM/CAM5/\" filename = \"ts_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc\" a = addfile(dir+filename,\"r\") ts = a->ts(0,:,:) ; Read first time step ts = ts-273.15 ; convert from Kelvin->Celsius ts@units = \"degC\" ;---Look at the variable's metadata, if desired printVarSummary(ts) ;---Open file or window to send graphical output to. wks = gsn_open_wks(\"png\",\"contour_ts_line\") ; \"png\", \"ps\", \"pdf\", \"x11\" ;---Create a default line contour plot. res = True plot = gsn_csm_contour_map(wks,ts,res) end","title":"Example 1"},{"location":"environment-and-software/ncl/#example-2","text":"Using a different script, you can create a more interesting visualization with the data that was used in the first example. Make an NCL script file named contour_ts_color.ncl using the sample script below. When you run it on Casper, the output to your working directory will be a color-filled contour called contour_ts_color.png . ;---------------------------------------------------------------------- ; This script creates filled contour plot of the first timestep of ; the \"ts\" variable on the given NetCDF file. ;---------------------------------------------------------------------- load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\" load \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_csm.ncl\" begin ;---Open file and read data dir = \"/glade/u/sampledata/ncl/CESM/CAM5/\" filename = \"ts_Amon_CESM1-CAM5_historical_r1i1p1_185001-200512.nc\" a = addfile(dir+filename,\"r\") ts = a->ts(0,:,:) ; Read first time step. ts = ts-273.15 ; Convert from Kelvin -> Celsius. ts@units = \"degC\" ;---Look at the variable's metadata, if desired printVarSummary(ts) ;---Open file or window to send graphical output to. wks = gsn_open_wks(\"png\",\"contour_ts_color\") ; \"png\", \"ps\", \"pdf\", \"x11\" ;---Set some graphical resources to customize the contour plot. res = True res@gsnMaximize = True ; Maximize plot in frame res@cnFillOn = True ; Turn on contour fill res@cnLinesOn = False ; Turn off contour lines res@cnLineLabelsOn = False ; Turn off line labels res@tiMainString = filename ; Add a main title res@gsnAddCyclic = True ; Add longitude cyclic point ;--Set the contour levels using \"nice_mnmxintvl\" function. mnmxint = nice_mnmxintvl( min(ts), max(ts), 18, False) res@cnLevelSelectionMode = \"ManualLevels\" res@cnMinLevelValF = mnmxint(0) res@cnMaxLevelValF = mnmxint(1) res@cnLevelSpacingF = mnmxint(2)/4. ; Decrease spacing for more levels ;---Create and draw the plot. plot = gsn_csm_contour_map(wks,ts,res) end","title":"Example 2"},{"location":"environment-and-software/numerical-libraries/","text":"Intel's Math Kernel Library \u00b6 The Intel Math Kernel Library ( Intel MKL ) contains highly optimized, extensively threaded math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. On NCAR systems the MKL is available through the mkl module. (See here for more discussion on interacting with the module system.) Intel MKL has the following functional categories : Linear algebra : BLAS routines are vector-vector (Level 1), matrix-vector (Level 2) and matrix-matrix (Level 3) operations for real and complex single and double precision data. LAPACK consists of tuned LU, Cholesky and QR factorizations, eigenvalue and least squares solvers. Sparse BLAS, ScaLAPACK, Sparse Solver, Extended Eigensolver (FEAST, PARDISO), PBLAS and BLACS. Fast Fourier Transforms (FFTs) from 1D to multidimensional, complex to complex, real to complex, and real to real transforms of arbitrary lengths. Applications written with the open source FFTW can be easily ported to MKL by linking with interface wrapper libraries provided as part of MKL for easy migration. Cluster versions of LAPACK and FFTs are also available as part of MKL to take advantage of MPI parallelism in addition to single node parallelism from multithreading. Vector math functions include computationally intensive core mathematical operations for single and double precision real and complex data types. These are similar to libm functions from compiler libraries but operate on vectors rather than scalars to provide better performance. There are various controls for setting accuracy, error mode and denormalized number handling to customize the behavior of the routines. Statistics functions include random number generators and probability distributions, optimized for multicore processors. Also included are compute-intensive in and out-of-core routines to compute basic statistics, estimation of dependencies etc. Data fitting functions include splines (linear, quadratic, cubic, look-up, stepwise constant) for 1-dimensional interpolation that can be used in data analytics, geometric modeling and surface approximation applications. Linking with the MKL \u00b6 The MKL ships with both serial and parallel versions of many of its core components, and with support for normal and \"long\" (64-bit) indexing integers. See the Link Line Advisor for guidance on how to select precise combinations of the MKL functionality when linking into your application. Cray LibSci \u00b6 Derecho Only! Most Cray software such as LibSci is only available on Derecho. Cray's LibSci is a collection of numerical routines tuned for performance on Cray systems. Most LibSci components contain both serial and and parallel routines optimized specifically to make best use of Cray processors and interconnect architectures. The general components of Cray LibSci are: BLAS (Basic Linear Algebra Subroutines) CBLAS (C interface to the legacy BLAS) BLACS (Basic Linear Algebra Communication Subprograms) LAPACK (Linear Algebra Package) ScaLAPACK (Parallel Linear Algebra routines) IRT (Iterative Refinement Toolkit) \u2010 a library of solvers and tools that provides solutions to linear systems using single\u2010precision factorizations while preserving accuracy through mixed\u2010precision iterative refinement. CrayBLAS \u2010 a library of BLAS routines highly optimized for Cray systems. (For further information, see man intro_blas3 .) For additional details see: $ module load cray-libsci $ man intro_libsci FFTW \u00b6 FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). On NCAR systems single-node and distributed-memory parallel implementations of FFTW are available through the fftw and fftw-mpi modules, respectively. (See here for more discussion on interacting with the module system.)","title":"Numerical Libraries"},{"location":"environment-and-software/numerical-libraries/#intels-math-kernel-library","text":"The Intel Math Kernel Library ( Intel MKL ) contains highly optimized, extensively threaded math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, Sparse Solvers, Fast Fourier Transforms, Vector Math, and more. On NCAR systems the MKL is available through the mkl module. (See here for more discussion on interacting with the module system.) Intel MKL has the following functional categories : Linear algebra : BLAS routines are vector-vector (Level 1), matrix-vector (Level 2) and matrix-matrix (Level 3) operations for real and complex single and double precision data. LAPACK consists of tuned LU, Cholesky and QR factorizations, eigenvalue and least squares solvers. Sparse BLAS, ScaLAPACK, Sparse Solver, Extended Eigensolver (FEAST, PARDISO), PBLAS and BLACS. Fast Fourier Transforms (FFTs) from 1D to multidimensional, complex to complex, real to complex, and real to real transforms of arbitrary lengths. Applications written with the open source FFTW can be easily ported to MKL by linking with interface wrapper libraries provided as part of MKL for easy migration. Cluster versions of LAPACK and FFTs are also available as part of MKL to take advantage of MPI parallelism in addition to single node parallelism from multithreading. Vector math functions include computationally intensive core mathematical operations for single and double precision real and complex data types. These are similar to libm functions from compiler libraries but operate on vectors rather than scalars to provide better performance. There are various controls for setting accuracy, error mode and denormalized number handling to customize the behavior of the routines. Statistics functions include random number generators and probability distributions, optimized for multicore processors. Also included are compute-intensive in and out-of-core routines to compute basic statistics, estimation of dependencies etc. Data fitting functions include splines (linear, quadratic, cubic, look-up, stepwise constant) for 1-dimensional interpolation that can be used in data analytics, geometric modeling and surface approximation applications.","title":"Intel's Math Kernel Library"},{"location":"environment-and-software/numerical-libraries/#linking-with-the-mkl","text":"The MKL ships with both serial and parallel versions of many of its core components, and with support for normal and \"long\" (64-bit) indexing integers. See the Link Line Advisor for guidance on how to select precise combinations of the MKL functionality when linking into your application.","title":"Linking with the MKL"},{"location":"environment-and-software/numerical-libraries/#cray-libsci","text":"Derecho Only! Most Cray software such as LibSci is only available on Derecho. Cray's LibSci is a collection of numerical routines tuned for performance on Cray systems. Most LibSci components contain both serial and and parallel routines optimized specifically to make best use of Cray processors and interconnect architectures. The general components of Cray LibSci are: BLAS (Basic Linear Algebra Subroutines) CBLAS (C interface to the legacy BLAS) BLACS (Basic Linear Algebra Communication Subprograms) LAPACK (Linear Algebra Package) ScaLAPACK (Parallel Linear Algebra routines) IRT (Iterative Refinement Toolkit) \u2010 a library of solvers and tools that provides solutions to linear systems using single\u2010precision factorizations while preserving accuracy through mixed\u2010precision iterative refinement. CrayBLAS \u2010 a library of BLAS routines highly optimized for Cray systems. (For further information, see man intro_blas3 .) For additional details see: $ module load cray-libsci $ man intro_libsci","title":"Cray LibSci"},{"location":"environment-and-software/numerical-libraries/#fftw","text":"FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST). On NCAR systems single-node and distributed-memory parallel implementations of FFTW are available through the fftw and fftw-mpi modules, respectively. (See here for more discussion on interacting with the module system.)","title":"FFTW"},{"location":"environment-and-software/benchmarks/","text":"Benchmarks \u00b6 Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements. NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance. NCAR's benchmarking application packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, version number and download details are provided in the README files. Releases \u00b6 2019/2020 Release 2015 Release","title":"Benchmarks"},{"location":"environment-and-software/benchmarks/#benchmarks","text":"Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements. NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance. NCAR's benchmarking application packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, version number and download details are provided in the README files.","title":"Benchmarks"},{"location":"environment-and-software/benchmarks/#releases","text":"2019/2020 Release 2015 Release","title":"Releases"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/","text":"NCAR benchmarking applications- 2015 release \u00b6 Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements. Thus, NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance. NCAR's benchmarking applications are listed in the tables below, along with file names, sizes, and checksums. These packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, you will find version number and download details in the README files. Release Date: February 2, 2015 Last Updated: August 29, 2018 The benchmark download packages are available through the Globus-based NCAR Data Sharing Service. Instructions are given below for obtaining a Globus account, installing the required Globus software, and downloading the benchmark packages via the NCAR HPC Benchmarks endpoint. See the Globus instructions below for more information. Instructions for all benchmarks are available via Google Docs or direct download at this link: Application Benchmark Instructions \u00b6 These are the instructions for each of the application benchmarks in the table below. Application Benchmarks Description File Size (Bytes) MD5 Checksum HOMME HOMME benchmark and HOMME_COMM communication kernel HOMME_v1.tar.gz 2728264 b35d135f52b488d0bf9c1a07f2d02a93 HPCG High Performance Conjugate Gradient Solver hpcg-2.4_v1.tar.gz 69974 fef8b6614ddaf3c45b8dd1b8fb867df7 LES Large Eddy Simulation benchmark LES_v1.tar.gz 73200 f9017e36b1ea0f02a2169770b37fad54 MG2 Morrison Gettelman cloud microphysics kernel MG2kernel_v1.tar.gz 282822 53befeb7e418c074c80f6a5ad025144c MPAS-A MPAS Atmosphere benchmark MPAS_3.2_v1.tar.gz 2259609261 e9736920454952afb7e13c2e4f859457 POPperf POP Ocean model benchmark POPperf_v1.tar.gz 66480926 0fd078478dc6b5f326701ac09713fa49 WRF Weather Research and Forecasting model WRFV3_BENCH_v1.tar.gz 13260795166 4d5a7c02656dca8042cebe1e656c793b CESM Community Earth System Model Used in numerical correctness and system acceptance testing. http://www.cesm.ucar.edu/ I/O and Microbenchmarks Description File Size (Bytes) MD5 Checksum STREAM Node level memory benchmark STREAM_v1.tar.gz 10268 ee520d700a1fef3f746b9a8117952635 SHOC Scalable HeterOgeneous Computing (GPU benchmark) shoc_v1.tar.gz 10418387 f3a4146180cb720a04104ee40bd161ea OSU-MPI MPI communication benchmarks osu-micro-benchmarks-4.4.1_v1.tar.gz 151586 4bae164fc0aecd955adae1e9a9dc48d9 IOR I/O bandwidth and latency test ncar_ior_v1.tgz 144,608 dc91a37af717005c87ec1752524ef67b pyReshaper Application I/O kernel pyResBench_v1.tgz 1,938,268,372 67c9231e8bacb644d1a952b8793dc609 mdtest Metadata performance test ncar_mdtest_v1.tgz 93,074 c9f69c6cdc335409f96ebce7764babad Globus instructions \u00b6 Step 1: Obtain a Globus account \u00b6 Go to www.globus.org and click the Sign Up button in the upper-right corner. Step 2: Install Globus Connect Personal \u00b6 Go to www.globus.org . Under Products select Globus Connect and then Get Globus Connect Personal . Versions are available for Mac OS X, Linux and Windows. Step 3: Use Globus to download benchmarks \u00b6 Log in at www.globus.org with your Globus account Select Transfer Files In the left-hand window, enter NCAR HPC Benchmarks as the endpoint In the right-hand window, enter an endpoint at your own site or the endpoint that you established with Globus Connect Select the benchmark files you wish to download and click the right-hand arrow Your download will be submitted through the Globus Transfer service. You will receive an email when your transfer has completed or you can monitor from the Transfer window by selecting refresh list in the right-hand window.","title":"NCAR Benchmarking applications - 2015 release"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#ncar-benchmarking-applications-2015-release","text":"Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements. Thus, NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance. NCAR's benchmarking applications are listed in the tables below, along with file names, sizes, and checksums. These packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, you will find version number and download details in the README files. Release Date: February 2, 2015 Last Updated: August 29, 2018 The benchmark download packages are available through the Globus-based NCAR Data Sharing Service. Instructions are given below for obtaining a Globus account, installing the required Globus software, and downloading the benchmark packages via the NCAR HPC Benchmarks endpoint. See the Globus instructions below for more information. Instructions for all benchmarks are available via Google Docs or direct download at this link:","title":"NCAR benchmarking applications- 2015 release"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#application-benchmark-instructions","text":"These are the instructions for each of the application benchmarks in the table below. Application Benchmarks Description File Size (Bytes) MD5 Checksum HOMME HOMME benchmark and HOMME_COMM communication kernel HOMME_v1.tar.gz 2728264 b35d135f52b488d0bf9c1a07f2d02a93 HPCG High Performance Conjugate Gradient Solver hpcg-2.4_v1.tar.gz 69974 fef8b6614ddaf3c45b8dd1b8fb867df7 LES Large Eddy Simulation benchmark LES_v1.tar.gz 73200 f9017e36b1ea0f02a2169770b37fad54 MG2 Morrison Gettelman cloud microphysics kernel MG2kernel_v1.tar.gz 282822 53befeb7e418c074c80f6a5ad025144c MPAS-A MPAS Atmosphere benchmark MPAS_3.2_v1.tar.gz 2259609261 e9736920454952afb7e13c2e4f859457 POPperf POP Ocean model benchmark POPperf_v1.tar.gz 66480926 0fd078478dc6b5f326701ac09713fa49 WRF Weather Research and Forecasting model WRFV3_BENCH_v1.tar.gz 13260795166 4d5a7c02656dca8042cebe1e656c793b CESM Community Earth System Model Used in numerical correctness and system acceptance testing. http://www.cesm.ucar.edu/ I/O and Microbenchmarks Description File Size (Bytes) MD5 Checksum STREAM Node level memory benchmark STREAM_v1.tar.gz 10268 ee520d700a1fef3f746b9a8117952635 SHOC Scalable HeterOgeneous Computing (GPU benchmark) shoc_v1.tar.gz 10418387 f3a4146180cb720a04104ee40bd161ea OSU-MPI MPI communication benchmarks osu-micro-benchmarks-4.4.1_v1.tar.gz 151586 4bae164fc0aecd955adae1e9a9dc48d9 IOR I/O bandwidth and latency test ncar_ior_v1.tgz 144,608 dc91a37af717005c87ec1752524ef67b pyReshaper Application I/O kernel pyResBench_v1.tgz 1,938,268,372 67c9231e8bacb644d1a952b8793dc609 mdtest Metadata performance test ncar_mdtest_v1.tgz 93,074 c9f69c6cdc335409f96ebce7764babad","title":"Application Benchmark Instructions"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#globus-instructions","text":"","title":"Globus instructions"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#step-1-obtain-a-globus-account","text":"Go to www.globus.org and click the Sign Up button in the upper-right corner.","title":"Step 1: Obtain a Globus account"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#step-2-install-globus-connect-personal","text":"Go to www.globus.org . Under Products select Globus Connect and then Get Globus Connect Personal . Versions are available for Mac OS X, Linux and Windows.","title":"Step 2: Install Globus Connect Personal"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2015/#step-3-use-globus-to-download-benchmarks","text":"Log in at www.globus.org with your Globus account Select Transfer Files In the left-hand window, enter NCAR HPC Benchmarks as the endpoint In the right-hand window, enter an endpoint at your own site or the endpoint that you established with Globus Connect Select the benchmark files you wish to download and click the right-hand arrow Your download will be submitted through the Globus Transfer service. You will receive an email when your transfer has completed or you can monitor from the Transfer window by selecting refresh list in the right-hand window.","title":"Step 3: Use Globus to download benchmarks"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/","text":"NCAR benchmarking applications - 2019-2020 release \u00b6 31 March 2020 - Benchmark Q&As updated For questions and answers regarding the NWSC-3 HPC Benchmarks, refer to the updated NWSC-3 Benchmarks Q&As document. 19 March 2020 - Updated benchmarks released Please note that the NWSC-3 HPC Benchmarks have been updated to include changes to the GOES and OSU MPI benchmarks. Prospective Offerors for the NWSC-3 Request for Proposal (RFP), which will be released 2 April 2020, should download the updated benchmark code, input cases, and instructions (see below). NCAR does not plan to make any additional changes to the HPC Benchmarks unless there are issues with the ones provided here. 19 July 2019 - Benchmarks released The NWSC-3 HPC Benchmarks are available ahead of an anticipated release of the NWSC-3 Request for Proposal (RFP) in Q1 of 2020. NCAR does not plan to make any changes to the HPC Benchmarks unless there are issues with the ones provided here. Release Date: July 19, 2019 Last Updated: March 19, 2020 Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements. Thus, NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance. NCAR's benchmarking applications are listed in the table below, along with file names, sizes, and checksums. These packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, you will find version number and download details in the README files. Documentation and benchmark download packages are available through the Globus-based NCAR Data Sharing Service. Instructions are given below for obtaining a Globus account, installing the required Globus software, and downloading the benchmark packages via the NCAR HPC Benchmarks endpoint. See the Globus instructions below for more information. Benchmarks \u00b6 Name Description File Contents File Name Size (Bytes) MD5 Checksum CLUBB Physics Kernel Instructions CLUBB_2019-05-19.pdf 63246 006fb83f72d3cc4042c361701f837ea4 Benchmark Files CLUBB_2019-05-19.tar.gz 184931304 626d0c5108f76c662d8e722fa64cfe82 DART_WRF Model Kernel Instructions DART_WRF_2019-05-20.pdf 59749 cbe78c13a80073910c0275467c52ee2c Benchmark Files DART_WRF_2019-05-20.tar.gz 258607013 a388e961fce0960d096417cd45e1f00f GOES ML Benchmark Instructions GOES16_2020-04-27.pdf 71665 1d9ba301526d25cd768304e14fa5ab14 Benchmark Files GOES16_2020-04-27.tar.gz 3578032111 5517e495689c75c4f71478d5d3f45e7e MG2 Physics Kernel Instructions MG2_2019-05-20.pdf 61804 c13a288f425993504ab9ce5db692c008 Benchmark Files MG2_2019-05-20.tar.gz 85366943 25ebc145a374d3ccd1d410f9d495261a MPAS-A* GPU-capable Atmospheric Model Instructions MPAS_2019-06-26.pdf 274813 cf523aa8e3a9d889d11817d0d07edca9 Benchmark Files (For access, follow the instructions below) Input Data MPAS_2020-04-27_data.tar.gz 17270465681 0ced450ce164b86cb9a6c82a5dcfd966 Stream Memory Bandwidth Instructions Stream_2019-05-22.pdf 69436 577d9da38eed93d782d6a046c36d7353 Benchmark Files Stream_2019-05-22.tar.gz 20743 23d9d58f8d709553c7e409ab1b1e44cc WACCM Physics Kernel Instructions WACCM_2019-05-19.pdf 62391 6bb2d8bd1df3471ad93a17540e2c2c17 Benchmark Files WACCM_2019-05-19.tar.gz 18289529 b31c36c58f5ecbbb613caaa39b663b32 WRF Weather Research and Forecasting (WRF) Model Instructions WRF_2019-09-06.pdf 93213 2a47071030e4417d4873938333a60af9 Benchmark Files WRF_2019-09-06.tar.gz 9778202346 bec5bf5cc682b14ebb30a2da51d381ab OSU MPI MPI Communications Benchmark Instructions osu-micro-benchmarks-5.5_2020-03-12.pdf 62732 4baea167d0698973e751b75e578ac6bb Benchmark Files osu-micro-benchmarks-5.5_2020-03-12.tar.gz 765369 bcb970d5a1f3424e2c7302ff60611008 Globus instructions \u00b6 Step 1: Obtain a Globus account \u00b6 Go to www.globus.org and click the Sign Up button in the upper-right corner. Step 2: Install Globus Connect Personal \u00b6 Go to https://www.globus.org/globus-connect-personal and install the version of Globus Connect Personal appropriate for your computer. Versions are available for Mac OS X, Linux, and Windows. Step 3: Use Globus to download benchmarks \u00b6 Access the NCAR HPC Benchmarks folder on Globus. (You will need to log in to Globus with the account created in Step 1.) Select the files you wish to download and click Transfer or Sync to in the right-hand pane. Select the endpoint you wish to transfer files to. This can be the computer where you installed Globus Connect Personal in Step 2, or another Globus endpoint to which you have access. Click on the Start button below the file manager to initiate the transfer. Your download will be submitted through the Globus Transfer service. You will receive an email when your transfer has completed. You can monitor the transfer by clicking Activity in the left-hand menu to bring up the Globus Activity view. MPAS-A benchmark access Access to the MPAS-A benchmark code is restricted. To obtain access, follow the instructions below. Instructions for obtaining NWSC-3 MPAS-A benchmark source code Code releases for the MPAS-A GPU project will occur through the open-source GitHub site. However, before you may access the site, you are required to sign the MPAS-A Confidentiality Agreement . To obtain access to the MPAS-A GPU GitHub site, send both your signed MPAS-A Confidentiality Agreement and your GitHub account/login to Alison Propes , UCAR's Subcontract/Procurement Manager. Note that all materials (including source code, products derived from source code, and documents) related to NWSC-3 MPAS should not be distributed, either formally or informally, in any form. Publishing any kind of results obtained from the NWSC-3 MPAS-A source code requires written consent from UCAR.","title":"NCAR Benchmarking applications - 2019-2020 release"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#ncar-benchmarking-applications-2019-2020-release","text":"31 March 2020 - Benchmark Q&As updated For questions and answers regarding the NWSC-3 HPC Benchmarks, refer to the updated NWSC-3 Benchmarks Q&As document. 19 March 2020 - Updated benchmarks released Please note that the NWSC-3 HPC Benchmarks have been updated to include changes to the GOES and OSU MPI benchmarks. Prospective Offerors for the NWSC-3 Request for Proposal (RFP), which will be released 2 April 2020, should download the updated benchmark code, input cases, and instructions (see below). NCAR does not plan to make any additional changes to the HPC Benchmarks unless there are issues with the ones provided here. 19 July 2019 - Benchmarks released The NWSC-3 HPC Benchmarks are available ahead of an anticipated release of the NWSC-3 Request for Proposal (RFP) in Q1 of 2020. NCAR does not plan to make any changes to the HPC Benchmarks unless there are issues with the ones provided here. Release Date: July 19, 2019 Last Updated: March 19, 2020 Ensuring that real applications perform well on NCAR computing platforms is critical for getting the best value out of complex and costly high-performance computing and storage resources. Climate and weather applications are large, often with millions of lines of code, and are generally difficult to configure in a way that permits ease of use for things such as system deployments, upgrades, and procurements. Thus, NCAR has developed a suite of application kernels, micro-benchmarks, and full applications with moderate input cases that can be used as proxies for the full applications and still provide meaningful information and insights into system performance. A few of these are well-known benchmarks that are commonly used in HPC for characterizing system performance. NCAR's benchmarking applications are listed in the table below, along with file names, sizes, and checksums. These packages include source files, build scripts, and input data sets required to compile and run the applications. In cases where the benchmarks depend on applications and libraries that are not part of the package distributions, you will find version number and download details in the README files. Documentation and benchmark download packages are available through the Globus-based NCAR Data Sharing Service. Instructions are given below for obtaining a Globus account, installing the required Globus software, and downloading the benchmark packages via the NCAR HPC Benchmarks endpoint. See the Globus instructions below for more information.","title":"NCAR benchmarking applications - 2019-2020 release"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#benchmarks","text":"Name Description File Contents File Name Size (Bytes) MD5 Checksum CLUBB Physics Kernel Instructions CLUBB_2019-05-19.pdf 63246 006fb83f72d3cc4042c361701f837ea4 Benchmark Files CLUBB_2019-05-19.tar.gz 184931304 626d0c5108f76c662d8e722fa64cfe82 DART_WRF Model Kernel Instructions DART_WRF_2019-05-20.pdf 59749 cbe78c13a80073910c0275467c52ee2c Benchmark Files DART_WRF_2019-05-20.tar.gz 258607013 a388e961fce0960d096417cd45e1f00f GOES ML Benchmark Instructions GOES16_2020-04-27.pdf 71665 1d9ba301526d25cd768304e14fa5ab14 Benchmark Files GOES16_2020-04-27.tar.gz 3578032111 5517e495689c75c4f71478d5d3f45e7e MG2 Physics Kernel Instructions MG2_2019-05-20.pdf 61804 c13a288f425993504ab9ce5db692c008 Benchmark Files MG2_2019-05-20.tar.gz 85366943 25ebc145a374d3ccd1d410f9d495261a MPAS-A* GPU-capable Atmospheric Model Instructions MPAS_2019-06-26.pdf 274813 cf523aa8e3a9d889d11817d0d07edca9 Benchmark Files (For access, follow the instructions below) Input Data MPAS_2020-04-27_data.tar.gz 17270465681 0ced450ce164b86cb9a6c82a5dcfd966 Stream Memory Bandwidth Instructions Stream_2019-05-22.pdf 69436 577d9da38eed93d782d6a046c36d7353 Benchmark Files Stream_2019-05-22.tar.gz 20743 23d9d58f8d709553c7e409ab1b1e44cc WACCM Physics Kernel Instructions WACCM_2019-05-19.pdf 62391 6bb2d8bd1df3471ad93a17540e2c2c17 Benchmark Files WACCM_2019-05-19.tar.gz 18289529 b31c36c58f5ecbbb613caaa39b663b32 WRF Weather Research and Forecasting (WRF) Model Instructions WRF_2019-09-06.pdf 93213 2a47071030e4417d4873938333a60af9 Benchmark Files WRF_2019-09-06.tar.gz 9778202346 bec5bf5cc682b14ebb30a2da51d381ab OSU MPI MPI Communications Benchmark Instructions osu-micro-benchmarks-5.5_2020-03-12.pdf 62732 4baea167d0698973e751b75e578ac6bb Benchmark Files osu-micro-benchmarks-5.5_2020-03-12.tar.gz 765369 bcb970d5a1f3424e2c7302ff60611008","title":"Benchmarks"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#globus-instructions","text":"","title":"Globus instructions"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#step-1-obtain-a-globus-account","text":"Go to www.globus.org and click the Sign Up button in the upper-right corner.","title":"Step 1: Obtain a Globus account"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#step-2-install-globus-connect-personal","text":"Go to https://www.globus.org/globus-connect-personal and install the version of Globus Connect Personal appropriate for your computer. Versions are available for Mac OS X, Linux, and Windows.","title":"Step 2: Install Globus Connect Personal"},{"location":"environment-and-software/benchmarks/ncar-benchmarking-2019-2020/#step-3-use-globus-to-download-benchmarks","text":"Access the NCAR HPC Benchmarks folder on Globus. (You will need to log in to Globus with the account created in Step 1.) Select the files you wish to download and click Transfer or Sync to in the right-hand pane. Select the endpoint you wish to transfer files to. This can be the computer where you installed Globus Connect Personal in Step 2, or another Globus endpoint to which you have access. Click on the Start button below the file manager to initiate the transfer. Your download will be submitted through the Globus Transfer service. You will receive an email when your transfer has completed. You can monitor the transfer by clicking Activity in the left-hand menu to bring up the Globus Activity view. MPAS-A benchmark access Access to the MPAS-A benchmark code is restricted. To obtain access, follow the instructions below. Instructions for obtaining NWSC-3 MPAS-A benchmark source code Code releases for the MPAS-A GPU project will occur through the open-source GitHub site. However, before you may access the site, you are required to sign the MPAS-A Confidentiality Agreement . To obtain access to the MPAS-A GPU GitHub site, send both your signed MPAS-A Confidentiality Agreement and your GitHub account/login to Alison Propes , UCAR's Subcontract/Procurement Manager. Note that all materials (including source code, products derived from source code, and documents) related to NWSC-3 MPAS should not be distributed, either formally or informally, in any form. Publishing any kind of results obtained from the NWSC-3 MPAS-A source code requires written consent from UCAR.","title":"Step 3: Use Globus to download benchmarks"},{"location":"environment-and-software/hpc-software/","text":"Software for HPC users \u00b6 NCAR and CISL have developed and support a number of freely available software packages for visualization, data analysis, weather prediction, and high-performance computation. These include models that help researchers understand the impact of regional and global climate change, and tools and libraries for analyzing and visualizing data. Software available on the HPC and data analysis systems that CISL operates also includes open-source and commercial third-party products used for programming, analysis, and file management tasks. Image from the CISL Visualization Gallery Full Software List \u00b6 Derecho Casper ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- apptainer: apptainer/1.1.7, apptainer/1.1.9 arm-forge: arm-forge/22.1.3 atp: atp/3.14.18, atp/3.15.0 cce: cce/15.0.1 cdo: cdo/2.1.1, cdo/2.2.2 charliecloud: charliecloud/0.32, charliecloud/0.33 cmake: cmake/3.26.3 conda: conda/latest cp2k: cp2k/2023.2 cray-ccdb: cray-ccdb/4.12.13 cray-dyninst: cray-dyninst/12.1.1 cray-libsci: cray-libsci/23.02.1.1 cray-mpich: cray-mpich/8.1.25 cray-mrnet: cray-mrnet/5.0.4 cray-stat: cray-stat/4.11.13 craype: craype/2.7.20 cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1 cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8 cutensor: cutensor/1.7.0.1 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 eccodes: eccodes/2.25.0 ecflow: ecflow/5.8.3 esmf: esmf/8.4.2, esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.6.4, gdal/3.7.1 gdb4hpc: gdb4hpc/4.14.7 geos: geos/3.9.1 go: go/1.20.3, go/1.20.6 gptl: gptl/8.1.1 grads: grads/2.2.1, grads/2.2.3 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 idl: idl/8.9.0 intel: intel/2023.0.0, intel/2023.2.1 intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1 intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0 intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 matlab: matlab/R2023a mkl: mkl/2023.0.0, mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 mpifileutils: mpifileutils/0.11.1 mvapich: mvapich/3.0b ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.06, ncarenv/23.09 nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.4, nco/5.1.6 ncview: ncview/2.1.8, ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7 osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ... papi: papi/7.0.0.1 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ... pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perftools: perftools perftools-base: perftools-base/23.03.0 perftools-lite: perftools-lite perftools-lite-events: perftools-lite-events perftools-lite-gpu: perftools-lite-gpu perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload perl: perl/5.36.0, perl/5.38.0 podman: podman/4.3.1, podman/4.5.1 proj: proj/8.2.1 rstudio: rstudio/2023.09.0 sanitizers4hpc: sanitizers4hpc/1.0.4 superlu: superlu/5.3.0 superlu-dist: superlu-dist/8.1.2 texlive: texlive/20220321 udunits: udunits/2.2.28 valgrind4hpc: valgrind4hpc/2.12.11 vtune: vtune/2023.0.0, vtune/2023.2.0 ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ---------------------------------------------------------------------------- ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- adios2: adios2/2.9.1 apptainer: apptainer/1.1.9 cdo: cdo/2.2.2 charliecloud: charliecloud/0.33 clang: clang/16.0.6 cmake: cmake/3.26.3 conda: conda/latest cuda: cuda/11.8.0 cudnn: cudnn/8.7.0.84-11.8 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 doxygen: doxygen/1.8.20 eccodes: eccodes/2.25.0 eigen: eigen/3.4.0 esmf: esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.7.1 geos: geos/3.9.1 go: go/1.20.6 grads: grads/2.2.3 grib-util: grib-util/1.2.4 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 intel: intel/2023.2.1 intel-classic: intel-classic/2023.2.1 intel-oneapi: intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 mkl: mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.09 nccmp: nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.6 ncview: ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/23.7 octave: octave/8.2.0 openblas: openblas/0.3.23 openmpi: openmpi/4.1.5 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/2.6.1, parallelio/2.6.2 paraview: paraview/5.11.1 pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perl: perl/5.38.0 pocl: pocl/3.0 podman: podman/4.5.1 proj: proj/8.2.1 texlive: texlive/20220321 ucx: ucx/1.14.1 udunits: udunits/2.2.28 vapor: vapor/3.9.0 vexcl: vexcl/1.4.3 visit: visit/3.3.3 vtune: vtune/2023.2.0 wgrib2: wgrib2/3.1.1 xxdiff: xxdiff/latest ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ---------------------------------------------------------------------------- For information about a system's unique user environment, consult the documentation for that resource at these links: Derecho documentation Casper documentation Also, see these pages for information regarding software packages and tools that are available for use on our systems: Community models Data analysis and visualization Environment modules Tools for debugging, profiling, and optimizing code Utilities","title":"Software for HPC users"},{"location":"environment-and-software/hpc-software/#software-for-hpc-users","text":"NCAR and CISL have developed and support a number of freely available software packages for visualization, data analysis, weather prediction, and high-performance computation. These include models that help researchers understand the impact of regional and global climate change, and tools and libraries for analyzing and visualizing data. Software available on the HPC and data analysis systems that CISL operates also includes open-source and commercial third-party products used for programming, analysis, and file management tasks. Image from the CISL Visualization Gallery","title":"Software for HPC users"},{"location":"environment-and-software/hpc-software/#full-software-list","text":"Derecho Casper ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- apptainer: apptainer/1.1.7, apptainer/1.1.9 arm-forge: arm-forge/22.1.3 atp: atp/3.14.18, atp/3.15.0 cce: cce/15.0.1 cdo: cdo/2.1.1, cdo/2.2.2 charliecloud: charliecloud/0.32, charliecloud/0.33 cmake: cmake/3.26.3 conda: conda/latest cp2k: cp2k/2023.2 cray-ccdb: cray-ccdb/4.12.13 cray-dyninst: cray-dyninst/12.1.1 cray-libsci: cray-libsci/23.02.1.1 cray-mpich: cray-mpich/8.1.25 cray-mrnet: cray-mrnet/5.0.4 cray-stat: cray-stat/4.11.13 craype: craype/2.7.20 cuda: cuda/11.7.1, cuda/11.8.0, cuda/12.2.1 cudnn: cudnn/8.5.0.96-11.7, cudnn/8.7.0.84-11.8 cutensor: cutensor/1.7.0.1 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 eccodes: eccodes/2.25.0 ecflow: ecflow/5.8.3 esmf: esmf/8.4.2, esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.6.4, gdal/3.7.1 gdb4hpc: gdb4hpc/4.14.7 geos: geos/3.9.1 go: go/1.20.3, go/1.20.6 gptl: gptl/8.1.1 grads: grads/2.2.1, grads/2.2.3 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 idl: idl/8.9.0 intel: intel/2023.0.0, intel/2023.2.1 intel-classic: intel-classic/2023.0.0, intel-classic/2023.2.1 intel-mpi: intel-mpi/2021.8.0, intel-mpi/2021.10.0 intel-oneapi: intel-oneapi/2023.0.0, intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 matlab: matlab/R2023a mkl: mkl/2023.0.0, mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 mpifileutils: mpifileutils/0.11.1 mvapich: mvapich/3.0b ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.06, ncarenv/23.09 nccmp: nccmp/1.9.0.1, nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.4, nco/5.1.6 ncview: ncview/2.1.8, ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/21.3, nvhpc/23.1, nvhpc/23.5, nvhpc/23.7 osu-micro-benchmarks: osu-micro-benchmarks/7.1-1, ... papi: papi/7.0.0.1 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/1.10.1, parallelio/2.5.10, parallelio/2.6.0, ... pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perftools: perftools perftools-base: perftools-base/23.03.0 perftools-lite: perftools-lite perftools-lite-events: perftools-lite-events perftools-lite-gpu: perftools-lite-gpu perftools-lite-hbm: perftools-lite-hbm perftools-lite-loops: perftools-lite-loops perftools-preload: perftools-preload perl: perl/5.36.0, perl/5.38.0 podman: podman/4.3.1, podman/4.5.1 proj: proj/8.2.1 rstudio: rstudio/2023.09.0 sanitizers4hpc: sanitizers4hpc/1.0.4 superlu: superlu/5.3.0 superlu-dist: superlu-dist/8.1.2 texlive: texlive/20220321 udunits: udunits/2.2.28 valgrind4hpc: valgrind4hpc/2.12.11 vtune: vtune/2023.0.0, vtune/2023.2.0 ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ---------------------------------------------------------------------------- ---------------------------------------------------------------------------- The following is a list of the modules and extensions currently available: ---------------------------------------------------------------------------- adios2: adios2/2.9.1 apptainer: apptainer/1.1.9 cdo: cdo/2.2.2 charliecloud: charliecloud/0.33 clang: clang/16.0.6 cmake: cmake/3.26.3 conda: conda/latest cuda: cuda/11.8.0 cudnn: cudnn/8.7.0.84-11.8 darshan-runtime: darshan-runtime/3.4.2 darshan-util: darshan-util/3.4.2 doxygen: doxygen/1.8.20 eccodes: eccodes/2.25.0 eigen: eigen/3.4.0 esmf: esmf/8.5.0 fftw: fftw/3.3.10 fftw-mpi: fftw-mpi/3.3.10 gcc: gcc/12.2.0 gdal: gdal/3.7.1 geos: geos/3.9.1 go: go/1.20.6 grads: grads/2.2.3 grib-util: grib-util/1.2.4 hdf: hdf/4.2.15 hdf5: hdf5/1.12.2 hdf5-mpi: hdf5-mpi/1.12.2 intel: intel/2023.2.1 intel-classic: intel-classic/2023.2.1 intel-oneapi: intel-oneapi/2023.2.1 julia: julia/1.9.2 linaro-forge: linaro-forge/23.0 mkl: mkl/2023.2.0 mpi-serial: mpi-serial/2.3.0 ncarcompilers: ncarcompilers/1.0.0 ncarenv: ncarenv/23.09 nccmp: nccmp/1.9.1.0 ncl: ncl/6.6.2 nco: nco/5.1.6 ncview: ncview/2.1.9 ncvis: ncvis/2022.08.28 netcdf: netcdf/4.9.2 netcdf-mpi: netcdf-mpi/4.9.2 nvhpc: nvhpc/23.7 octave: octave/8.2.0 openblas: openblas/0.3.23 openmpi: openmpi/4.1.5 parallel-netcdf: parallel-netcdf/1.12.3 parallelio: parallelio/2.6.1, parallelio/2.6.2 paraview: paraview/5.11.1 pcre: pcre/8.45 peak-memusage: peak-memusage/3.0.1 perl: perl/5.38.0 pocl: pocl/3.0 podman: podman/4.5.1 proj: proj/8.2.1 texlive: texlive/20220321 ucx: ucx/1.14.1 udunits: udunits/2.2.28 vapor: vapor/3.9.0 vexcl: vexcl/1.4.3 visit: visit/3.3.3 vtune: vtune/2023.2.0 wgrib2: wgrib2/3.1.1 xxdiff: xxdiff/latest ---------------------------------------------------------------------------- To learn more about a package execute: $ module spider Foo where \"Foo\" is the name of a module. To find detailed information about a particular package you must specify the version if there is more than one version: $ module spider Foo/11.1 ---------------------------------------------------------------------------- For information about a system's unique user environment, consult the documentation for that resource at these links: Derecho documentation Casper documentation Also, see these pages for information regarding software packages and tools that are available for use on our systems: Community models Data analysis and visualization Environment modules Tools for debugging, profiling, and optimizing code Utilities","title":"Full Software List"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/intel-parallel-studio-xe/","text":"","title":"Intel Parallel Studio XE"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/","text":"Debugging and profiling with Forge tools \u00b6 The Linaro Forge tools \u2013 which include DDT , MAP , and Performance Reports \u2013 are provided for debugging, profiling, and optimizing code. Forge tools can be used with Fortran, C, C++, and Python code, and both CPU and GPU code can be analyzed. While Forge can be used with both serial and parallel applications, its real strength is in profiling large MPI codes that span many nodes \u2013 a task that is very challenging with traditional debugging and profiling tools like gdb and gprof. To get started, first configure the client interface on your local machine by following the recommended procedures below. This will allow you to begin debugging and profiling jobs. Note These tools were formerly known as Arm Forge (and Allinea Forge before that). Modules for versions before 23.0 will be found under the old arm-forge name. Preparing your code \u00b6 For compiled codes (e.g., Fortran/C/C++), you will need to add debug symbols to the binary to allow DDT and MAP to sample the program during execution. However, you do not need to add these symbols when using Performance Reports. CPU code: Use the -g option when you compile your code before debugging or profiling. CUDA code: Include both the -g and -G options for the NVIDIA compilers to debug GPU code. Do not move or remove the source code, binary, or executable files from the directory or directories in which you compiled them. Preparing applications that use Cray-MPICH MPI for profiling \u00b6 A limitation currently exists in Cray's Common Tools Interface which prevents preloading of the MPI sampling library at profiling time. This limitation means that any profiling done on codes that use Cray-MPICH MPI with either MAP or Performance Reports will require manual creation of the sampler library and linking of this library into your application executable. For example, let's suppose you wish to sample a simple MPI model called cfd.exe . Here is how you would need to link the executable: # First, create the sampler library module load linaro-forge make-profiler-libraries --platform = cray --lib-type = shared # Now, link sampler library into executable mpicc -L $( pwd ) -lmap-sampler-pmpi -lmap-sampler -Wl,--eh-frame-hdr -Wl,-rpath = $( pwd ) -o cfd.exe main.o driver.o physics.o These extra linking steps can be challenging to incorporate into some custom build systems. In this case, you can leverage flags supported by the ncarcompilers wrapper to avoid injecting the necessary flags manually. For example: # First, create the sampler library module load linaro-forge ncarcompilers make-profiler-libraries --platform = cray --lib-type = shared # While building the application, link the sampler using the NCAR wrapper (bash syntax) NCAR_LDFLAGS_FORGE = $( pwd ) NCAR_LIBS_FORGE = \"-lmap-sampler-pmpi -lmap-sampler -Wl,--eh-frame-hdr\" ./build.sh Using DDT or MAP via a Remote Connection \u00b6 Both DDT and MAP feature complex graphical user interfaces (GUIs) that perform best when run on your local machine. To support this workflow, these tools provide a \"remote-connect\" option that allows your application to run on the remote system (e.g., Derecho) while the GUI runs on your workstation. (The tools also run from your command line interface). Client interface setup \u00b6 The client software version that you use locally and the server version that you use on Derecho must be the same. We recommend using the latest version available. Run module avail linaro-forge to identify the latest version. Procedure \u00b6 Download the client software from the Linaro site . Install and start the client on your local machine. From the \"Remote Launch\" menu (see image), select Configure . Configure as shown in the following image. The configuration will apply to both DDT and MAP, so you only need to do it once. Enter your username followed by @ and the connection name ( derecho.hpc.ucar.edu , for example) in the \u201cHost Name\u201d field. Then, fill in the \u201cRemote Installation Directory\u201d field. Once you have loaded the linaro-forge module, you can get the installation directory by echoing the following variable: echo $NCAR_ROOT_LINARO_FORGE Click OK . Running a script \u00b6 Prepare a job script. Specify the \"main\" submission queue on Derecho and customize the script with your own project code, job name, and so on. On the last line of your script, use ddt --connect (or map --connect ) instead of mpiexec . ddt --connect ./my_executable Submit your job when indicated below. Procedure \u00b6 Start the client interface on your local machine. From the \"Remote Launch\" menu, select the name of the host configuration you created in the previous step. When the following dialog box appears, authenticate as usual. (It may be necessary to click Show Terminal to see the authentication window). After you log in, return to your normal terminal window and load the modules you need. (We recommend including module load commands in your job scripts). module load linaro-forge/23.0 Debugging GPU Jobs To debug GPU jobs, you may need to set the environment variable CUDA_DEBUGGER_SOFTWARE_PREEMPTION to 1. export CUDA_DEBUGGER_SOFTWARE_PREEMPTION = 1 Submit your job script on your command line as in this example: qsub my-debug-script.bash When your job starts, the GUI will show that a \u201cReverse Connect Request\u201d has been made. Accept the request to continue. A \u201cRun\u201d window will open and display settings imported from your job script. Review the settings. If your program uses Cray MPICH, make sure the MPI is specified as Cray PALS where shown in the following image. After reviewing the settings, click Run and the DDT or MAP window will open. Quit when you\u2019re finished so the license is available to other users. Performance Reports \u00b6 Performance Reports is another profiling tool provided by Linaro Forge. It summarizes the performance, memory usage, I/O, and more of application runs. To generate a performance report, submit a batch job which runs your application prepended by the perf-report command (there is no remote connection mode). You do not need to compile your application with the -g debug option first. Modify your batch script to load the linaro-forge module that you want to use and include perf-report as shown in the sample scripts below. When your job runs, the output will include both text and HTML report files. For additional information, see the Linaro Forge product documentation . Sample bash script \u00b6 #!/bin/bash #PBS -N pr-job #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q main #PBS -j oe #PBS -l select=2:ncpus=128:mpiprocs=128 module load linaro-forge/23.0 export TMPDIR = $SCRATCH /temp mkdir -p $TMPDIR ### Run the executable perf-report --mpi -n 256 ./executable_name.exe","title":"Debugging and Profiling with Forge Tools"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#debugging-and-profiling-with-forge-tools","text":"The Linaro Forge tools \u2013 which include DDT , MAP , and Performance Reports \u2013 are provided for debugging, profiling, and optimizing code. Forge tools can be used with Fortran, C, C++, and Python code, and both CPU and GPU code can be analyzed. While Forge can be used with both serial and parallel applications, its real strength is in profiling large MPI codes that span many nodes \u2013 a task that is very challenging with traditional debugging and profiling tools like gdb and gprof. To get started, first configure the client interface on your local machine by following the recommended procedures below. This will allow you to begin debugging and profiling jobs. Note These tools were formerly known as Arm Forge (and Allinea Forge before that). Modules for versions before 23.0 will be found under the old arm-forge name.","title":"Debugging and profiling with Forge tools"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#preparing-your-code","text":"For compiled codes (e.g., Fortran/C/C++), you will need to add debug symbols to the binary to allow DDT and MAP to sample the program during execution. However, you do not need to add these symbols when using Performance Reports. CPU code: Use the -g option when you compile your code before debugging or profiling. CUDA code: Include both the -g and -G options for the NVIDIA compilers to debug GPU code. Do not move or remove the source code, binary, or executable files from the directory or directories in which you compiled them.","title":"Preparing your code"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#preparing-applications-that-use-cray-mpich-mpi-for-profiling","text":"A limitation currently exists in Cray's Common Tools Interface which prevents preloading of the MPI sampling library at profiling time. This limitation means that any profiling done on codes that use Cray-MPICH MPI with either MAP or Performance Reports will require manual creation of the sampler library and linking of this library into your application executable. For example, let's suppose you wish to sample a simple MPI model called cfd.exe . Here is how you would need to link the executable: # First, create the sampler library module load linaro-forge make-profiler-libraries --platform = cray --lib-type = shared # Now, link sampler library into executable mpicc -L $( pwd ) -lmap-sampler-pmpi -lmap-sampler -Wl,--eh-frame-hdr -Wl,-rpath = $( pwd ) -o cfd.exe main.o driver.o physics.o These extra linking steps can be challenging to incorporate into some custom build systems. In this case, you can leverage flags supported by the ncarcompilers wrapper to avoid injecting the necessary flags manually. For example: # First, create the sampler library module load linaro-forge ncarcompilers make-profiler-libraries --platform = cray --lib-type = shared # While building the application, link the sampler using the NCAR wrapper (bash syntax) NCAR_LDFLAGS_FORGE = $( pwd ) NCAR_LIBS_FORGE = \"-lmap-sampler-pmpi -lmap-sampler -Wl,--eh-frame-hdr\" ./build.sh","title":"Preparing applications that use Cray-MPICH MPI for profiling"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#using-ddt-or-map-via-a-remote-connection","text":"Both DDT and MAP feature complex graphical user interfaces (GUIs) that perform best when run on your local machine. To support this workflow, these tools provide a \"remote-connect\" option that allows your application to run on the remote system (e.g., Derecho) while the GUI runs on your workstation. (The tools also run from your command line interface).","title":"Using DDT or MAP via a Remote Connection"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#client-interface-setup","text":"The client software version that you use locally and the server version that you use on Derecho must be the same. We recommend using the latest version available. Run module avail linaro-forge to identify the latest version.","title":"Client interface setup"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#procedure","text":"Download the client software from the Linaro site . Install and start the client on your local machine. From the \"Remote Launch\" menu (see image), select Configure . Configure as shown in the following image. The configuration will apply to both DDT and MAP, so you only need to do it once. Enter your username followed by @ and the connection name ( derecho.hpc.ucar.edu , for example) in the \u201cHost Name\u201d field. Then, fill in the \u201cRemote Installation Directory\u201d field. Once you have loaded the linaro-forge module, you can get the installation directory by echoing the following variable: echo $NCAR_ROOT_LINARO_FORGE Click OK .","title":"Procedure"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#running-a-script","text":"Prepare a job script. Specify the \"main\" submission queue on Derecho and customize the script with your own project code, job name, and so on. On the last line of your script, use ddt --connect (or map --connect ) instead of mpiexec . ddt --connect ./my_executable Submit your job when indicated below.","title":"Running a script"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#procedure_1","text":"Start the client interface on your local machine. From the \"Remote Launch\" menu, select the name of the host configuration you created in the previous step. When the following dialog box appears, authenticate as usual. (It may be necessary to click Show Terminal to see the authentication window). After you log in, return to your normal terminal window and load the modules you need. (We recommend including module load commands in your job scripts). module load linaro-forge/23.0 Debugging GPU Jobs To debug GPU jobs, you may need to set the environment variable CUDA_DEBUGGER_SOFTWARE_PREEMPTION to 1. export CUDA_DEBUGGER_SOFTWARE_PREEMPTION = 1 Submit your job script on your command line as in this example: qsub my-debug-script.bash When your job starts, the GUI will show that a \u201cReverse Connect Request\u201d has been made. Accept the request to continue. A \u201cRun\u201d window will open and display settings imported from your job script. Review the settings. If your program uses Cray MPICH, make sure the MPI is specified as Cray PALS where shown in the following image. After reviewing the settings, click Run and the DDT or MAP window will open. Quit when you\u2019re finished so the license is available to other users.","title":"Procedure"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#performance-reports","text":"Performance Reports is another profiling tool provided by Linaro Forge. It summarizes the performance, memory usage, I/O, and more of application runs. To generate a performance report, submit a batch job which runs your application prepended by the perf-report command (there is no remote connection mode). You do not need to compile your application with the -g debug option first. Modify your batch script to load the linaro-forge module that you want to use and include perf-report as shown in the sample scripts below. When your job runs, the output will include both text and HTML report files. For additional information, see the Linaro Forge product documentation .","title":"Performance Reports"},{"location":"environment-and-software/hpc-software/profiling-and-debuggers/running-ddt-map-and-pr-jobs/#sample-bash-script","text":"#!/bin/bash #PBS -N pr-job #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q main #PBS -j oe #PBS -l select=2:ncpus=128:mpiprocs=128 module load linaro-forge/23.0 export TMPDIR = $SCRATCH /temp mkdir -p $TMPDIR ### Run the executable perf-report --mpi -n 256 ./executable_name.exe","title":"Sample bash script"},{"location":"environment-and-software/user-environment/","text":"Overview \u00b6 Users have the ability to customize and tailor their software environment to meet specific needs, using three different processes depending on use case: Modules vs. Conda - what's right for my use case? The two main methods for gaining access to software are through the module systsem or conda environments. Understandably, this complexity is often the source of confusion. In general, users should prefer the module system when leveraging performance-critical parallel computing, particularly with compiled languages and MPI. NCAR's modules provide access to many versions of popular compiler suites, MPI implementations, and dependent software libraries, all compiled to achieve maximum performance on a particular host system CPU architecture. Conversely, Conda provides a convenient mechanism for creating and sharing stand-alone, consistent software environments; particularly for Python and R usage. Conda environments are commonplace in data analysis and AI/ML workflows, where software packages often rely on a precise set of specifically configured dependencies. Conda packages typically work well in an HPC environment, with MPI being a common notable exception: Conda-provided MPI implementations are usually configured for generic systems, and likely do not achieve optimal performance on \"exotic\" HPC networks. If you run into difficulties with Conda and MPI packages, reach out to consulting . Modules \u00b6 Software modules helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, environment variables, and dependencies automatically, allowing many versions of software packages to be available across the system, built with different combinations of compilers, communications libraries, etc... Modules are a standard feature on most multi-user HPC systems to allow for multiple, potentially conflicting versions of compilers and software to be available to different users simultaneously. See our Modules page for a full discussion of module usage. Conda \u00b6 Conda is an open source package management system and environment management system that runs on Windows, macOS, and Linux. Conda installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language. Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. ( For more details, see the conda project documentation . ) Conda within an HPC Environment \u00b6 NCAR system users access Python via Conda environments , which are self-contained installations of Python itself, Python packages, and the software dependencies those packages rely on. We provide a common conda module available through the module system , and encourage all users to leverage this common installation. Prefer NCAR's common conda environment While it is possible for users to install conda into their own personal workspaces, we discourage this approach. Instead, users should investigate the common conda environment module available through the module system . Using this common installation facilitates sharing environments between colleagues and is configured internally to make optimum use of the NCAR GLADE file spaces . Containers \u00b6","title":"Overview"},{"location":"environment-and-software/user-environment/#overview","text":"Users have the ability to customize and tailor their software environment to meet specific needs, using three different processes depending on use case: Modules vs. Conda - what's right for my use case? The two main methods for gaining access to software are through the module systsem or conda environments. Understandably, this complexity is often the source of confusion. In general, users should prefer the module system when leveraging performance-critical parallel computing, particularly with compiled languages and MPI. NCAR's modules provide access to many versions of popular compiler suites, MPI implementations, and dependent software libraries, all compiled to achieve maximum performance on a particular host system CPU architecture. Conversely, Conda provides a convenient mechanism for creating and sharing stand-alone, consistent software environments; particularly for Python and R usage. Conda environments are commonplace in data analysis and AI/ML workflows, where software packages often rely on a precise set of specifically configured dependencies. Conda packages typically work well in an HPC environment, with MPI being a common notable exception: Conda-provided MPI implementations are usually configured for generic systems, and likely do not achieve optimal performance on \"exotic\" HPC networks. If you run into difficulties with Conda and MPI packages, reach out to consulting .","title":"Overview"},{"location":"environment-and-software/user-environment/#modules","text":"Software modules helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, environment variables, and dependencies automatically, allowing many versions of software packages to be available across the system, built with different combinations of compilers, communications libraries, etc... Modules are a standard feature on most multi-user HPC systems to allow for multiple, potentially conflicting versions of compilers and software to be available to different users simultaneously. See our Modules page for a full discussion of module usage.","title":"Modules"},{"location":"environment-and-software/user-environment/#conda","text":"Conda is an open source package management system and environment management system that runs on Windows, macOS, and Linux. Conda installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language. Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because conda is also an environment manager. With just a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run your usual version of Python in your normal environment. ( For more details, see the conda project documentation . )","title":"Conda"},{"location":"environment-and-software/user-environment/#conda-within-an-hpc-environment","text":"NCAR system users access Python via Conda environments , which are self-contained installations of Python itself, Python packages, and the software dependencies those packages rely on. We provide a common conda module available through the module system , and encourage all users to leverage this common installation. Prefer NCAR's common conda environment While it is possible for users to install conda into their own personal workspaces, we discourage this approach. Instead, users should investigate the common conda environment module available through the module system . Using this common installation facilitates sharing environments between colleagues and is configured internally to make optimum use of the NCAR GLADE file spaces .","title":"Conda within an HPC Environment"},{"location":"environment-and-software/user-environment/#containers","text":"","title":"Containers"},{"location":"environment-and-software/user-environment/conda/","text":"Using Conda \u00b6 Updated 12/21/2022 This documentation has been revised to include the new update schedule for the NCAR Python Library as well as information for determining when to use managed environments vs. when to create your own environments. NCAR system users access Python via Conda environments, which are self-contained installations of Python itself, Python packages, and the software dependencies those packages rely on. The Conda environment manager is accessible by loading an environment module as described below. After loading the module, you can create your own environments or use NCAR Python Library (NPL) environments, which include Python 3.9 and a wide array of pre-installed geoscience, math, and computing packages. Python and Conda \u00b6 Python is one of the most popular languages for scientific data analysis, visualization, and machine learning. Not only is it well-suited for such applications, one of its main strengths is the availability of supported and well-made third-party libraries for many different applications. These libraries, which include a collection of Python modules and software, can be installed into a Python environment. While various methods and tools are available for managing Python environments, CISL recommends using Conda to manage them on NCAR systems. If Python packages are not managed carefully, the environments may suffer from incompatibilities between chosen packages and even cause problems for other applications. One of the key concerns is dependency management. Many Python users are already familiar with Conda , a sophisticated package manager that eases the burden of setting Python up to do scientific analysis. While it is not difficult to install Conda yourself, we provide a Conda environment module on all of our systems to let you skip that step. The version of Conda that the module provides is updated frequently and also includes Mamba . (Mamba is a re-implementation of Conda with a similar interface and it is typically much faster and more robust at creating environments from complex sets of requirements). Once loaded, the Conda module: provides access to conda and mamba commands. allows you to run conda activate without initializing Conda in your startup files. adds managed environments to your list of available Python environments. sets sensible default behaviors for Conda, such as making conda-forge the default package channel , locating your cached package downloads in your scratch space, and locating your personal Python environments in your work space. System-wide and personal conda installations If you already have a personal Miniconda installation initialized in your environment, the Conda module will take precedence only when it is loaded. To use your own install instead, simply unload the module. You can also override or augment our Conda configuration settings by defining your own in a ~/.condarc file. For more information on using Conda, consider exploring the official documentation . Additionally, the Conda Cheat Sheet is a helpful quick reference that covers the commands most users will need. Accessing Python via Conda \u00b6 Once you have the Conda module loaded, you can start using Python a number of ways. It is easy and often convenient to activate one of the environments CISL provides, such as the NCAR Python Library (NPL). However, you may wish to craft an environment that contains only the packages you care about, or use specific versions of different packages. Conda makes this process relatively straightforward. Both approaches are detailed in the following sections. Using managed environments \u00b6 You can access a number of pre-installed Python environments by loading the Conda module. To see available environments, run the list subcommand: conda env list The output will include both managed environments and any personal environments you have created, provided they are in Conda\u2019s search path. The NCAR Python Library \u00b6 Our primary managed environment is the NCAR Python Library ( NPL ), which contains a large collection of packages related to geoscience and data processing. (It does not currently contain GPU or machine learning packages.) The NPL can be accessed as follows: conda activate npl Once the environment is activated, the python command in your shell will use the version provided by the NPL. Currently, it is version Python 3.8, which is supported by the majority of relevant packages. Once package compatibility improves in newer versions, the NPL will be updated. To see the full list of packages in a managed environment, run the following command: conda list --name npl-2022b Update schedule for the NCAR Python Library \u00b6 The NPL is updated twice a year and identified with environment names such as npl-2022b and npl-2023a . New NPL versions contain the same collection of packages as the previous environments (plus any requested and approved additions), but we let the Mamba resolver find and use the latest compatible versions of each package. For convenience, we also provide an environment named npl that always points to the most recent version of the NPL. We recommend that you load a specific version instead if you want to ensure consistent behavior from each installed package. Creating your own Conda environment \u00b6 There are many reasons you might prefer to install your own Conda environment rather than use one of the managed environments mentioned above. These may include: Preferring a small collection of relevant packages Existing workflows that create environments Faster or irregular package update cadence Desiring your own specified matrix of Python and package versions Maximizing stability in a package development environment To install your own environment using Conda: Load our Conda module (or use your own Miniconda install) Use mamba create to create an environment from a set of requirements, as in the following example: mamba create -n my-env python = 3 .9 numpy scipy matplotlib pandas geocat-comp wrf-python That command will create a new environment called my-env with a recent version of Python 3.9.x. (you can choose a different Python version than the NPL when creating your own environments) and a small number of useful geoscience packages (plus any dependencies that Mamba determines are needed). The environment would be located in /glade/work/$USER/conda-envs/my-env if you are using the CISL Conda module. Mamba and Conda interoperability In general, any Conda command can be executed with Mamba instead. In practice, it is only beneficial to use Mamba when creating environments and installing packages. The Mamba developers currently advise against using it to activate environments. Using Conda in batch jobs \u00b6 The Conda module enables Python environment activation by setting a shell alias that calls initialization scripts. However, the alias does not carry over into batch jobs or other subshells. You can address this one of two ways: Explicitly load the Conda module at the start of your batch job. This will restore the alias in the shell environment and allow you to activate Conda environments. Run conda init (or conda init tcsh for tcsh users) after loading the module on a login node. That will add initialization commands to your ~/.bashrc or ~/.tcshrc file. Be aware that ~/.bashrc is not always sourced at the start of batch jobs, so this approach may require some trial and error to cover your individual use case. Reproducing Conda environments \u00b6 Cloning an environment \u00b6 It is possible to clone any environment in your conda env list using Mamba. For example, to clone the personal environment we created above, use the following command: mamba create -n my-clone --clone my-env Assuming the clone is created on the same file system (e.g., your conda-envs ), it will use very little space as Mamba will attempt to use hard links to avoid duplication. Using YAML files \u00b6 Sometimes it is useful to create an environment from the environment file of another environment. This allows Mamba to resolve all package requirements at the same time, which can produce more coherent environments and avoid situations in which the dependency resolver fails to find a solution. If the cloning method described above fails or seems to be taking too long, try this approach instead. Other benefits include having a hard copy of the specifications of an environment that you can share with colleagues, install on other systems, and version track with software like Git. First, use Conda to produce the YAML environment file from an existing environment. This example uses the NPL: conda env export [--from-history] -n npl > npl-environment.yml If you use the --from-history option, only packages that you explicitly request will be specified in the file, letting Conda choose newer dependencies for future environments. If you do not specify that option, all packages including dependencies will be documented explicitly in the YAML file. After creating the YAML file, you can use a text editor to make additions and changes to the package list; package additions and changes will be reflected in any new environments you create from the modified YAML file. You can then create new environments from the YAML file using Mamba: mamba env create -f npl-environment.yml -n my-npl Warning Large environments like the NPL can take a long time to clone or recreate using YAML files. Using Conda environments in Jupyter \u00b6 Managed environments can be accessed easily in a JupyterLab session using the NCAR JupyterHub service. Once you initiate a JupyterHub server, the managed kernels mentioned above (e.g., NPL 2022b ) should be visible on the main launcher page. You can use them to run both Notebooks and Consoles. Creating Jupyter kernels for your environments \u00b6 There are two ways to make your environments accessible in a Jupyter interface like NCAR\u2019s JupyterHub. Both involve creating a kernel for the environment. First, install the ipykernel package into your environment: conda activate my-env mamba install ipykernel At this point, your environment should show up automatically as a kernel in any Jupyter server on Cheyenne or Casper. This method is convenient and robust, and any shell settings (e.g., environment variables) needed by packages will be set upon kernel launch. You can also manually create a JSON-format kernel specification file in your home directory and give it a custom name using the ipykernel module. This approach allows you to share environments with colleagues, as they can simply activate your Conda environment on the command line and then create their own personal Jupyter kernel. With your environment activated, run the following: python -m ipykernel install --user --name = my-kernel Warning Kernels created using this approach will not have the previously mentioned shell settings imposed by conda activate , which may result in some packages (e.g., pyproj ) not functioning properly.","title":"Conda"},{"location":"environment-and-software/user-environment/conda/#using-conda","text":"Updated 12/21/2022 This documentation has been revised to include the new update schedule for the NCAR Python Library as well as information for determining when to use managed environments vs. when to create your own environments. NCAR system users access Python via Conda environments, which are self-contained installations of Python itself, Python packages, and the software dependencies those packages rely on. The Conda environment manager is accessible by loading an environment module as described below. After loading the module, you can create your own environments or use NCAR Python Library (NPL) environments, which include Python 3.9 and a wide array of pre-installed geoscience, math, and computing packages.","title":"Using Conda"},{"location":"environment-and-software/user-environment/conda/#python-and-conda","text":"Python is one of the most popular languages for scientific data analysis, visualization, and machine learning. Not only is it well-suited for such applications, one of its main strengths is the availability of supported and well-made third-party libraries for many different applications. These libraries, which include a collection of Python modules and software, can be installed into a Python environment. While various methods and tools are available for managing Python environments, CISL recommends using Conda to manage them on NCAR systems. If Python packages are not managed carefully, the environments may suffer from incompatibilities between chosen packages and even cause problems for other applications. One of the key concerns is dependency management. Many Python users are already familiar with Conda , a sophisticated package manager that eases the burden of setting Python up to do scientific analysis. While it is not difficult to install Conda yourself, we provide a Conda environment module on all of our systems to let you skip that step. The version of Conda that the module provides is updated frequently and also includes Mamba . (Mamba is a re-implementation of Conda with a similar interface and it is typically much faster and more robust at creating environments from complex sets of requirements). Once loaded, the Conda module: provides access to conda and mamba commands. allows you to run conda activate without initializing Conda in your startup files. adds managed environments to your list of available Python environments. sets sensible default behaviors for Conda, such as making conda-forge the default package channel , locating your cached package downloads in your scratch space, and locating your personal Python environments in your work space. System-wide and personal conda installations If you already have a personal Miniconda installation initialized in your environment, the Conda module will take precedence only when it is loaded. To use your own install instead, simply unload the module. You can also override or augment our Conda configuration settings by defining your own in a ~/.condarc file. For more information on using Conda, consider exploring the official documentation . Additionally, the Conda Cheat Sheet is a helpful quick reference that covers the commands most users will need.","title":"Python and Conda"},{"location":"environment-and-software/user-environment/conda/#accessing-python-via-conda","text":"Once you have the Conda module loaded, you can start using Python a number of ways. It is easy and often convenient to activate one of the environments CISL provides, such as the NCAR Python Library (NPL). However, you may wish to craft an environment that contains only the packages you care about, or use specific versions of different packages. Conda makes this process relatively straightforward. Both approaches are detailed in the following sections.","title":"Accessing Python via Conda"},{"location":"environment-and-software/user-environment/conda/#using-managed-environments","text":"You can access a number of pre-installed Python environments by loading the Conda module. To see available environments, run the list subcommand: conda env list The output will include both managed environments and any personal environments you have created, provided they are in Conda\u2019s search path.","title":"Using managed environments"},{"location":"environment-and-software/user-environment/conda/#the-ncar-python-library","text":"Our primary managed environment is the NCAR Python Library ( NPL ), which contains a large collection of packages related to geoscience and data processing. (It does not currently contain GPU or machine learning packages.) The NPL can be accessed as follows: conda activate npl Once the environment is activated, the python command in your shell will use the version provided by the NPL. Currently, it is version Python 3.8, which is supported by the majority of relevant packages. Once package compatibility improves in newer versions, the NPL will be updated. To see the full list of packages in a managed environment, run the following command: conda list --name npl-2022b","title":"The NCAR Python Library"},{"location":"environment-and-software/user-environment/conda/#update-schedule-for-the-ncar-python-library","text":"The NPL is updated twice a year and identified with environment names such as npl-2022b and npl-2023a . New NPL versions contain the same collection of packages as the previous environments (plus any requested and approved additions), but we let the Mamba resolver find and use the latest compatible versions of each package. For convenience, we also provide an environment named npl that always points to the most recent version of the NPL. We recommend that you load a specific version instead if you want to ensure consistent behavior from each installed package.","title":"Update schedule for the NCAR Python Library"},{"location":"environment-and-software/user-environment/conda/#creating-your-own-conda-environment","text":"There are many reasons you might prefer to install your own Conda environment rather than use one of the managed environments mentioned above. These may include: Preferring a small collection of relevant packages Existing workflows that create environments Faster or irregular package update cadence Desiring your own specified matrix of Python and package versions Maximizing stability in a package development environment To install your own environment using Conda: Load our Conda module (or use your own Miniconda install) Use mamba create to create an environment from a set of requirements, as in the following example: mamba create -n my-env python = 3 .9 numpy scipy matplotlib pandas geocat-comp wrf-python That command will create a new environment called my-env with a recent version of Python 3.9.x. (you can choose a different Python version than the NPL when creating your own environments) and a small number of useful geoscience packages (plus any dependencies that Mamba determines are needed). The environment would be located in /glade/work/$USER/conda-envs/my-env if you are using the CISL Conda module. Mamba and Conda interoperability In general, any Conda command can be executed with Mamba instead. In practice, it is only beneficial to use Mamba when creating environments and installing packages. The Mamba developers currently advise against using it to activate environments.","title":"Creating your own Conda environment"},{"location":"environment-and-software/user-environment/conda/#using-conda-in-batch-jobs","text":"The Conda module enables Python environment activation by setting a shell alias that calls initialization scripts. However, the alias does not carry over into batch jobs or other subshells. You can address this one of two ways: Explicitly load the Conda module at the start of your batch job. This will restore the alias in the shell environment and allow you to activate Conda environments. Run conda init (or conda init tcsh for tcsh users) after loading the module on a login node. That will add initialization commands to your ~/.bashrc or ~/.tcshrc file. Be aware that ~/.bashrc is not always sourced at the start of batch jobs, so this approach may require some trial and error to cover your individual use case.","title":"Using Conda in batch jobs"},{"location":"environment-and-software/user-environment/conda/#reproducing-conda-environments","text":"","title":"Reproducing Conda environments"},{"location":"environment-and-software/user-environment/conda/#cloning-an-environment","text":"It is possible to clone any environment in your conda env list using Mamba. For example, to clone the personal environment we created above, use the following command: mamba create -n my-clone --clone my-env Assuming the clone is created on the same file system (e.g., your conda-envs ), it will use very little space as Mamba will attempt to use hard links to avoid duplication.","title":"Cloning an environment"},{"location":"environment-and-software/user-environment/conda/#using-yaml-files","text":"Sometimes it is useful to create an environment from the environment file of another environment. This allows Mamba to resolve all package requirements at the same time, which can produce more coherent environments and avoid situations in which the dependency resolver fails to find a solution. If the cloning method described above fails or seems to be taking too long, try this approach instead. Other benefits include having a hard copy of the specifications of an environment that you can share with colleagues, install on other systems, and version track with software like Git. First, use Conda to produce the YAML environment file from an existing environment. This example uses the NPL: conda env export [--from-history] -n npl > npl-environment.yml If you use the --from-history option, only packages that you explicitly request will be specified in the file, letting Conda choose newer dependencies for future environments. If you do not specify that option, all packages including dependencies will be documented explicitly in the YAML file. After creating the YAML file, you can use a text editor to make additions and changes to the package list; package additions and changes will be reflected in any new environments you create from the modified YAML file. You can then create new environments from the YAML file using Mamba: mamba env create -f npl-environment.yml -n my-npl Warning Large environments like the NPL can take a long time to clone or recreate using YAML files.","title":"Using YAML files"},{"location":"environment-and-software/user-environment/conda/#using-conda-environments-in-jupyter","text":"Managed environments can be accessed easily in a JupyterLab session using the NCAR JupyterHub service. Once you initiate a JupyterHub server, the managed kernels mentioned above (e.g., NPL 2022b ) should be visible on the main launcher page. You can use them to run both Notebooks and Consoles.","title":"Using Conda environments in Jupyter"},{"location":"environment-and-software/user-environment/conda/#creating-jupyter-kernels-for-your-environments","text":"There are two ways to make your environments accessible in a Jupyter interface like NCAR\u2019s JupyterHub. Both involve creating a kernel for the environment. First, install the ipykernel package into your environment: conda activate my-env mamba install ipykernel At this point, your environment should show up automatically as a kernel in any Jupyter server on Cheyenne or Casper. This method is convenient and robust, and any shell settings (e.g., environment variables) needed by packages will be set upon kernel launch. You can also manually create a JSON-format kernel specification file in your home directory and give it a custom name using the ipykernel module. This approach allows you to share environments with colleagues, as they can simply activate your Conda environment on the command line and then create their own personal Jupyter kernel. With your environment activated, run the following: python -m ipykernel install --user --name = my-kernel Warning Kernels created using this approach will not have the previously mentioned shell settings imposed by conda activate , which may result in some packages (e.g., pyproj ) not functioning properly.","title":"Creating Jupyter kernels for your environments"},{"location":"environment-and-software/user-environment/customizing/","text":"Personalizing start files \u00b6 You can personalize your NCAR high-performance computing environment by using the sample content below in your startup files \u2013 the files that create your interactive login shell. The examples provide alternative color schemes and set some commonly used aliases. These examples also demonstrate how to define commands to be run for interactive sessions only and commands to be run for all new shells . This distinction can be important if you have some commands in your initialization files that would be disruptive for non-interactive connections, such as when using the scp command (which is not interactable and therefore will not benefit from module loads or aliases) or within a batch script. If you use bash or ksh, edit your .profile file. If you use tcsh, edit your .tcshrc file. PBS jobs on NCAR systems initialize your job environment with those files, so do not confuse them with .bashrc and .login files, which have different purposes and are not always sourced at login time.) Personalizing these files is optional; the information is provided in response to users' requests. Keep the following in mind when writing/revising your initialization files Consider making backup copies of your existing startup files before creating customized files. If you source other scripts in your initialization files, be sure to provide an absolute path to the file (or a consistent reference path like source ~/commands instead of source commands ). That way, the script will execute correctly whether it is run from a session within your home directory or anywhere else on the file systems. It is best to include only those commands and settings in your initialization files that are generic to all work you perform. Examples of these include environment variables and module load commands. Any settings specific to a particular workflow, or settings others may need to run your jobs, should be included in relevant shell and PBS batch scripts rather than in your initialization files. Some programs (conda, for example) insert initialization instructions into your startup files. These instructions will be executed for most new shells, including at the start of your PBS jobs. ~/.profile for bash and ksh users ~/.tcshrc for tcsh users ### Settings for interactive shells only if [[ $- == *i* ]] ; then # My Personal Prompt #PS1=\"\\u@\\h:\\w> \" # Another Colorful Prompt PS1 = \"\\[\\e[1;31;40m\\]\\u@\\[\\e[1;34;40m\\]\\H:\\[\\e[1;32;40m\\]\\w>\\[\\e[0m\\] \" export PS1 # Generic Aliases alias rm = 'rm -i' alias cp = 'cp -i' alias mv = 'mv -i' alias h = \"history | grep \" alias ls = \"ls --color\" alias vi = \"vim\" alias j = 'qstat' alias more = 'less' alias tail = 'tail ---disable-inotify -f' alias diffcolor = 'git diff --color=always --color-words' fi ### Settings for all shells export PATH = ~/bin: $PATH export PBS_ACCOUNT = CHEYENNE_PROJECT_CODE export DAV_PROJECT = CASPER_PROJECT_CODE ### Source .bashrc file to make environment more consistent ### (optional - some users prefer this; e.g., conda users) if [[ -f ~/.bashrc ]] ; then . ~/.bashrc fi ### Settings for interactive shells only tty > /dev/null if ( $status == 0 ) then # My Personal Prompt #set prompt = \"%n@%m:%~\" # Another Colorful Prompt set prompt = \"%{\\033[31;40m%}%n@%{\\033[0m%}%{\\033[1;34m%}%m:%{\\033[0m%}%{\\033[32;40m%}%~>%{\\033[0m%} \" # Generic Aliases alias rm 'rm -i' alias cp 'cp -i' alias mv 'mv -i' alias h \"history | grep \" alias ls \"ls --color\" alias vi \"vim\" alias j 'qstat' alias more 'less' alias diffcolor 'git diff --color=always --color-words' endif ### Settings for all shells setenv PATH ~/bin: $PATH setenv PBS_ACCOUNT CHEYENNE_PROJECT_CODE setenv DAV_PROJECT CASPER_PROJECT_CODE","title":"Customizing User Environment"},{"location":"environment-and-software/user-environment/customizing/#personalizing-start-files","text":"You can personalize your NCAR high-performance computing environment by using the sample content below in your startup files \u2013 the files that create your interactive login shell. The examples provide alternative color schemes and set some commonly used aliases. These examples also demonstrate how to define commands to be run for interactive sessions only and commands to be run for all new shells . This distinction can be important if you have some commands in your initialization files that would be disruptive for non-interactive connections, such as when using the scp command (which is not interactable and therefore will not benefit from module loads or aliases) or within a batch script. If you use bash or ksh, edit your .profile file. If you use tcsh, edit your .tcshrc file. PBS jobs on NCAR systems initialize your job environment with those files, so do not confuse them with .bashrc and .login files, which have different purposes and are not always sourced at login time.) Personalizing these files is optional; the information is provided in response to users' requests. Keep the following in mind when writing/revising your initialization files Consider making backup copies of your existing startup files before creating customized files. If you source other scripts in your initialization files, be sure to provide an absolute path to the file (or a consistent reference path like source ~/commands instead of source commands ). That way, the script will execute correctly whether it is run from a session within your home directory or anywhere else on the file systems. It is best to include only those commands and settings in your initialization files that are generic to all work you perform. Examples of these include environment variables and module load commands. Any settings specific to a particular workflow, or settings others may need to run your jobs, should be included in relevant shell and PBS batch scripts rather than in your initialization files. Some programs (conda, for example) insert initialization instructions into your startup files. These instructions will be executed for most new shells, including at the start of your PBS jobs. ~/.profile for bash and ksh users ~/.tcshrc for tcsh users ### Settings for interactive shells only if [[ $- == *i* ]] ; then # My Personal Prompt #PS1=\"\\u@\\h:\\w> \" # Another Colorful Prompt PS1 = \"\\[\\e[1;31;40m\\]\\u@\\[\\e[1;34;40m\\]\\H:\\[\\e[1;32;40m\\]\\w>\\[\\e[0m\\] \" export PS1 # Generic Aliases alias rm = 'rm -i' alias cp = 'cp -i' alias mv = 'mv -i' alias h = \"history | grep \" alias ls = \"ls --color\" alias vi = \"vim\" alias j = 'qstat' alias more = 'less' alias tail = 'tail ---disable-inotify -f' alias diffcolor = 'git diff --color=always --color-words' fi ### Settings for all shells export PATH = ~/bin: $PATH export PBS_ACCOUNT = CHEYENNE_PROJECT_CODE export DAV_PROJECT = CASPER_PROJECT_CODE ### Source .bashrc file to make environment more consistent ### (optional - some users prefer this; e.g., conda users) if [[ -f ~/.bashrc ]] ; then . ~/.bashrc fi ### Settings for interactive shells only tty > /dev/null if ( $status == 0 ) then # My Personal Prompt #set prompt = \"%n@%m:%~\" # Another Colorful Prompt set prompt = \"%{\\033[31;40m%}%n@%{\\033[0m%}%{\\033[1;34m%}%m:%{\\033[0m%}%{\\033[32;40m%}%~>%{\\033[0m%} \" # Generic Aliases alias rm 'rm -i' alias cp 'cp -i' alias mv 'mv -i' alias h \"history | grep \" alias ls \"ls --color\" alias vi \"vim\" alias j 'qstat' alias more 'less' alias diffcolor 'git diff --color=always --color-words' endif ### Settings for all shells setenv PATH ~/bin: $PATH setenv PBS_ACCOUNT CHEYENNE_PROJECT_CODE setenv DAV_PROJECT CASPER_PROJECT_CODE","title":"Personalizing start files"},{"location":"environment-and-software/user-environment/modules/","text":"Modules \u00b6 Overview \u00b6 The module utility helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, variables, and dependencies so you can compile and run jobs efficiently and make the most of your allocation. Some modules are loaded by default. To see which modules those are, run module list when you log in. Depending on the work you need to do, you can load additional modules or different modules, or you can create and save multiple customized environments as described below. Warning Do not use personalized start files to load environment modules ; it can cause conflicts. Instead, set up any unique environments that you need as described in the customized environments section below. Use that approach to create and save different environments for various aspects of your work \u2013 one for model runs and another for data analysis, for example. Essential module commands \u00b6 Following are descriptions of commonly used module commands. module av \u2013 Show which modules are available for use with the currently loaded compiler. Typical output: In the example above, ( L ) indicates which modules are currently loaded. The modules deployed at NCAR are hierarchical , with a base of common compilers and core software. The remainder of the output is dependent on what core software (namely, compilers) are chosen. module help \u2013 List options and subcommands for the module utility; or specify a modulefile by name for help with an individual module. module help module help netcdf module list \u2013 List the modules that are loaded. module load \u2013 Load the default version of a software package, or load a specified version. module load modulefile_name module load modulefile_name/n.n.n module purge \u2013 Unload all modules. Some users include this command in a batch script, followed by a sequence of module load commands to establish a customized environment for the job being submitted. module reset - Reset the system default modules. module spider \u2013 List all modules that exist on the system. This does not give you information on module dependencies or tell you which modules can be loaded without conflicts at that point. module spider modulefile_name/n.n.n \u2013 List all occurrences of modulefile_name/n.n.n on the system, including its module dependences. For example, to determine which compiler/MPI combinations provide netcdf-mpi/4.9.2 : module spider netcdf-mpi/4.9.2 ------------------------------------------------------------------------------------------------------ netcdf-mpi: netcdf-mpi/4.9.2 ------------------------------------------------------------------------------------------------------ You will need to load all module(s) on any one of the lines below before the \"netcdf-mpi/4.9.2\" module is available to load. ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25 ncarenv/23.09 gcc/12.2.0 mvapich/3.0b ncarenv/23.09 intel-classic/2023.2.1 cray-mpich/8.1.25 ncarenv/23.09 intel-classic/2023.2.1 intel-mpi/2021.10.0 ncarenv/23.09 intel-oneapi/2023.2.1 cray-mpich/8.1.25 ncarenv/23.09 intel-oneapi/2023.2.1 intel-mpi/2021.10.0 ncarenv/23.09 intel/2023.2.1 cray-mpich/8.1.25 ncarenv/23.09 intel/2023.2.1 intel-mpi/2021.10.0 ncarenv/23.09 nvhpc/23.7 cray-mpich/8.1.25 ... Each output line is a consistent set of modules that when loaded will provide access to the requested netcdf-mpi package. module swap \u2013 Unload one module and load a different one. Example: module swap netcdf pnetcdf module unload \u2013 Unload the specified software package. module unload modulefile_name module whatis \u2013 Get a short description of a module. Customized environments \u00b6 If you have created your own environment or want to have multiple collections of modules for various tasks, you can save those customized environments for easy reuse. To save a customized environment as your default environment, load the modules that you want to use, then simply run module save or module s . module save The cluster you are using will append a suffix to the name you provide. For example, on Casper your example will include .dav as a suffix. If you plan to set up additional custom environments for other needs, give each collection of modules a unique name. module save environment_name To use one of the custom environments from that list, use module restore , or module r , followed by the name. module restore environment_name To see a list of your customized, saved environments, use module savelist . module savelist To see which modules you've saved in a custom environment, use module describe as shown. module describe environment_name.xyz Remove a customized environment \u00b6 To remove a customized environment that you have saved: Change to your .lmod.d directory. List the files. Use rm to delete what you no longer need. cd /glade/u/home/username/.lmod.d ls -l rm environment_name.xyz Revise a customized environment \u00b6 To revise a customized environment: Restore (change to) that environment. Unload, load, or swap modules as needed. Save the environment as a default environment again with the same name. module restore myenvironment module load additional_module module save myenvironment The previously saved environment will be renamed automatically with the addition of a tilde (~). In the example just above, the previously saved environment would be renamed to myenvironment~ . Troubleshooting tips Situation: You load a custom default module collection (for example, module restore myenvironment ). You receive a warning similar to this: Lmod Warning: The following modules have changed: pgi Lmod Warning: Please re-save this collection What to do : \u201cRe-save\u201d the customized module collection by running module save and using the same environment name, as follows. module save myenvironment","title":"Modules"},{"location":"environment-and-software/user-environment/modules/#modules","text":"","title":"Modules"},{"location":"environment-and-software/user-environment/modules/#overview","text":"The module utility helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, variables, and dependencies so you can compile and run jobs efficiently and make the most of your allocation. Some modules are loaded by default. To see which modules those are, run module list when you log in. Depending on the work you need to do, you can load additional modules or different modules, or you can create and save multiple customized environments as described below. Warning Do not use personalized start files to load environment modules ; it can cause conflicts. Instead, set up any unique environments that you need as described in the customized environments section below. Use that approach to create and save different environments for various aspects of your work \u2013 one for model runs and another for data analysis, for example.","title":"Overview"},{"location":"environment-and-software/user-environment/modules/#essential-module-commands","text":"Following are descriptions of commonly used module commands. module av \u2013 Show which modules are available for use with the currently loaded compiler. Typical output: In the example above, ( L ) indicates which modules are currently loaded. The modules deployed at NCAR are hierarchical , with a base of common compilers and core software. The remainder of the output is dependent on what core software (namely, compilers) are chosen. module help \u2013 List options and subcommands for the module utility; or specify a modulefile by name for help with an individual module. module help module help netcdf module list \u2013 List the modules that are loaded. module load \u2013 Load the default version of a software package, or load a specified version. module load modulefile_name module load modulefile_name/n.n.n module purge \u2013 Unload all modules. Some users include this command in a batch script, followed by a sequence of module load commands to establish a customized environment for the job being submitted. module reset - Reset the system default modules. module spider \u2013 List all modules that exist on the system. This does not give you information on module dependencies or tell you which modules can be loaded without conflicts at that point. module spider modulefile_name/n.n.n \u2013 List all occurrences of modulefile_name/n.n.n on the system, including its module dependences. For example, to determine which compiler/MPI combinations provide netcdf-mpi/4.9.2 : module spider netcdf-mpi/4.9.2 ------------------------------------------------------------------------------------------------------ netcdf-mpi: netcdf-mpi/4.9.2 ------------------------------------------------------------------------------------------------------ You will need to load all module(s) on any one of the lines below before the \"netcdf-mpi/4.9.2\" module is available to load. ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25 ncarenv/23.09 gcc/12.2.0 mvapich/3.0b ncarenv/23.09 intel-classic/2023.2.1 cray-mpich/8.1.25 ncarenv/23.09 intel-classic/2023.2.1 intel-mpi/2021.10.0 ncarenv/23.09 intel-oneapi/2023.2.1 cray-mpich/8.1.25 ncarenv/23.09 intel-oneapi/2023.2.1 intel-mpi/2021.10.0 ncarenv/23.09 intel/2023.2.1 cray-mpich/8.1.25 ncarenv/23.09 intel/2023.2.1 intel-mpi/2021.10.0 ncarenv/23.09 nvhpc/23.7 cray-mpich/8.1.25 ... Each output line is a consistent set of modules that when loaded will provide access to the requested netcdf-mpi package. module swap \u2013 Unload one module and load a different one. Example: module swap netcdf pnetcdf module unload \u2013 Unload the specified software package. module unload modulefile_name module whatis \u2013 Get a short description of a module.","title":"Essential module commands"},{"location":"environment-and-software/user-environment/modules/#customized-environments","text":"If you have created your own environment or want to have multiple collections of modules for various tasks, you can save those customized environments for easy reuse. To save a customized environment as your default environment, load the modules that you want to use, then simply run module save or module s . module save The cluster you are using will append a suffix to the name you provide. For example, on Casper your example will include .dav as a suffix. If you plan to set up additional custom environments for other needs, give each collection of modules a unique name. module save environment_name To use one of the custom environments from that list, use module restore , or module r , followed by the name. module restore environment_name To see a list of your customized, saved environments, use module savelist . module savelist To see which modules you've saved in a custom environment, use module describe as shown. module describe environment_name.xyz","title":"Customized environments"},{"location":"environment-and-software/user-environment/modules/#remove-a-customized-environment","text":"To remove a customized environment that you have saved: Change to your .lmod.d directory. List the files. Use rm to delete what you no longer need. cd /glade/u/home/username/.lmod.d ls -l rm environment_name.xyz","title":"Remove a customized environment"},{"location":"environment-and-software/user-environment/modules/#revise-a-customized-environment","text":"To revise a customized environment: Restore (change to) that environment. Unload, load, or swap modules as needed. Save the environment as a default environment again with the same name. module restore myenvironment module load additional_module module save myenvironment The previously saved environment will be renamed automatically with the addition of a tilde (~). In the example just above, the previously saved environment would be renamed to myenvironment~ . Troubleshooting tips Situation: You load a custom default module collection (for example, module restore myenvironment ). You receive a warning similar to this: Lmod Warning: The following modules have changed: pgi Lmod Warning: Please re-save this collection What to do : \u201cRe-save\u201d the customized module collection by running module save and using the same environment name, as follows. module save myenvironment","title":"Revise a customized environment"},{"location":"environment-and-software/user-environment/utilities/","text":"Utilities \u00b6 CISL provides the tools described on this page to support users' code development efforts and to facilitate good programming and file management practices. Compressing and archiving files \u00b6 Because files often contain large amounts of redundant data and empty space, you can reduce their size significantly with tools such as gzip and bzip2 . Bzip2 provides greater compression, but it is also slower. Gzip offers a good balance of compression rate and speed. Note that gzip and bzip2 are able to compress only individual files . See the tools' man pages for details. To both package and compress a group of files , use the tar command to create an archive file \u2013 also called a tar file or tarball \u2013 as shown in the following examples. Also use the tar command to extract files when needed. Create an archive file with tar : tar -cvf archive_name.tar file1 file2 file3 Extract files from the archive file: tar -xvf archive_name.tar Create a bzip2 -compressed archive file: tar cjf archive_name.tar.bz2 filename Extract files from the archive: tar xf archive_name.tar.bz2 Create a gzip -compressed archive file: tar czf archive_name.tgz filename Extract files from the gzipped archive: tar xf archive_name.tgz The \" j \" and \" z \" options that are included when creating files with bzip2 or gzip are not needed for *extracting *files; they are used automatically. (If they are included in a script you use, there is no harm in leaving them there.) Editing files \u00b6 Among the most widely used text editors are Vim , Vi and Emacs . Several excellent tutorials are available for text editors, including the following: GNU Emacs Manual \u2013 gnu.org The Emacs Tutorial \u2013 Free Software Foundation A Tutorial Introduction to GNU Emacs \u2013 University of Chicago Library Making executables \u00b6 Using simple makefiles with the appropriate compiler is usually sufficient for making your own executables on Cheyenne. If you are tasked with developing cross-platform software, however, consider using the GNU Build System , also known as GNU Autotools . This is a suite of programming tools designed to assist in making source code packages portable to different UNIX-like systems. It consists of the GNU Utility programs autoconf, automake, and libtool. Man pages are available for each. Generating the executable usually is a two-step process using the GNU Build System : Invoke the configure script, which creates a makefile with some settings that are appropriate to the machine. Invoke make , which will use the makefile to compile the executable, often using a complex set of rules. GNU make ( gmake ) is the GNU incarnation of POSIX make , and as such it is more user friendly and full-featured. Usually it is used as a tool for managing compilation and builds of software packages, but it can do more. It functions consistently across platforms. See the GNU Make Manual . Version control \u00b6 Git ( git ) is the most popular distributed version control system in use and is available on NCAR Systems. Mercurial ( hg ) is another good distributed version control system. Its functionality is similar to that of Git.","title":"Utilities"},{"location":"environment-and-software/user-environment/utilities/#utilities","text":"CISL provides the tools described on this page to support users' code development efforts and to facilitate good programming and file management practices.","title":"Utilities"},{"location":"environment-and-software/user-environment/utilities/#compressing-and-archiving-files","text":"Because files often contain large amounts of redundant data and empty space, you can reduce their size significantly with tools such as gzip and bzip2 . Bzip2 provides greater compression, but it is also slower. Gzip offers a good balance of compression rate and speed. Note that gzip and bzip2 are able to compress only individual files . See the tools' man pages for details. To both package and compress a group of files , use the tar command to create an archive file \u2013 also called a tar file or tarball \u2013 as shown in the following examples. Also use the tar command to extract files when needed. Create an archive file with tar : tar -cvf archive_name.tar file1 file2 file3 Extract files from the archive file: tar -xvf archive_name.tar Create a bzip2 -compressed archive file: tar cjf archive_name.tar.bz2 filename Extract files from the archive: tar xf archive_name.tar.bz2 Create a gzip -compressed archive file: tar czf archive_name.tgz filename Extract files from the gzipped archive: tar xf archive_name.tgz The \" j \" and \" z \" options that are included when creating files with bzip2 or gzip are not needed for *extracting *files; they are used automatically. (If they are included in a script you use, there is no harm in leaving them there.)","title":"Compressing and archiving files"},{"location":"environment-and-software/user-environment/utilities/#editing-files","text":"Among the most widely used text editors are Vim , Vi and Emacs . Several excellent tutorials are available for text editors, including the following: GNU Emacs Manual \u2013 gnu.org The Emacs Tutorial \u2013 Free Software Foundation A Tutorial Introduction to GNU Emacs \u2013 University of Chicago Library","title":"Editing files"},{"location":"environment-and-software/user-environment/utilities/#making-executables","text":"Using simple makefiles with the appropriate compiler is usually sufficient for making your own executables on Cheyenne. If you are tasked with developing cross-platform software, however, consider using the GNU Build System , also known as GNU Autotools . This is a suite of programming tools designed to assist in making source code packages portable to different UNIX-like systems. It consists of the GNU Utility programs autoconf, automake, and libtool. Man pages are available for each. Generating the executable usually is a two-step process using the GNU Build System : Invoke the configure script, which creates a makefile with some settings that are appropriate to the machine. Invoke make , which will use the makefile to compile the executable, often using a complex set of rules. GNU make ( gmake ) is the GNU incarnation of POSIX make , and as such it is more user friendly and full-featured. Usually it is used as a tool for managing compilation and builds of software packages, but it can do more. It functions consistently across platforms. See the GNU Make Manual .","title":"Making executables"},{"location":"environment-and-software/user-environment/utilities/#version-control","text":"Git ( git ) is the most popular distributed version control system in use and is available on NCAR Systems. Mercurial ( hg ) is another good distributed version control system. Its functionality is similar to that of Git.","title":"Version control"},{"location":"environment-and-software/user-environment/containers/","text":"Using Containers on NCAR Systems \u00b6 Introduction \u00b6 A container is a unit of software that packages code with its required dependencies in order to run in an isolated, controlled environment. This allows you to run an application in a way that is predictable, repeatable, and without the uncertainty of inconsistent code execution across diverse development and production environments. A container will always start up and run code the same way, regardless of what machine it exists on. Terminology \u00b6 Operating System (OS): This is the software that manages all other software on a computer, along with the hardware. Kernel : This is the core component of the OS that handles interacting with a machine\u2019s hardware. The kernel translates requests for physical resources - CPU cycles, RAM, I/O, etc... - between software processes and the hardware. Container Image : An image is a read-only template that contains all the code, dependencies, libraries, and supporting files that are required to launch a container. This is a unit of software that packages code with its required dependencies in order to run in an isolated, controlled environment. Container images virtualize an OS, but not a kernel (in contrast to heavier-weight virtual machine technology). Images can be built manually, or retrieved from one of several Image Registries . Image Registry : This is a solution for container image storage and sharing. Several popular examples are Docker Hub , Quay.io and GitHub's container registry . Additionally, private image registries can be implemented. Image Repository : This is a specific location for a container image within an image registry. It contains the latest image along with the history of it within a registry. Containers : A container is an instance of an image. Running a container requires a container runtime environment and a CPU instruction set architecture (e.g. x86_64 , arm64 ) compatible with the image from which the container is instantiated. A single container image can be used to run many container instances. A Container Runtime is a software component that can run containers on a host operating system. Container runtimes are responsible for loading container images from a repository, monitoring local system resources, isolating system resources for use of a container, and managing container life cycle. Container Engines are complete solutions for container technologies - including the container, the container runtime underlying it, the container image and the tools to build it, and potentially can include container image registries. Container Runtimes in an HPC Environment \u00b6 The most poplar container runtime is Docker , and nearly all other container platforms seek to be compatible with Docker. Unfortunately, Docker's original design required elevated privileges and therefore was incompatible with shared HPC systems from a security perspective. (These limitations have been addressed to some extent with recent developments in \"rootless\" Docker, however it is still rare to find Docker installed on HPC systems.) Due to Docker's ubiquity and common tooling we will discuss it later as a viable platform for local container image development, for example when running Docker on a laptop or external system. To address Docker's security concerns, a number of alternative runtimes better suited for HPC use have been developed. Some notable examples include Apptainer (formerly, Singularity), Charliecloud , and Podman , which are all currently installed on NCAR's HPC resources and available through the module system. Charliecloud vs. Apptainer vs. Podman - Which is right for me? Unfortunately, with the current state of container technology on HPC systems it is difficult to provide a single recommended runtime. If you are familiar with one of these tools already, by all means use it - we will strive to maintain the most popular tools that are compatible with our security requirements. For users beginning to work with containers, here are some general considerations: Apptainer and the Singularity family of tools that precedes it was created from the outset to run complex applications on HPC clusters in a simple, portable, and reproducible way. It is well documented, stable, and mature. Probably the most significant drawback to Apptainer is building images, as it relies on its own recipe definition file format - known as \" def files\" - somewhat limiting build interoperability with other tools, including Docker. If you have an existing Singularity def file, use Apptainer. If you have relatively complex Docker build recipes (e.g. Dockerfiles ) or want easy interoperability with other tools in the build stage, you may consider one of the other alternatives. Charliecloud similarly runs containers with no privileged operations or daemons and minimal configuration changes on HPC center resources, and is also designed for HPC from the outset. Charliecloud's build steps are largely compatible with Dockerfile definition recipes, making it a good choice for users already invested in the Docker build process. By default Charliecloud stores container images in a directory tree, which can be performance limiting for large scale jobs on parallel file systems (see this paper for more information), however this limitation is largely overcome when using compressed SqushFUSE image files. Perhaps the one drawback to Charliecloud is that it appears less widely deployed across HPC sites that Apptainer, with a smaller community support base. Podman is envisioned as a complete replacement for Docker, which has benefits and drawbacks in an HPC environment. If you have a complex Docker workflow, Podman might be a drop-in replacement. The caveat is that some features supported in Podman are not implemented in an HPC environment due to security concerns, such as switching user IDs within a Dockerfile , etc... Podman has widespread industry support beyond just the HPC community. One downside to Podman is at present containers are straightforward to install at an individual level, but somewhat difficult to share within a group of users, as Podman does not easily support running containers from single compressed image files, limiting scalability when used under MPI. We cannot support all use cases for each runtime NCAR CISL staff will attempt to support containerized workflows using at least one of the tools mentioned above, and may recommend one tool over others given a specific use case. Due to both technical and personnel resource limitations we cannot force-fit any particular tool simply based on a users' personal preference. Use Cases \u00b6 Based on feedback from NCAR HPC users , some of the more common use cases envisioned for containers on our HPC systems are: To insulate a users' application from changes in the host environment, To run applications developed elsewhere, particularly when difficult to install directly, and To run legacy applications that are difficult to build with a modern software stack. Additionally, users expressed a desire to: Deploy in-house developed applications within a container to make it easier for others to install, Offer trainings outside of NCAR resources in a controlled environment, and Use cloud-based systems, but with a NCAR-HPC \"look and feel.\" To support these needs users must: Be able to run modeling codes across many nodes with MPI (with or without GPUs), or Run specialized containers within a single node as part of an analysis workflow. We address all of these use cases in our discussion of working with containers . Additionally we provide a collection of containerized workflow examples building upon this reference foundation. References \u00b6 Containers@TACC DigitalOcean's Introduction to Containers Open Container Initiative","title":"Using Containers on NCAR Systems"},{"location":"environment-and-software/user-environment/containers/#using-containers-on-ncar-systems","text":"","title":"Using Containers on NCAR Systems"},{"location":"environment-and-software/user-environment/containers/#introduction","text":"A container is a unit of software that packages code with its required dependencies in order to run in an isolated, controlled environment. This allows you to run an application in a way that is predictable, repeatable, and without the uncertainty of inconsistent code execution across diverse development and production environments. A container will always start up and run code the same way, regardless of what machine it exists on.","title":"Introduction"},{"location":"environment-and-software/user-environment/containers/#terminology","text":"Operating System (OS): This is the software that manages all other software on a computer, along with the hardware. Kernel : This is the core component of the OS that handles interacting with a machine\u2019s hardware. The kernel translates requests for physical resources - CPU cycles, RAM, I/O, etc... - between software processes and the hardware. Container Image : An image is a read-only template that contains all the code, dependencies, libraries, and supporting files that are required to launch a container. This is a unit of software that packages code with its required dependencies in order to run in an isolated, controlled environment. Container images virtualize an OS, but not a kernel (in contrast to heavier-weight virtual machine technology). Images can be built manually, or retrieved from one of several Image Registries . Image Registry : This is a solution for container image storage and sharing. Several popular examples are Docker Hub , Quay.io and GitHub's container registry . Additionally, private image registries can be implemented. Image Repository : This is a specific location for a container image within an image registry. It contains the latest image along with the history of it within a registry. Containers : A container is an instance of an image. Running a container requires a container runtime environment and a CPU instruction set architecture (e.g. x86_64 , arm64 ) compatible with the image from which the container is instantiated. A single container image can be used to run many container instances. A Container Runtime is a software component that can run containers on a host operating system. Container runtimes are responsible for loading container images from a repository, monitoring local system resources, isolating system resources for use of a container, and managing container life cycle. Container Engines are complete solutions for container technologies - including the container, the container runtime underlying it, the container image and the tools to build it, and potentially can include container image registries.","title":"Terminology"},{"location":"environment-and-software/user-environment/containers/#container-runtimes-in-an-hpc-environment","text":"The most poplar container runtime is Docker , and nearly all other container platforms seek to be compatible with Docker. Unfortunately, Docker's original design required elevated privileges and therefore was incompatible with shared HPC systems from a security perspective. (These limitations have been addressed to some extent with recent developments in \"rootless\" Docker, however it is still rare to find Docker installed on HPC systems.) Due to Docker's ubiquity and common tooling we will discuss it later as a viable platform for local container image development, for example when running Docker on a laptop or external system. To address Docker's security concerns, a number of alternative runtimes better suited for HPC use have been developed. Some notable examples include Apptainer (formerly, Singularity), Charliecloud , and Podman , which are all currently installed on NCAR's HPC resources and available through the module system. Charliecloud vs. Apptainer vs. Podman - Which is right for me? Unfortunately, with the current state of container technology on HPC systems it is difficult to provide a single recommended runtime. If you are familiar with one of these tools already, by all means use it - we will strive to maintain the most popular tools that are compatible with our security requirements. For users beginning to work with containers, here are some general considerations: Apptainer and the Singularity family of tools that precedes it was created from the outset to run complex applications on HPC clusters in a simple, portable, and reproducible way. It is well documented, stable, and mature. Probably the most significant drawback to Apptainer is building images, as it relies on its own recipe definition file format - known as \" def files\" - somewhat limiting build interoperability with other tools, including Docker. If you have an existing Singularity def file, use Apptainer. If you have relatively complex Docker build recipes (e.g. Dockerfiles ) or want easy interoperability with other tools in the build stage, you may consider one of the other alternatives. Charliecloud similarly runs containers with no privileged operations or daemons and minimal configuration changes on HPC center resources, and is also designed for HPC from the outset. Charliecloud's build steps are largely compatible with Dockerfile definition recipes, making it a good choice for users already invested in the Docker build process. By default Charliecloud stores container images in a directory tree, which can be performance limiting for large scale jobs on parallel file systems (see this paper for more information), however this limitation is largely overcome when using compressed SqushFUSE image files. Perhaps the one drawback to Charliecloud is that it appears less widely deployed across HPC sites that Apptainer, with a smaller community support base. Podman is envisioned as a complete replacement for Docker, which has benefits and drawbacks in an HPC environment. If you have a complex Docker workflow, Podman might be a drop-in replacement. The caveat is that some features supported in Podman are not implemented in an HPC environment due to security concerns, such as switching user IDs within a Dockerfile , etc... Podman has widespread industry support beyond just the HPC community. One downside to Podman is at present containers are straightforward to install at an individual level, but somewhat difficult to share within a group of users, as Podman does not easily support running containers from single compressed image files, limiting scalability when used under MPI. We cannot support all use cases for each runtime NCAR CISL staff will attempt to support containerized workflows using at least one of the tools mentioned above, and may recommend one tool over others given a specific use case. Due to both technical and personnel resource limitations we cannot force-fit any particular tool simply based on a users' personal preference.","title":"Container Runtimes in an HPC Environment"},{"location":"environment-and-software/user-environment/containers/#use-cases","text":"Based on feedback from NCAR HPC users , some of the more common use cases envisioned for containers on our HPC systems are: To insulate a users' application from changes in the host environment, To run applications developed elsewhere, particularly when difficult to install directly, and To run legacy applications that are difficult to build with a modern software stack. Additionally, users expressed a desire to: Deploy in-house developed applications within a container to make it easier for others to install, Offer trainings outside of NCAR resources in a controlled environment, and Use cloud-based systems, but with a NCAR-HPC \"look and feel.\" To support these needs users must: Be able to run modeling codes across many nodes with MPI (with or without GPUs), or Run specialized containers within a single node as part of an analysis workflow. We address all of these use cases in our discussion of working with containers . Additionally we provide a collection of containerized workflow examples building upon this reference foundation.","title":"Use Cases"},{"location":"environment-and-software/user-environment/containers/#references","text":"Containers@TACC DigitalOcean's Introduction to Containers Open Container Initiative","title":"References"},{"location":"environment-and-software/user-environment/containers/examples/","text":"Sample Containerized Workflows \u00b6 Warning This page contains a sample of containerized workflows that demonstrate various techniques built up in practice, often from resolving user issues. We do not necessarily endorse or support each use case, rather these examples are provided in hopes they may be useful to demonstrate (i) sample containerized workflows, and (ii) solutions to various problems you may encounter. NVIDIA's NGC containers \u00b6 NVIDIA's NGC is a catalog of software optimized for GPUs. NGC containers allow you to run data science projects \"out of the box\" without installing, configuring, or integrating the infrastructure. NVIDIA's Modulus physics-ML framework \u00b6 NVIDIA Modulus is an open source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods. NVIDIA provides a frequently updated Docker image with a containerized PyTorch installation that can be run under Apptainer, albeit with some effort. Because the container is designed for Docker, some additional steps are required as discussed below. Running containerized NVIDIA-Modulus on a single Casper GPU Rather than pull the container and run as-is, we will create a derived container that allows us to encapsulate our desired changes. The primary reason for this is the Modulus container assumes the container is writable and makes changes during execution. Since we will run under Apptainer using a compressed, read-only image, this fails. Therefore we will make our own derived image and make the requisite changes during the build process. This is accomplished first by creating a simple Apptainer definition file: my_modulus.def From: nvcr.io/nvidia/modulus/modulus:23.09 %post # update pip python -m pip install --upgrade pip # use pip to install additional packages needed for examples later pip install warp-lang # Remove cuda compat layer (https://github.com/NVIDIA/nvidia-docker/issues/1256) # note that the source container attempts to do this at run-time, but that will # fail when launched read-only. So we do that here instead. # (This issue will likely be resolved with newer versions of nvidia-modulus) rm -rf /usr/local/cuda/compat/lib The definition file begins by pulling a specified version of the Modulus container, then modifying it in our %post step. In %post we update the pip Python package installer, use pip to install some additional Python packages not in the base image but required for the examples run later, and finally removes a conflicting path from the source image. Using the my_modulus.def file we now create our derived container and store it as a SIF: module load apptainer TMPDIR=/var/tmp/ singularity build my_modulus.sif my_modulus.def Note in this step we have explicitly set TMPDIR to a local file system, as occasionally containers fail to build on the large parallel file systems usually used for TMPDIR within NCAR. (The failure symptoms are usually fatal error messages related to xattrs .) Fetch some examples so we can test our installation: git clone https://github.com/NVIDIA/modulus.git Run the container in an interactive session on a single Casper GPU. We will launch an interactive session, then run the container interactively with the singularity shell command. # Interactive PBS submission from a login node: qsub -I -A <ACCOUNT> -q casper -l select=1:ncpus=4:mpiprocs=4:ngpus=1 -l gpu_type=v100 -l walltime=1:00:00 # Then on the GPU node: module load apptainer singularity shell \\ --nv --cleanenv \\ --bind /glade/work \\ --bind /glade/campaign \\ --bind /glade/derecho/scratch \\ ./my_modulus.sif Note the command line arguments to singularity shell : --nv : enable Nvidia support, --cleanenv : clean environment before running container, causing the container to be launched with no knowledge of environment variables set on the host. This is default behavior for Docker, and is required in this case to prevent conflicting CUDA_* and other environment variable settings from confusing the contanierized PyTorch. --bind /glade/work etc...: binds host file systems into the container, allowing us to read and write from GLADE. Now we are inside the container, as evidenced by the Apptainer> command line prompt in the final step of this example. We will run one of the sample problems checked out in step 3: Apptainer> cd modulus/examples/cfd/darcy_fno/ Apptainer> python ./train_fno_darcy.py Warp 0.10.1 initialized: CUDA Toolkit: 11.5, Driver: 12.3 Devices: \"cpu\" | x86_64 \"cuda:0\" | Tesla V100-SXM2-32GB (sm_70) Kernel cache: /glade/u/home/benkirk/.cache/warp/0.10.1 [21:04:13 - mlflow - WARNING] Checking MLFlow logging location is working (if this hangs its not) [21:04:13 - mlflow - INFO] MLFlow logging location is working [21:04:13 - mlflow - INFO] No Darcy_FNO experiment found, creating... [21:04:13 - checkpoint - WARNING] Provided checkpoint directory ./checkpoints does not exist, skipping load [21:04:13 - darcy_fno - WARNING] Model FourierNeuralOperator does not support AMP on GPUs, turning off [21:04:13 - darcy_fno - WARNING] Model FourierNeuralOperator does not support AMP on GPUs, turning off [21:04:13 - darcy_fno - INFO] Training started... Module modulus.datapipes.benchmarks.kernels.initialization load on device 'cuda:0' took 205.84 ms Module modulus.datapipes.benchmarks.kernels.utils load on device 'cuda:0' took 212.94 ms Module modulus.datapipes.benchmarks.kernels.finite_difference load on device 'cuda:0' took 670.44 ms [21:04:46 - train - INFO] Epoch 1 Metrics: Learning Rate = 1.000e-03, loss = 6.553e-01 [21:04:46 - train - INFO] Epoch Execution Time: 3.241e+01s, Time/Iter: 1.013e+03ms [21:05:14 - train - INFO] Epoch 2 Metrics: Learning Rate = 1.000e-03, loss = 4.255e-02 [21:05:14 - train - INFO] Epoch Execution Time: 2.812e+01s, Time/Iter: 8.786e+02ms [...] While this example demonstrated running the container interactively, alternatively steps 3 and 4 can be combined to be run inside a PBS batch job. Popular AI/ML tools \u00b6 Optimized Tensorflow and PyTorch models are available directly from the NGC. Running AI/ML tools from NGC containers Building an image with Apptainer Anticipating that we may want to make additions to the container, we will build our own derived Apptainer image using a Definition file. Tensorflow PyTorch ngc_tensorflow.def Bootstrap: docker From: nvcr.io/nvidia/tensorflow:23.11-tf2-py3 %post # update pip python -m pip install --upgrade pip [...] module load apptainer TMPDIR = /var/tmp/ singularity build my_image.sif ngc_tensorflow.def ngc_pytorch.def Bootstrap: docker From: nvcr.io/nvidia/pytorch:23.11-py3 %post # update pip python -m pip install --upgrade pip [...] module load apptainer TMPDIR = /var/tmp/ singularity build my_image.sif ngc_pytorch.def Run the image module load apptainer singularity shell \\ --nv --cleanenv \\ --bind /glade/work \\ --bind /glade/campaign \\ --bind /glade/derecho/scratch \\ ./my_image.sif [...] Apptainer> We are now inside the container. Note the command line arguments to singularity shell : --nv --cleanenv enables NVIDIA support with a clean environment; --bind /glade/work etc...: binds host file systems into the container, allowing us to read and write from GLADE. Building and running containerized FastEddy under MPI on GPUs \u00b6 Warning While the result of this demonstration is a functional application, we recommend against using this container for production FastEddy workflows! It is much easier to simply build FasyEddy \"bare metal\" when operating inside the NCAR HPC environment!! This example demonstrates building a containerized version of FastEddy from the open-source variant hosted on GitHub . It is provided for demonstration purposes because it demonstrates several common issues encountered when running GPU-aware MPI applications inside containers across multiple nodes, particularly when binding the host MPI into the container, and the source code is open for any interested user to follow along and adapt. About FastEddy \u00b6 FastEddy is a large-eddy simulation (LES) model developed by the Research Applications Laboratory (RAL) here at NCAR. The fundamental premise of FastEddy model development is to leverage the accelerated and more power efficient computing capacity of graphics processing units (GPU)s to enable not only more widespread use of LES in research activities but also to pursue the adoption of microscale and multiscale, turbulence-resolving, atmospheric boundary layer modeling into local scale weather prediction or actionable science and engineering applications. Containerization approach \u00b6 The container is built off-premises with docker from three related images, each providing a foundation for the next. We begin with a Rockylinux version 8 operating system with OpenHPC version 2 installed, then add a CUDA development environment and a CUDA-aware MPICH installation on top, and finally add the FastEddy source and compiled program. A benefit of this layered approach is that the intermediate images created in steps 1 and 2 can be beneficial in their own right, providing base layers for other projects with similar needs. Additionally, by building the image externally with Docker we are able to switch user IDs within the process (discussed further below), which has some benefits when using containers to enable development workflows. Building the image \u00b6 Build framework For complete details of the build process, see the Docker-based container build framework described here . The image was built external to the HPC environment and then pushed to Docker Hub. (For users only interested in the details of running such a container, see instructions for running the container below.) In this case a simple Mac laptop with git , GNU make , and docker all installed locally was used and the process takes about an hour; any similarly configured system should suffice. No GPU devices are required to build the image. The base layer \u00b6 The Rockylinx 8 + OpenHPC base layer For the base layer we deploy an OpenHPC v2 installation on top of a Rocklinux v8 base image. OpenHPC provides access to many pre-complied scientific libraries and applications, and supports a matrix of compilers and MPI permutations. and we will select one that works well with Derecho. Notably, at present OpenHPC does not natively support CUDA installations, however we will address this limitation in the subsequent steps. rocky8/OpenHPC-mpich/Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 FROM docker.io/rockylinux/rockylinux:8 ADD extras/docker-clean /usr/bin/docker-clean ARG COMPILER_VERSION=gnu9 ARG MPI_FAMILY=mpich ARG MPI_FAMILY_VARIANT=mpich-ofi ARG MPICH_VERSION=3.4.3 ARG OSU_VERSION=7.2 # Basic OpenHPC development environment setup, derived from Install_guide-Rocky8-Warewulf-SLURM-2.4 RUN echo \"yum/dnf config\" \\ && set -x \\ && adduser plainuser \\ && chmod a+rx /usr/bin/docker-clean && docker-clean \\ && yum -y update \\ && yum -y install which git tar curl xz bzip2 patch \\ && yum -y install http://repos.openhpc.community/OpenHPC/2/EL_8/x86_64/ohpc-release-2-1.el8.x86_64.rpm \\ && yum -y install dnf-plugins-core \\ && yum config-manager --set-enabled powertools \\ && yum -y install ohpc-base \\ && yum -y install lmod-ohpc nhc-ohpc ohpc-autotools \\ && yum -y install ${COMPILER_VERSION}-compilers-ohpc \\ && yum -y install hwloc-ohpc valgrind-ohpc \\ && yum -y install ${MPI_FAMILY_VARIANT}-${COMPILER_VERSION}-ohpc \\ && yum -y install lmod-defaults-${COMPILER_VERSION}-${MPI_FAMILY_VARIANT}-ohpc \\ && docker-clean # Prevent mpicxx from linking -lmpicxx, which we do not need, and cannot use on our Cray-EX RUN sed -i 's/cxxlibs=\"-lmpicxx\"/cxxlibs= #\"-lmpicxx\"/g' /opt/ohpc/pub/mpi/${MPI_FAMILY_VARIANT}-${COMPILER_VERSION}-ohpc/3.4.2/bin/mpicxx RUN mkdir -p /opt/local \\ && chown -R plainuser: /home/plainuser/ /opt/local COPY extras/hello_world_mpi.C /home/plainuser/ USER plainuser SHELL [\"/bin/bash\", \"-lc\"] RUN echo \"Installing MPI benchmark applications\" \\ && whoami && module avail && module list \\ && echo \"OSU:\" \\ && cd /tmp && curl -Sl https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz | tar xz \\ && cd osu-micro-benchmarks-${OSU_VERSION} \\ && ./configure --help \\ && ./configure --prefix=/opt/local/osu-micro-benchmarks-${OSU_VERSION} \\ CXX=$(which mpicxx) CC=$(which mpicc) FC=$(which mpif90) F77=$(which mpif77) \\ && make -j 8 V=0 && rm -rf /opt/local/osu-micro-benchmarks-${OSU_VERSION} && make install \\ && cd && rm -rf /tmp/osu-micro-benchmarks-${OSU_VERSION} \\ && cd /opt/local && mpicxx -o hello_world_mpi /home/plainuser/hello_world_mpi.C -fopenmp \\ && echo \"IMB:\" \\ && cd /opt/local && rm -rf imb-2021.3 && git clone https://github.com/intel/mpi-benchmarks.git imb-2021.3 \\ && cd /opt/local/imb-2021.3 && git checkout 8ba5d968272b6e7b384f91b6597d1c4590faf3db \\ && CXX=$(which mpicxx) CC=$(which mpicc) make \\ && make -C src_cpp -f Makefile TARGET=MPI1 clean \\ && make -C src_cpp -f Makefile TARGET=NBC clean\\ && make -C src_cpp -f Makefile TARGET=RMA clean \\ && make -C src_cpp -f Makefile TARGET=EXT clean \\ && make -C src_cpp -f Makefile TARGET=IO clean \\ && make -C src_cpp -f Makefile TARGET=MT clean \\ && make -C src_c/P2P -f Makefile TARGET=P2P clean \\ && docker-clean # Local Variables: # mode: sh # End: Dockerfile Steps The image begins with a minimal Rockylinux v8 image, and adds a utility script docker-clean copied from the build host. We parameterize several variables with build arguments using the ARG instructions. (Build arguments are available within the image build process as environment variables, but not when running the resulting container image; rather ENV instructions can be used for those purposes. For a full discussion of Dockerfiles and supported instructions see here .) We then perform a number of RUN steps. When running docker , each RUN step creates a subsequent layer in the image. (We follow general Docker guidance and also strive to combine related commands inside a handful of RUN instructions.) The first RUN instruction takes us from the very basic Rockylinux 8 source image to a full OpenHPC installation. We add a non-privileged user plainuser to leverage later, update the OS image with any available security patches, and then generally follow an OpenHPC installation recipe to add compilers, MPI, and other useful development tools. The second RUN step works around an issue we would find later when attempting to run the image on Derecho. Specifically, the OpenHPC mpich-ofi package provides support for the long-deprecated MPI C++ interface. This is not present on Derecho with the cray-mpich implementation we will ultimately use to run the container. Since we do not need this support, here we hack the mpicxx wrapper so that it does not link in -lmpicxx , the problematic library. The third and following RUN instructions steps create a directory space /opt/local we can use from our unprivileged plainuser account, copy in some more files, and then switch to plainuser to test the development environment by installing some common MPI benchmarks. Discussion OpenHPC v2 supports both OpenSUSE and Rocklinux 8 as its base OS. It would be natural to choose OpenSUSE for similarity to Casper and Derecho, however by choosing instead Rocklinux we gain access to a different build environment, which has benefits for developers looking to improve portability. This process followed here can also be thought of as a \"roadmap\" for deploying the application at similarly configured external sites. OpenHPC supports openmpi and mpich MPI implementations, with the latter in two forms: mpich-ucx and mpich-ofi . In this example we intentionally choose mpich-ofi with prior knowledge of the target execution environment. On Derecho the primary MPI implementation is cray-mpich (itself forked from mpich ) which uses an HPE-proprietary libfabric interface to the Slingshot v11 high-speed communication fabric. Notice that each RUN step is finalized with a docker-clean command . This utility script removes temporary files and cached data to minimize the size of the resulting image layers. One consequence is that the first dnf package manager interaction in a RUN statement will re-cache these data. Since cached data are not relevant in the final image - especially when run much later on - we recommend removing it to reduce image bloat. In this example we are intentional switching between root (the default user in the build process) and our unprivileged plainuser account. Particularly in development workflows, we want to be sure compilation and installation steps work properly as an unprivileged user, and tools such as the lmod module system and mpiexec often are intended not to be used as root . Since MPI container runtime inregration can be a pain point at execution, we install OSU's and Intel's MPI benchmark suites to aid in deployment testing, independent of any user application. Building the image docker build --tag <dockerhub_username>/rocky8-openhpc-mpich:latest . Adding CUDA & CUDA-aware MPICH \u00b6 Adding CUDA + CUDA-aware MPICH Next we add CUDA and add a CUDA-aware MPI installation. We choose a specific version of the open-source MPICH library (both to closely match what is provided by OpenHPC and for Derecho compatibility) and configure it to use the pre-existing OpenHPC artifacts ( hwloc , libfabric ) as dependencies. For both cuda and the new mpich we also install \"modulefiles\" so the new additions are available in the typical module environment. Finally, we re-install one of the MPI benchmark applications, this time with CUDA support. rocky8/OpenHPC-cuda/Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 FROM benjaminkirk/rocky8-openhpc-mpich:latest ARG COMPILER_VERSION=gnu9 ARG MPI_FAMILY=mpich ARG MPI_FAMILY_VARIANT=mpich-ofi ARG MPICH_VERSION=3.4.3 ARG OSU_VERSION=7.2 USER root # https://developer.nvidia.com/cuda-11-7-1-download-archive RUN echo \"Cuda\" \\ && curl -O https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm \\ && rpm -i cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm && rm -f cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm \\ && dnf -y install cuda && rm /var/cuda-repo-rhel8-11-7-local/*.rpm && dnf config-manager --disable cuda-rhel8-11-7-local \\ && echo \"RDMA prereqs\" \\ && dnf -y install libibverbs-devel libpsm2-devel \\ && docker-clean RUN mkdir /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/cuda COPY extras/cuda-11.7 /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/cuda/11.7 COPY extras/mpich-${MPICH_VERSION}-ofi-cuda /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/mpich/${MPICH_VERSION}-ofi-cuda COPY extras/hello_world.cu /home/plainuser RUN mkdir -p /opt/local \\ && chown -R plainuser: /home/plainuser/ /opt/local USER plainuser SHELL [\"/bin/bash\", \"-lc\"] RUN whoami && module avail \\ && module load -mpich +hwloc +libfabric +cuda && module list \\ && cd /opt/local && nvcc -o hello_cuda /home/plainuser/hello_world.cu --cudart shared \\ && cd /tmp && curl -sSL https://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz | tar xz \\ && cd mpich-${MPICH_VERSION} \\ && ./configure --help \\ && ./configure --prefix=/opt/local/mpich-${MPICH_VERSION}-cuda \\ CC=$(which gcc) CXX=$(which g++) FC=$(which gfortran) F77=$(which gfortran) \\ --enable-fortran \\ --with-libfabric=${LIBFABRIC_DIR} \\ --with-hwloc-prefix=${HWLOC_DIR} \\ --with-cuda=${CUDA_HOME} \\ && make -j 8 && make install \\ && docker-clean # Prevent mpicxx from linking -lmpicxx, which we do not need, and cannot use on our Cray-EX RUN sed -i 's/cxxlibs=\"-lmpicxx\"/cxxlibs= #\"-lmpicxx\"/g' /opt/local/mpich-${MPICH_VERSION}-cuda/bin/mpicxx RUN echo \"Installing MPI benchmark applications\" \\ && whoami && module avail \\ && module load mpich/${MPICH_VERSION}-ofi-cuda && module list \\ && echo \"OSU:\" \\ && cd /tmp && curl -Sl https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz | tar xz \\ && cd osu-micro-benchmarks-${OSU_VERSION} \\ && ./configure --help \\ && ./configure --prefix=/opt/local/osu-micro-benchmarks-${OSU_VERSION} \\ CXX=$(which mpicxx) CC=$(which mpicc) FC=$(which mpif90) F77=$(which mpif77) LIBS=\"-L${CUDA_HOME}/targets/x86_64-linux/lib -lcudart\" \\ --enable-cuda --with-cuda=${CUDA_HOME} \\ && make -j 8 V=0 && rm -rf /opt/local/osu-micro-benchmarks-${OSU_VERSION} && make install \\ && cd && rm -rf /tmp/osu-micro-benchmarks-${OSU_VERSION} \\ && cd /opt/local && mpicxx -o hello_world_mpi /home/plainuser/hello_world_mpi.C -fopenmp \\ && docker-clean # Local Variables: # mode: sh # End: Dockerfile Steps We switch back to the root user so we can modify the operating system installation within the image. The first RUN instruction installs a full CUDA development environment and some additional development packages required to build MPI later. The next RUN instructions install modulefiles into the image so we can access the CUDA and (upcoming) MPICH installation, and clean up file permissions. The remaining steps are executed again as our unprivileged plainuser . The fourth RUN instruction downloads, configures, and installs MPICH. The version is chosen to closely match the baseline MPICH already installed in the image and uses some of its dependencies, and we also enable CUDA support. In the final RUN instruction we re-install one of the MPI benchmark applications, this time with CUDA support. Discussion There are several ways to install CUDA, here we choose a \"local repo\" installation because it allows us to control versions, but are careful also to remove the downloaded packages after installation, freeing up 3GB+ in the image. The CUDA development environment is very large and it is difficult to separate unnecessary components, so is step increases the size of the image from ~1.2GB to 8.8GB. We leave all components in the development image, including tools we will very likely not need inside a container such as nsight-systems and nsight-compute . For applications built on top of this image, a user could optionally remove these components later to decrease their final image size (demonstrated next). Building the image docker build --tag <dockerhub_username>/rocky8-openhpc-mpich-cuda:latest . Building FastEddy \u00b6 Adding FastEddy rocky8/OpenHPC-FastEddy/Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 FROM benjaminkirk/rocky8-openhpc-mpich-cuda:latest ARG MPICH_VERSION=3.4.3 USER root RUN echo \"netcdf: serial netcdf from the base OS\" \\ && yum -y install \\ netcdf-devel \\ && echo \"removing unnecessary NVIDIA components to shrink container image\" \\ && rm -rf /opt/local/nvidia /usr/local/cuda-11.7/targets/x86_64-linux/lib/*_static.a \\ && docker-clean USER plainuser RUN echo \"FastEddy - source\" \\ && cd /opt/local && git clone https://github.com/NCAR/FastEddy-model.git && git clone https://github.com/NCAR/FastEddy-tutorials.git \\ && cd FastEddy-model/SRC/FEMAIN \\ && sed -i 's/TEST_LIBS = -lm -lmpi -lstdc++ -lcurand/TEST_LIBS = -lm -lmpi -lstdc++ $(LIBS)/g' Makefile \\ && sed -i 's/TEST_CU_LIBS = -lm -lmpi -lcurand/TEST_CU_LIBS = -lm -lmpi $(LIBS)/g' Makefile \\ && docker-clean RUN echo \"FastEddy - build\" \\ && module avail && module load mpich/${MPICH_VERSION}-ofi-cuda && module list \\ && cd /opt/local/FastEddy-model/SRC && fe_inc= && for d in */ */*/ ; do fe_inc=\"-I$(pwd)/${d} ${fe_inc}\" ; done \\ && cd FEMAIN && make \\ INCLUDES=\"${fe_inc} -I${MPI_DIR}/include/ -I${CUDA_HOME}/targets/x86_64-linux/include/\" \\ LIBS=\"-L${CUDA_HOME}/targets/x86_64-linux/lib -lcurand -lcudart -lcuda -L/usr/lib64 -lnetcdf\" \\ && ldd ./FastEddy \\ && docker-clean # Local Variables: # mode: sh # End: Dockerfile Steps Again we switch back to root for performing operating system level tasks, as our base image left us as plainuser . The first RUN instruction installs the development package for NetCDF - an additional application dependency not already satisfied. We also remove some particularly large CUDA components from the development image not required in the final application image. Then again as plainuser , the next RUN instruction downloads the FastEddy open-source variant. We make some changes to the definition of a few hard-coded make variables so that we can specify installation paths during linking later. The final RUN instruction then builds FastEddy. We build up and use custom INCLUDE and LIBS variables, specifying some unique paths for the particular build environment. Discussion When building the image locally with Docker, the space savings from step (2) are not immediately apparent. This is a result of the Docker \"layer\" approach: the content still exists in the base layer and is only \"logically\" removed by the commands listed above. The space savings is realized on the HPC system when we \"pull\" the image with singularity . If an even smaller container image is desired, even more components could be stripped: CUDA numerical libraries the application does not need, or even the containerized MPIs after we are done with them. As we will see next, we replace the container MPI with the host MPI at run-time, so technically no MPI is required inside the container when we are done using it for compilation. Building the image docker build --tag <dockerhub_username>/rocky8-openhpc-fasteddy:latest . Pushing the image to Docker Hub docker push <dockerhub_username>/rocky8-openhpc-fasteddy:latest Running the container on Derecho \u00b6 With the container built from the steps above (or simply pulling the resulting image from Docker Hub), we are now ready to run a sample test case on Derecho. We choose Example02_CBL.in from the FastEddy Tutorial and modify it to run on 24 GPUs (full steps listed here ). The PBS job script listed below shows the steps required to \"bind\" the host MPI into the container. Containerized FastEddy PBS Script run_fasteddy_container.pbs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" Discussion The mpiexec command is fairly standard. Note that we are using it to launch singularity , which in turn will start up the containerized FastEddy executable. The singularity exec command line is complex, so let's deconstruct it here: We make use of the --bind argument first to mount familiar GLADE file systems within the container, and again to \"inject\" the host MPI into the container (as described here ). The /run directory necessity is not immediately obvious but is used by Cray-MPICH as part of the launching process. We also need to use the --env to set the LD_LIBRARY_PATH inside the image so that the application can find the proper host libraries. Recall when we built the FastEddy executable in the containerized environment it had no knowledge of these host-specific paths. Similarly, we use --env to set the LD_PRELOAD environment variable inside the container. This will cause a particular Cray-MPICH library to be loaded prior to application initialization. This step is not required for \"bare metal\" execution. We set some important Cray-MPICH specific MPICH_* environment variables as well to enable CUDA-awareness ( MPICH_GPU_* ) and work around an MPI run-time error ( MPICH_SMP_SINGLE_COPY_MODE ) that will otherwise appear. Finally, a note on the --bind /usr/lib64:/host/lib64 argument. Injecting the host MPI requires that some shared libraries from the host's /usr/lib64 directory be visible inside the image. However, this path also exists inside the image and contains other libraries needed by the application. We cannot simply bind the hosts directory into the same path, doing so will mask these other libraries. So we bind the host's /usr/lib64 into the container image at /host/lib64 , and make sure this path is set in the LD_LIBRARY_PATH variable as well. Because we want these particular host libraries found as last resort (not taking precedence over similar libraries in the container, we append /host/lib64 to the LD_LIBRARY_PATH search path. The arguments above were determined iteratively through trial and error. Such is the reality of containerized MPI applications and proprietary host MPI integration. Feel free to experiment with the PBS file, omitting some of the --bind and --env arguments and observing the resulting error message, however do NOT modify the MPICH_GPU_* variables , doing so may trigger a very unfortunate kernel driver bug and render the GPU compute nodes unusable. Pulling the image We begin with pulling the image from Docke Hub and constructing a SIF. (If you want to test your own built/pushed image, replace benjaminkirk with your own <dockerhub_username> as specified in the tag/push operations listed above.) derecho$ singularity pull rocky8-openhpc-fasteddy.sif docker://benjaminkirk/rocky8-openhpc-fasteddy:latest [...] derecho$ ls -lh rocky8-openhpc-fasteddy.sif -rwxr-xr-x 1 someuser ncar 3.1G Dec 5 17:08 rocky8-openhpc-fasteddy.sif Running the job derecho$ mkdir ./output derecho$ qsub -A <account> run_fasteddy_container.pbs derecho$ tail -f fasteddy_job.log","title":"Example Workflows"},{"location":"environment-and-software/user-environment/containers/examples/#sample-containerized-workflows","text":"Warning This page contains a sample of containerized workflows that demonstrate various techniques built up in practice, often from resolving user issues. We do not necessarily endorse or support each use case, rather these examples are provided in hopes they may be useful to demonstrate (i) sample containerized workflows, and (ii) solutions to various problems you may encounter.","title":"Sample Containerized Workflows"},{"location":"environment-and-software/user-environment/containers/examples/#nvidias-ngc-containers","text":"NVIDIA's NGC is a catalog of software optimized for GPUs. NGC containers allow you to run data science projects \"out of the box\" without installing, configuring, or integrating the infrastructure.","title":"NVIDIA's NGC containers"},{"location":"environment-and-software/user-environment/containers/examples/#nvidias-modulus-physics-ml-framework","text":"NVIDIA Modulus is an open source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods. NVIDIA provides a frequently updated Docker image with a containerized PyTorch installation that can be run under Apptainer, albeit with some effort. Because the container is designed for Docker, some additional steps are required as discussed below. Running containerized NVIDIA-Modulus on a single Casper GPU Rather than pull the container and run as-is, we will create a derived container that allows us to encapsulate our desired changes. The primary reason for this is the Modulus container assumes the container is writable and makes changes during execution. Since we will run under Apptainer using a compressed, read-only image, this fails. Therefore we will make our own derived image and make the requisite changes during the build process. This is accomplished first by creating a simple Apptainer definition file: my_modulus.def From: nvcr.io/nvidia/modulus/modulus:23.09 %post # update pip python -m pip install --upgrade pip # use pip to install additional packages needed for examples later pip install warp-lang # Remove cuda compat layer (https://github.com/NVIDIA/nvidia-docker/issues/1256) # note that the source container attempts to do this at run-time, but that will # fail when launched read-only. So we do that here instead. # (This issue will likely be resolved with newer versions of nvidia-modulus) rm -rf /usr/local/cuda/compat/lib The definition file begins by pulling a specified version of the Modulus container, then modifying it in our %post step. In %post we update the pip Python package installer, use pip to install some additional Python packages not in the base image but required for the examples run later, and finally removes a conflicting path from the source image. Using the my_modulus.def file we now create our derived container and store it as a SIF: module load apptainer TMPDIR=/var/tmp/ singularity build my_modulus.sif my_modulus.def Note in this step we have explicitly set TMPDIR to a local file system, as occasionally containers fail to build on the large parallel file systems usually used for TMPDIR within NCAR. (The failure symptoms are usually fatal error messages related to xattrs .) Fetch some examples so we can test our installation: git clone https://github.com/NVIDIA/modulus.git Run the container in an interactive session on a single Casper GPU. We will launch an interactive session, then run the container interactively with the singularity shell command. # Interactive PBS submission from a login node: qsub -I -A <ACCOUNT> -q casper -l select=1:ncpus=4:mpiprocs=4:ngpus=1 -l gpu_type=v100 -l walltime=1:00:00 # Then on the GPU node: module load apptainer singularity shell \\ --nv --cleanenv \\ --bind /glade/work \\ --bind /glade/campaign \\ --bind /glade/derecho/scratch \\ ./my_modulus.sif Note the command line arguments to singularity shell : --nv : enable Nvidia support, --cleanenv : clean environment before running container, causing the container to be launched with no knowledge of environment variables set on the host. This is default behavior for Docker, and is required in this case to prevent conflicting CUDA_* and other environment variable settings from confusing the contanierized PyTorch. --bind /glade/work etc...: binds host file systems into the container, allowing us to read and write from GLADE. Now we are inside the container, as evidenced by the Apptainer> command line prompt in the final step of this example. We will run one of the sample problems checked out in step 3: Apptainer> cd modulus/examples/cfd/darcy_fno/ Apptainer> python ./train_fno_darcy.py Warp 0.10.1 initialized: CUDA Toolkit: 11.5, Driver: 12.3 Devices: \"cpu\" | x86_64 \"cuda:0\" | Tesla V100-SXM2-32GB (sm_70) Kernel cache: /glade/u/home/benkirk/.cache/warp/0.10.1 [21:04:13 - mlflow - WARNING] Checking MLFlow logging location is working (if this hangs its not) [21:04:13 - mlflow - INFO] MLFlow logging location is working [21:04:13 - mlflow - INFO] No Darcy_FNO experiment found, creating... [21:04:13 - checkpoint - WARNING] Provided checkpoint directory ./checkpoints does not exist, skipping load [21:04:13 - darcy_fno - WARNING] Model FourierNeuralOperator does not support AMP on GPUs, turning off [21:04:13 - darcy_fno - WARNING] Model FourierNeuralOperator does not support AMP on GPUs, turning off [21:04:13 - darcy_fno - INFO] Training started... Module modulus.datapipes.benchmarks.kernels.initialization load on device 'cuda:0' took 205.84 ms Module modulus.datapipes.benchmarks.kernels.utils load on device 'cuda:0' took 212.94 ms Module modulus.datapipes.benchmarks.kernels.finite_difference load on device 'cuda:0' took 670.44 ms [21:04:46 - train - INFO] Epoch 1 Metrics: Learning Rate = 1.000e-03, loss = 6.553e-01 [21:04:46 - train - INFO] Epoch Execution Time: 3.241e+01s, Time/Iter: 1.013e+03ms [21:05:14 - train - INFO] Epoch 2 Metrics: Learning Rate = 1.000e-03, loss = 4.255e-02 [21:05:14 - train - INFO] Epoch Execution Time: 2.812e+01s, Time/Iter: 8.786e+02ms [...] While this example demonstrated running the container interactively, alternatively steps 3 and 4 can be combined to be run inside a PBS batch job.","title":"NVIDIA's Modulus physics-ML framework"},{"location":"environment-and-software/user-environment/containers/examples/#popular-aiml-tools","text":"Optimized Tensorflow and PyTorch models are available directly from the NGC. Running AI/ML tools from NGC containers Building an image with Apptainer Anticipating that we may want to make additions to the container, we will build our own derived Apptainer image using a Definition file. Tensorflow PyTorch ngc_tensorflow.def Bootstrap: docker From: nvcr.io/nvidia/tensorflow:23.11-tf2-py3 %post # update pip python -m pip install --upgrade pip [...] module load apptainer TMPDIR = /var/tmp/ singularity build my_image.sif ngc_tensorflow.def ngc_pytorch.def Bootstrap: docker From: nvcr.io/nvidia/pytorch:23.11-py3 %post # update pip python -m pip install --upgrade pip [...] module load apptainer TMPDIR = /var/tmp/ singularity build my_image.sif ngc_pytorch.def Run the image module load apptainer singularity shell \\ --nv --cleanenv \\ --bind /glade/work \\ --bind /glade/campaign \\ --bind /glade/derecho/scratch \\ ./my_image.sif [...] Apptainer> We are now inside the container. Note the command line arguments to singularity shell : --nv --cleanenv enables NVIDIA support with a clean environment; --bind /glade/work etc...: binds host file systems into the container, allowing us to read and write from GLADE.","title":"Popular AI/ML tools"},{"location":"environment-and-software/user-environment/containers/examples/#building-and-running-containerized-fasteddy-under-mpi-on-gpus","text":"Warning While the result of this demonstration is a functional application, we recommend against using this container for production FastEddy workflows! It is much easier to simply build FasyEddy \"bare metal\" when operating inside the NCAR HPC environment!! This example demonstrates building a containerized version of FastEddy from the open-source variant hosted on GitHub . It is provided for demonstration purposes because it demonstrates several common issues encountered when running GPU-aware MPI applications inside containers across multiple nodes, particularly when binding the host MPI into the container, and the source code is open for any interested user to follow along and adapt.","title":"Building and running containerized FastEddy under MPI on GPUs"},{"location":"environment-and-software/user-environment/containers/examples/#about-fasteddy","text":"FastEddy is a large-eddy simulation (LES) model developed by the Research Applications Laboratory (RAL) here at NCAR. The fundamental premise of FastEddy model development is to leverage the accelerated and more power efficient computing capacity of graphics processing units (GPU)s to enable not only more widespread use of LES in research activities but also to pursue the adoption of microscale and multiscale, turbulence-resolving, atmospheric boundary layer modeling into local scale weather prediction or actionable science and engineering applications.","title":"About FastEddy"},{"location":"environment-and-software/user-environment/containers/examples/#containerization-approach","text":"The container is built off-premises with docker from three related images, each providing a foundation for the next. We begin with a Rockylinux version 8 operating system with OpenHPC version 2 installed, then add a CUDA development environment and a CUDA-aware MPICH installation on top, and finally add the FastEddy source and compiled program. A benefit of this layered approach is that the intermediate images created in steps 1 and 2 can be beneficial in their own right, providing base layers for other projects with similar needs. Additionally, by building the image externally with Docker we are able to switch user IDs within the process (discussed further below), which has some benefits when using containers to enable development workflows.","title":"Containerization approach"},{"location":"environment-and-software/user-environment/containers/examples/#building-the-image","text":"Build framework For complete details of the build process, see the Docker-based container build framework described here . The image was built external to the HPC environment and then pushed to Docker Hub. (For users only interested in the details of running such a container, see instructions for running the container below.) In this case a simple Mac laptop with git , GNU make , and docker all installed locally was used and the process takes about an hour; any similarly configured system should suffice. No GPU devices are required to build the image.","title":"Building the image"},{"location":"environment-and-software/user-environment/containers/examples/#the-base-layer","text":"The Rockylinx 8 + OpenHPC base layer For the base layer we deploy an OpenHPC v2 installation on top of a Rocklinux v8 base image. OpenHPC provides access to many pre-complied scientific libraries and applications, and supports a matrix of compilers and MPI permutations. and we will select one that works well with Derecho. Notably, at present OpenHPC does not natively support CUDA installations, however we will address this limitation in the subsequent steps. rocky8/OpenHPC-mpich/Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 FROM docker.io/rockylinux/rockylinux:8 ADD extras/docker-clean /usr/bin/docker-clean ARG COMPILER_VERSION=gnu9 ARG MPI_FAMILY=mpich ARG MPI_FAMILY_VARIANT=mpich-ofi ARG MPICH_VERSION=3.4.3 ARG OSU_VERSION=7.2 # Basic OpenHPC development environment setup, derived from Install_guide-Rocky8-Warewulf-SLURM-2.4 RUN echo \"yum/dnf config\" \\ && set -x \\ && adduser plainuser \\ && chmod a+rx /usr/bin/docker-clean && docker-clean \\ && yum -y update \\ && yum -y install which git tar curl xz bzip2 patch \\ && yum -y install http://repos.openhpc.community/OpenHPC/2/EL_8/x86_64/ohpc-release-2-1.el8.x86_64.rpm \\ && yum -y install dnf-plugins-core \\ && yum config-manager --set-enabled powertools \\ && yum -y install ohpc-base \\ && yum -y install lmod-ohpc nhc-ohpc ohpc-autotools \\ && yum -y install ${COMPILER_VERSION}-compilers-ohpc \\ && yum -y install hwloc-ohpc valgrind-ohpc \\ && yum -y install ${MPI_FAMILY_VARIANT}-${COMPILER_VERSION}-ohpc \\ && yum -y install lmod-defaults-${COMPILER_VERSION}-${MPI_FAMILY_VARIANT}-ohpc \\ && docker-clean # Prevent mpicxx from linking -lmpicxx, which we do not need, and cannot use on our Cray-EX RUN sed -i 's/cxxlibs=\"-lmpicxx\"/cxxlibs= #\"-lmpicxx\"/g' /opt/ohpc/pub/mpi/${MPI_FAMILY_VARIANT}-${COMPILER_VERSION}-ohpc/3.4.2/bin/mpicxx RUN mkdir -p /opt/local \\ && chown -R plainuser: /home/plainuser/ /opt/local COPY extras/hello_world_mpi.C /home/plainuser/ USER plainuser SHELL [\"/bin/bash\", \"-lc\"] RUN echo \"Installing MPI benchmark applications\" \\ && whoami && module avail && module list \\ && echo \"OSU:\" \\ && cd /tmp && curl -Sl https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz | tar xz \\ && cd osu-micro-benchmarks-${OSU_VERSION} \\ && ./configure --help \\ && ./configure --prefix=/opt/local/osu-micro-benchmarks-${OSU_VERSION} \\ CXX=$(which mpicxx) CC=$(which mpicc) FC=$(which mpif90) F77=$(which mpif77) \\ && make -j 8 V=0 && rm -rf /opt/local/osu-micro-benchmarks-${OSU_VERSION} && make install \\ && cd && rm -rf /tmp/osu-micro-benchmarks-${OSU_VERSION} \\ && cd /opt/local && mpicxx -o hello_world_mpi /home/plainuser/hello_world_mpi.C -fopenmp \\ && echo \"IMB:\" \\ && cd /opt/local && rm -rf imb-2021.3 && git clone https://github.com/intel/mpi-benchmarks.git imb-2021.3 \\ && cd /opt/local/imb-2021.3 && git checkout 8ba5d968272b6e7b384f91b6597d1c4590faf3db \\ && CXX=$(which mpicxx) CC=$(which mpicc) make \\ && make -C src_cpp -f Makefile TARGET=MPI1 clean \\ && make -C src_cpp -f Makefile TARGET=NBC clean\\ && make -C src_cpp -f Makefile TARGET=RMA clean \\ && make -C src_cpp -f Makefile TARGET=EXT clean \\ && make -C src_cpp -f Makefile TARGET=IO clean \\ && make -C src_cpp -f Makefile TARGET=MT clean \\ && make -C src_c/P2P -f Makefile TARGET=P2P clean \\ && docker-clean # Local Variables: # mode: sh # End: Dockerfile Steps The image begins with a minimal Rockylinux v8 image, and adds a utility script docker-clean copied from the build host. We parameterize several variables with build arguments using the ARG instructions. (Build arguments are available within the image build process as environment variables, but not when running the resulting container image; rather ENV instructions can be used for those purposes. For a full discussion of Dockerfiles and supported instructions see here .) We then perform a number of RUN steps. When running docker , each RUN step creates a subsequent layer in the image. (We follow general Docker guidance and also strive to combine related commands inside a handful of RUN instructions.) The first RUN instruction takes us from the very basic Rockylinux 8 source image to a full OpenHPC installation. We add a non-privileged user plainuser to leverage later, update the OS image with any available security patches, and then generally follow an OpenHPC installation recipe to add compilers, MPI, and other useful development tools. The second RUN step works around an issue we would find later when attempting to run the image on Derecho. Specifically, the OpenHPC mpich-ofi package provides support for the long-deprecated MPI C++ interface. This is not present on Derecho with the cray-mpich implementation we will ultimately use to run the container. Since we do not need this support, here we hack the mpicxx wrapper so that it does not link in -lmpicxx , the problematic library. The third and following RUN instructions steps create a directory space /opt/local we can use from our unprivileged plainuser account, copy in some more files, and then switch to plainuser to test the development environment by installing some common MPI benchmarks. Discussion OpenHPC v2 supports both OpenSUSE and Rocklinux 8 as its base OS. It would be natural to choose OpenSUSE for similarity to Casper and Derecho, however by choosing instead Rocklinux we gain access to a different build environment, which has benefits for developers looking to improve portability. This process followed here can also be thought of as a \"roadmap\" for deploying the application at similarly configured external sites. OpenHPC supports openmpi and mpich MPI implementations, with the latter in two forms: mpich-ucx and mpich-ofi . In this example we intentionally choose mpich-ofi with prior knowledge of the target execution environment. On Derecho the primary MPI implementation is cray-mpich (itself forked from mpich ) which uses an HPE-proprietary libfabric interface to the Slingshot v11 high-speed communication fabric. Notice that each RUN step is finalized with a docker-clean command . This utility script removes temporary files and cached data to minimize the size of the resulting image layers. One consequence is that the first dnf package manager interaction in a RUN statement will re-cache these data. Since cached data are not relevant in the final image - especially when run much later on - we recommend removing it to reduce image bloat. In this example we are intentional switching between root (the default user in the build process) and our unprivileged plainuser account. Particularly in development workflows, we want to be sure compilation and installation steps work properly as an unprivileged user, and tools such as the lmod module system and mpiexec often are intended not to be used as root . Since MPI container runtime inregration can be a pain point at execution, we install OSU's and Intel's MPI benchmark suites to aid in deployment testing, independent of any user application. Building the image docker build --tag <dockerhub_username>/rocky8-openhpc-mpich:latest .","title":"The base layer"},{"location":"environment-and-software/user-environment/containers/examples/#adding-cuda-cuda-aware-mpich","text":"Adding CUDA + CUDA-aware MPICH Next we add CUDA and add a CUDA-aware MPI installation. We choose a specific version of the open-source MPICH library (both to closely match what is provided by OpenHPC and for Derecho compatibility) and configure it to use the pre-existing OpenHPC artifacts ( hwloc , libfabric ) as dependencies. For both cuda and the new mpich we also install \"modulefiles\" so the new additions are available in the typical module environment. Finally, we re-install one of the MPI benchmark applications, this time with CUDA support. rocky8/OpenHPC-cuda/Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 FROM benjaminkirk/rocky8-openhpc-mpich:latest ARG COMPILER_VERSION=gnu9 ARG MPI_FAMILY=mpich ARG MPI_FAMILY_VARIANT=mpich-ofi ARG MPICH_VERSION=3.4.3 ARG OSU_VERSION=7.2 USER root # https://developer.nvidia.com/cuda-11-7-1-download-archive RUN echo \"Cuda\" \\ && curl -O https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm \\ && rpm -i cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm && rm -f cuda-repo-rhel8-11-7-local-11.7.1_515.65.01-1.x86_64.rpm \\ && dnf -y install cuda && rm /var/cuda-repo-rhel8-11-7-local/*.rpm && dnf config-manager --disable cuda-rhel8-11-7-local \\ && echo \"RDMA prereqs\" \\ && dnf -y install libibverbs-devel libpsm2-devel \\ && docker-clean RUN mkdir /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/cuda COPY extras/cuda-11.7 /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/cuda/11.7 COPY extras/mpich-${MPICH_VERSION}-ofi-cuda /opt/ohpc/pub/moduledeps/${COMPILER_VERSION}/mpich/${MPICH_VERSION}-ofi-cuda COPY extras/hello_world.cu /home/plainuser RUN mkdir -p /opt/local \\ && chown -R plainuser: /home/plainuser/ /opt/local USER plainuser SHELL [\"/bin/bash\", \"-lc\"] RUN whoami && module avail \\ && module load -mpich +hwloc +libfabric +cuda && module list \\ && cd /opt/local && nvcc -o hello_cuda /home/plainuser/hello_world.cu --cudart shared \\ && cd /tmp && curl -sSL https://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz | tar xz \\ && cd mpich-${MPICH_VERSION} \\ && ./configure --help \\ && ./configure --prefix=/opt/local/mpich-${MPICH_VERSION}-cuda \\ CC=$(which gcc) CXX=$(which g++) FC=$(which gfortran) F77=$(which gfortran) \\ --enable-fortran \\ --with-libfabric=${LIBFABRIC_DIR} \\ --with-hwloc-prefix=${HWLOC_DIR} \\ --with-cuda=${CUDA_HOME} \\ && make -j 8 && make install \\ && docker-clean # Prevent mpicxx from linking -lmpicxx, which we do not need, and cannot use on our Cray-EX RUN sed -i 's/cxxlibs=\"-lmpicxx\"/cxxlibs= #\"-lmpicxx\"/g' /opt/local/mpich-${MPICH_VERSION}-cuda/bin/mpicxx RUN echo \"Installing MPI benchmark applications\" \\ && whoami && module avail \\ && module load mpich/${MPICH_VERSION}-ofi-cuda && module list \\ && echo \"OSU:\" \\ && cd /tmp && curl -Sl https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-${OSU_VERSION}.tar.gz | tar xz \\ && cd osu-micro-benchmarks-${OSU_VERSION} \\ && ./configure --help \\ && ./configure --prefix=/opt/local/osu-micro-benchmarks-${OSU_VERSION} \\ CXX=$(which mpicxx) CC=$(which mpicc) FC=$(which mpif90) F77=$(which mpif77) LIBS=\"-L${CUDA_HOME}/targets/x86_64-linux/lib -lcudart\" \\ --enable-cuda --with-cuda=${CUDA_HOME} \\ && make -j 8 V=0 && rm -rf /opt/local/osu-micro-benchmarks-${OSU_VERSION} && make install \\ && cd && rm -rf /tmp/osu-micro-benchmarks-${OSU_VERSION} \\ && cd /opt/local && mpicxx -o hello_world_mpi /home/plainuser/hello_world_mpi.C -fopenmp \\ && docker-clean # Local Variables: # mode: sh # End: Dockerfile Steps We switch back to the root user so we can modify the operating system installation within the image. The first RUN instruction installs a full CUDA development environment and some additional development packages required to build MPI later. The next RUN instructions install modulefiles into the image so we can access the CUDA and (upcoming) MPICH installation, and clean up file permissions. The remaining steps are executed again as our unprivileged plainuser . The fourth RUN instruction downloads, configures, and installs MPICH. The version is chosen to closely match the baseline MPICH already installed in the image and uses some of its dependencies, and we also enable CUDA support. In the final RUN instruction we re-install one of the MPI benchmark applications, this time with CUDA support. Discussion There are several ways to install CUDA, here we choose a \"local repo\" installation because it allows us to control versions, but are careful also to remove the downloaded packages after installation, freeing up 3GB+ in the image. The CUDA development environment is very large and it is difficult to separate unnecessary components, so is step increases the size of the image from ~1.2GB to 8.8GB. We leave all components in the development image, including tools we will very likely not need inside a container such as nsight-systems and nsight-compute . For applications built on top of this image, a user could optionally remove these components later to decrease their final image size (demonstrated next). Building the image docker build --tag <dockerhub_username>/rocky8-openhpc-mpich-cuda:latest .","title":"Adding CUDA &amp; CUDA-aware MPICH"},{"location":"environment-and-software/user-environment/containers/examples/#building-fasteddy","text":"Adding FastEddy rocky8/OpenHPC-FastEddy/Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 FROM benjaminkirk/rocky8-openhpc-mpich-cuda:latest ARG MPICH_VERSION=3.4.3 USER root RUN echo \"netcdf: serial netcdf from the base OS\" \\ && yum -y install \\ netcdf-devel \\ && echo \"removing unnecessary NVIDIA components to shrink container image\" \\ && rm -rf /opt/local/nvidia /usr/local/cuda-11.7/targets/x86_64-linux/lib/*_static.a \\ && docker-clean USER plainuser RUN echo \"FastEddy - source\" \\ && cd /opt/local && git clone https://github.com/NCAR/FastEddy-model.git && git clone https://github.com/NCAR/FastEddy-tutorials.git \\ && cd FastEddy-model/SRC/FEMAIN \\ && sed -i 's/TEST_LIBS = -lm -lmpi -lstdc++ -lcurand/TEST_LIBS = -lm -lmpi -lstdc++ $(LIBS)/g' Makefile \\ && sed -i 's/TEST_CU_LIBS = -lm -lmpi -lcurand/TEST_CU_LIBS = -lm -lmpi $(LIBS)/g' Makefile \\ && docker-clean RUN echo \"FastEddy - build\" \\ && module avail && module load mpich/${MPICH_VERSION}-ofi-cuda && module list \\ && cd /opt/local/FastEddy-model/SRC && fe_inc= && for d in */ */*/ ; do fe_inc=\"-I$(pwd)/${d} ${fe_inc}\" ; done \\ && cd FEMAIN && make \\ INCLUDES=\"${fe_inc} -I${MPI_DIR}/include/ -I${CUDA_HOME}/targets/x86_64-linux/include/\" \\ LIBS=\"-L${CUDA_HOME}/targets/x86_64-linux/lib -lcurand -lcudart -lcuda -L/usr/lib64 -lnetcdf\" \\ && ldd ./FastEddy \\ && docker-clean # Local Variables: # mode: sh # End: Dockerfile Steps Again we switch back to root for performing operating system level tasks, as our base image left us as plainuser . The first RUN instruction installs the development package for NetCDF - an additional application dependency not already satisfied. We also remove some particularly large CUDA components from the development image not required in the final application image. Then again as plainuser , the next RUN instruction downloads the FastEddy open-source variant. We make some changes to the definition of a few hard-coded make variables so that we can specify installation paths during linking later. The final RUN instruction then builds FastEddy. We build up and use custom INCLUDE and LIBS variables, specifying some unique paths for the particular build environment. Discussion When building the image locally with Docker, the space savings from step (2) are not immediately apparent. This is a result of the Docker \"layer\" approach: the content still exists in the base layer and is only \"logically\" removed by the commands listed above. The space savings is realized on the HPC system when we \"pull\" the image with singularity . If an even smaller container image is desired, even more components could be stripped: CUDA numerical libraries the application does not need, or even the containerized MPIs after we are done with them. As we will see next, we replace the container MPI with the host MPI at run-time, so technically no MPI is required inside the container when we are done using it for compilation. Building the image docker build --tag <dockerhub_username>/rocky8-openhpc-fasteddy:latest . Pushing the image to Docker Hub docker push <dockerhub_username>/rocky8-openhpc-fasteddy:latest","title":"Building FastEddy"},{"location":"environment-and-software/user-environment/containers/examples/#running-the-container-on-derecho","text":"With the container built from the steps above (or simply pulling the resulting image from Docker Hub), we are now ready to run a sample test case on Derecho. We choose Example02_CBL.in from the FastEddy Tutorial and modify it to run on 24 GPUs (full steps listed here ). The PBS job script listed below shows the steps required to \"bind\" the host MPI into the container. Containerized FastEddy PBS Script run_fasteddy_container.pbs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" Discussion The mpiexec command is fairly standard. Note that we are using it to launch singularity , which in turn will start up the containerized FastEddy executable. The singularity exec command line is complex, so let's deconstruct it here: We make use of the --bind argument first to mount familiar GLADE file systems within the container, and again to \"inject\" the host MPI into the container (as described here ). The /run directory necessity is not immediately obvious but is used by Cray-MPICH as part of the launching process. We also need to use the --env to set the LD_LIBRARY_PATH inside the image so that the application can find the proper host libraries. Recall when we built the FastEddy executable in the containerized environment it had no knowledge of these host-specific paths. Similarly, we use --env to set the LD_PRELOAD environment variable inside the container. This will cause a particular Cray-MPICH library to be loaded prior to application initialization. This step is not required for \"bare metal\" execution. We set some important Cray-MPICH specific MPICH_* environment variables as well to enable CUDA-awareness ( MPICH_GPU_* ) and work around an MPI run-time error ( MPICH_SMP_SINGLE_COPY_MODE ) that will otherwise appear. Finally, a note on the --bind /usr/lib64:/host/lib64 argument. Injecting the host MPI requires that some shared libraries from the host's /usr/lib64 directory be visible inside the image. However, this path also exists inside the image and contains other libraries needed by the application. We cannot simply bind the hosts directory into the same path, doing so will mask these other libraries. So we bind the host's /usr/lib64 into the container image at /host/lib64 , and make sure this path is set in the LD_LIBRARY_PATH variable as well. Because we want these particular host libraries found as last resort (not taking precedence over similar libraries in the container, we append /host/lib64 to the LD_LIBRARY_PATH search path. The arguments above were determined iteratively through trial and error. Such is the reality of containerized MPI applications and proprietary host MPI integration. Feel free to experiment with the PBS file, omitting some of the --bind and --env arguments and observing the resulting error message, however do NOT modify the MPICH_GPU_* variables , doing so may trigger a very unfortunate kernel driver bug and render the GPU compute nodes unusable. Pulling the image We begin with pulling the image from Docke Hub and constructing a SIF. (If you want to test your own built/pushed image, replace benjaminkirk with your own <dockerhub_username> as specified in the tag/push operations listed above.) derecho$ singularity pull rocky8-openhpc-fasteddy.sif docker://benjaminkirk/rocky8-openhpc-fasteddy:latest [...] derecho$ ls -lh rocky8-openhpc-fasteddy.sif -rwxr-xr-x 1 someuser ncar 3.1G Dec 5 17:08 rocky8-openhpc-fasteddy.sif Running the job derecho$ mkdir ./output derecho$ qsub -A <account> run_fasteddy_container.pbs derecho$ tail -f fasteddy_job.log","title":"Running the container on Derecho"},{"location":"environment-and-software/user-environment/containers/working_with_containers/","text":"The first step in running a container is usually (i) to \"pull\" a pre-existing container from an external repository, or (ii) to build a container according to a recipe. When a container is built locally, it is usually desirable to then share it with a larger community, often by \"pushing\" the resulting image to an external repository. We cover these topics generally in the next sections, which will be valuable both for users unfamiliar with containers and to others familiar with a tool such as Docker but unclear on how to execute these steps specifically in one of our supported runtimes. General container usage \u00b6 The concepts of pull, build, and push are common regardless of container run-time, however the specifics vary. We will highlight these specific steps for each supported run time in the examples below. Example scenario In the examples below we will work incrementally through a simple but realistic example use case: building a container using the latest version of a different operating system to provide tools not available on the host. Specifically, we will: Begin with a basic Rocky Linux 9 container image fetched from Docker Hub, Demonstrate building our own derived container image with additional packages and tools, and Demonstrate sharing the resulting image. Pulling a container \u00b6 Pulling & converting a simple container image Apptainer Charliecloud Podman Pulling & listing images We will use the command singularity pull from the apptainer module to pull our image and save it in Singularity Image Format (SIF): singularity pull casper$ singularity pull ./rocky9.sif docker://rockylinux/rockylinux:9 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 4031b0359885 done Copying config 175264fac6 done Writing manifest to image destination Storing signatures 2023/11/27 15:12:32 info unpack layer: sha256:4031b03598854f77c4ae1e53c2fdca86fdb41eb95f1f051416ce2e363fc8cdd2 INFO: Creating SIF file... Prefer Apptainer's SIF image format SIF images are much better suited for use on large parallel file systems than large directory trees, and can easily be shared with other users. Like most run times, Apptainer supports several image storage formats, including unpacked directory tree \"sandboxes\" and compressed read-only image bundles in Singularity Image Format (SIF). We recommend using read-only, compressed SIF images for both performance and best practices reasons. While sandboxes may be tempting to create \"writable\" containers, they create sprawling directory trees of many small files, which slow container startup time and complicate management especially on shared, parallel file systems. Furthermore, writable container images undercut the encapsulation and repeatability benefits offered by containerization. It is possible to run containers on top of SIF images with temporary write layers if necessary. Running a simple command from the container We cover running containers in much more detail here , however below we will use the command ch-run to inspect the contents of the file /etc/os-release inside the container : singularity exec casper$ singularity exec ./rocky9.sif cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"9.2 (Blue Onyx)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"9.2\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\" ANSI_COLOR=\"0;32\" LOGO=\"fedora-logo-icon\" CPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" SUPPORT_END=\"2032-05-31\" ROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\" ROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\" REDHAT_SUPPORT_PRODUCT=\"Rocky Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\" This is functionally a hello-world type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment. apptainer vs. singularity As of version 3, the commands apptainer and singularity are synonymous. We will use the latter as there is a wide array of existing documentation referencing the singularity executable across the internet. Pulling & listing images We will use the command ch-image from the charliecloud module to pull and list images: ch-image pull & ch-image list # Pull the requested image, storing into Charliecloud ' s internal format casper$ ch-image pull rockylinux/rockylinux:9 pulling image: rockylinux/rockylinux:9 requesting arch: amd64 manifest list: downloading: 100% manifest: downloading: 100% config: downloading: 100% layer 1/1: 4031b03: downloading: 63.6/63.6 MiB (100%) pulled image: adding to build cache flattening image layer 1/1: 4031b03: listing validating tarball members layer 1/1: 4031b03: changed 34 absolute symbolic and/or hard links to relative resolving whiteouts layer 1/1: 4031b03: extracting image arch: amd64 # List all known images casper$ ch-image list rockylinux/rockylinux:9 See ch-image --help for more details and options. Prefer Charliecloud's bundled SquashFUSE image format After running the two commands above, the requested container has been downloaded and unpacked into Charliecloud's storage directory tree in its native format. This often is on temporary storage, and it is advisable to use ch-convert to convert the image to one of several other image formats before use. Converting the image On NCAR's HPC systems we strive to support the squash SquashFS file system archive, which allows the container to be converted to a single, compressed file. This is much better suited for use on large parallel file systems, and can easily be shared with other users. The command ch-convert can be used to convert images between Charliecloud's supported formats . ch-convert # Convert from Charliecloud ' s internal format to a compressed SquashFUSE image casper$ ch-convert rockylinux/rockylinux:9 ./rocky9.sqfs Parallel mksquashfs: Using 72 processors Creating 4.0 filesystem on ./rocky9.sqfs, block size 65536. [=====================================================================|] 8075/8075 100% Exportable Squashfs 4.0 filesystem, gzip compressed, data block size 65536 compressed data, compressed metadata, compressed fragments, compressed xattrs, compressed ids duplicates are removed Filesystem size 61709.90 Kbytes (60.26 Mbytes) 35.85% of uncompressed filesystem size (172122.07 Kbytes) Inode table size 80336 bytes (78.45 Kbytes) 27.36% of uncompressed inode table size (293634 bytes) Directory table size 88626 bytes (86.55 Kbytes) 43.45% of uncompressed directory table size (203971 bytes) Number of duplicate files found 1875 Number of inodes 8122 Number of files 6194 Number of fragments 693 Number of symbolic links 909 Number of device nodes 0 Number of fifo nodes 0 Number of socket nodes 0 Number of directories 1019 Number of hard-links 0 Number of ids (unique uids + gids) 1 Number of uids 1 root (0) Number of gids 1 root (0) See ch-convert --help for more details and options. Running a simple command from the container We cover running containers in much more detail here , however below we will use the command ch-run to inspect the contents of the file /etc/os-release inside the container : ch-run casper$ ch-run ./rocky9.sqfs -- cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"9.2 (Blue Onyx)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"9.2\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\" ANSI_COLOR=\"0;32\" LOGO=\"fedora-logo-icon\" CPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" SUPPORT_END=\"2032-05-31\" ROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\" ROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\" REDHAT_SUPPORT_PRODUCT=\"Rocky Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\" This is functionally a hello-world type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment. Pulling & listing images We will use the command podman pull and podman images from the podman module to pull and list images: podman image pull & podman images # Pull the requested image, storing into Podman ' s internal image format casper$ podman image pull docker://rockylinux/rockylinux:9 Trying to pull docker.io/rockylinux/rockylinux:9... Getting image source signatures Copying blob 4031b0359885 done Copying config 175264fac6 done Writing manifest to image destination Storing signatures 175264fac6da4392fb2a9761583c81f509745629daee81de29beb7051f360db7 # list known ( downloaded ) images casper$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/rockylinux/rockylinux 9 175264fac6da 6 months ago 181 MB Running a simple command from the container We cover running containers in much more detail here , however below we will use the command ch-run to inspect the contents of the file /etc/os-release inside the container : podman run casper$ podman run rockylinux/rockylinux:9 cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"9.2 (Blue Onyx)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"9.2\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\" ANSI_COLOR=\"0;32\" LOGO=\"fedora-logo-icon\" CPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" SUPPORT_END=\"2032-05-31\" ROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\" ROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\" REDHAT_SUPPORT_PRODUCT=\"Rocky Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\" This is functionally a hello-world type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment. Podman vs. Docker Podman seeks to be functionally equivalent with Docker , so many Docker commands you may be familiar with will work the same. Building a container from a definition file \u00b6 In the examples above, we pulled a ready-made container image. For most practical applications we will want instead to build our own container image, often beginning with a base image from a public repository as shown above but extending it to meet a specific need. This process begins with a \"recipe\" file listing the steps required. By way of terminology, such recipes are typically referred to as Dockerfiles and usually follow a common format. Charliecloud and Podman support Dockerfiles directly. Apptainer is an outlier in this regard, and supports its own \"definition\" file format (commonly referred to as def -files). In this section we describe the general form of these build recipe files and provide simple build examples for the supported run-times. Anatomy of build recipes \u00b6 Dockerfiles and Apptainer Definition files Dockerfile Apptainer Definition files Following from the Docker documentation , a basic Dockerfile is Sample Dockerfile FROM rockylinux/rockylinux:9 RUN yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y groupinstall \"Development Tools\" \\ && dnf -y install \\ chrpath \\ bzip2 autoconf automake libtool \\ gcc gcc-c++ gcc-gfortran emacs make procps-ng openmpi-devel \\ && yum clean all RUN [...] ENV [...] Following from the Apptainer documentation , a basic definition file is Sample Definition File Bootstrap: docker From: docker.io/rockylinux/rockylinux:9 %post yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y install gimp \\ && dnf clean all --verbose %environment [...] We can now use the general form of these definition files to demonstrate constructing our own derived container image. Building a container from a recipe file Apptainer Charliecloud Podman Deffile: Bootstrap: docker From: docker.io/rockylinux/rockylinux:9 %post yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y install xpdf \\ && dnf clean all --verbose %environment export MY_CONTANER_VAR=\"foo\" We use the command singularity build to create a compressed SIF directly from the Deffile : casper$ TMPDIR=/var/tmp/ singularity build my_rocky9.sif Deffile [...] Dockerfile: FROM rockylinux/rockylinux:9 RUN yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y groupinstall \"Development Tools\" \\ && dnf -y install \\ chrpath \\ bzip2 autoconf automake libtool \\ gcc gcc-c++ gcc-gfortran emacs make procps-ng openmpi-devel \\ && dnf clean all --verbose We use the command ch-image build to build a container from the Dockerfile : casper$ ch-image build --force fakeroot --tag my_rocky9 . [...] Charliecloud builds in its internal format, which requires conversion before running. As shown above, we will convert the image to our preferred SquashFS format: casper$ ch-image list my_rocky9 rockylinux/rockylinux:9 benkirk@casper20(61)$ ch-convert my_rocky9 ./my_rocky9.sqfs input: ch-image my_rocky9 output: squash ./my_rocky9.sqfs packing ... [...] Dockerfile: FROM docker.io/rockylinux/rockylinux:9 RUN yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y install xpdf \\ && dnf clean all --verbose We use the command podman build to build a container from the Dockerfile : casper$ podman build --tag my_rocky9 . [...] Container image builds directly on the HPC systems can be fragile As discussed previously, security concerns in the HPC environment restrict certain container image build operations that require elevated privileges. Simple operations such as compiling code within a container to augment with a tool, or customizing the execution environment will likely work fine. Additionally, installing most packages through an operating system package manager usually works as well. A common failure, however, is building containers that switch user IDs or change ownership of files within the build process. This can occur explicitly through a USER statement or through installation of some package. In either case, the underlying issue is that on the host the user has access to only a single user ID - their own. Many complex containers violate this restriction. We cannot support such build processes securely, even with so-called \"rootless\" container installations. An alternative and popular workflow is to build containers externally to the HPC environment on a resource where the user has elevated privileges, likely using Docker. The finalized images is then pushed to an image repository, and then pulled into the HPC environment. We will not demonstrate the approach here due to the variability of external environments, however the process is straightforward for the user familiar with the build steps discussed below. Interaction with environment variables \u00b6 Different container run-times differ greatly in how they handle environment variables set externally on the host. Docker (and by similarity Podman) usually runs containers in a \"clean\" environment, that is, variables set on the host do not exist inside the container. Charliecloud by contrast passes nearly all environment variables from the host into the container. Apptainer takes a middle ground, by default passing along host environment variables, but this behavior can be changed with command-line arguments. The Docker/Podman approach makes sense when containerizing small services or pieces of code that you want to run identically everywhere, independent of the host environment. Conversely, the Apptainer & Charliecloud approach makes more sense when many useful variables are already defined in the environment. Regardless, at some point when working with containers you will undoubtedly find an issue that is traced either to a variable being inherited from the host when you don't want it to be, or missing in the container when you really want it. Each run-time allows for environment variables to be set explicitly from the run command line. Additionally, when building containers you can generally embed environment variables as well. Again, this behavior is run-time dependent as shown in the following example. Container run-times and environment variables Host Environment We set the following variables on the host system prior to launching the container: export HOST_VAR = \"foo\" export TOGGLE_VAR = \"set_from_OUTSIDE\" unset RANDOM_VAR Container Environment We then construct a tiny container image (<10MB) from the minimal Alpine Linux distribution. When we run the container it will report the value of several environment variables: HOST_VAR , CONTAINER_VAR , TOGGLE_VAR , and RANDOM_VAR (if set). Apptainer Charliecloud Podman The Apptainer Definition file %environment section allows us to define variables that exist inside the container. Definition File Bootstrap: docker From: docker.io/alpine:latest %files ./extras/speak.sh /opt/container/speak.sh ./extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh %post chmod +x /opt/container/*.sh %environment export CONTAINER_VAR=\"bar\" export TOGGLE_VAR=\"set_from_INSIDE\" Container Environment Examples Running the container shows that HOST_VAR is passed in by default, and TOGGLE_VAR retains its value from inside the container definition unless explicitly passed via the --env argument. The argument --cleanenv prevents external variables from being passed. # Step #1: host$ singularity run ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #2: host$ singularity run --cleanenv ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #3: host$ singularity run --cleanenv --env TOGGLE_VAR=set_from_OUTSIDE ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #4: host$ singularity run --cleanenv --env RANDOM_VAR=set_on_command-line ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE RANDOM_VAR=set_on_command-line ------------------------------ The Charliecloud Dockerfile ENV section allows us to define variables, but its use seems inconsistent in practice. Dockerfile FROM alpine:latest COPY extras/speak.sh /opt/container/speak.sh COPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh RUN chmod +x /opt/container/*.sh \\ && mkdir -p /glade/u/home /glade/work /glade/campaign /glade/derecho/scratch /random/path ENV CONTAINER_VAR=\"bar\" \\ TOGGLE_VAR=\"set_from_INSIDE\" Container Environment Examples Running the container shows that HOST_VAR is passed in by default, and TOGGLE_VAR takes its value from the host. The environment variables set from the ENV instruction are disregarded by default, but are honored when the --set-env arguments used. The argument --set-env=VAR=val allows variables to be passed to the container. The argument --unset-env can be passed to prevent certain specified host variables from passing into the container, with --unset-env=\"*\" preventing any. # Step #1: host$ ch-run ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR= TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #2: host$ ch-run --set-env ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #3: host$ ch-run '--unset-env=*' ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR= TOGGLE_VAR= ------------------------------ # Step #4: host$ ch-run --set-env=TOGGLE_VAR=set_from_OUTSIDE ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR= TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #5: host$ ch-run --set-env=RANDOM_VAR=set_on_command-line ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR= TOGGLE_VAR=set_from_OUTSIDE RANDOM_VAR=set_on_command-line ------------------------------ Discussion Charliecloud honors the ENV Dockerfile instruction at build-time but disregards it at run-time by default, the argument --set-env is required to regain access to these variables. The Podman Dockerfile ENV section allows us to define variables that exist inside the container. Dockerfile FROM docker.io/alpine:latest COPY extras/speak.sh /opt/container/speak.sh COPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh RUN chmod +x /opt/container/*.sh ENV CONTAINER_VAR=\"bar\" \\ TOGGLE_VAR=\"set_from_INSIDE\" Container Environment Examples Running the container shows that HOST_VAR is not passed in by default, and TOGGLE_VAR retains its value from inside the container definition unless explicitly passed via the --env argument. # Step #1: host$ podman run my_alpine:latest /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #2: host$ podman run --env TOGGLE_VAR=set_from_OUTSIDE my_alpine:latest /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #3: host$ podman run --env RANDOM_VAR=set_on_command-line my_alpine:latest /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE RANDOM_VAR=set_on_command-line ------------------------------ Summary Each container run-time allows you to pass environment variables in via command line arguments, and have different default behaviors with respect to host-defined variables. Some allow you to set default values that exist inside the container. Our best guidance is simply be aware of what is defined in your execution environment, pass critical values via command line arguments to avoid ambiguity, and perform error checking on environment variable values inside your image to be safe. Full definitions of the test cases can be found here . Accessing host file systems \u00b6 Similar to treatment of environment variables, each container run-time has unique behavior with respect to home and initial working directories inside containers. By default all provide minimal access to the host file systems, however they allow for host directories to be \"bind mounted\" into the container upon request. Following the same approach outlined above, we use our minimal container image to illustrate default and optional file system accessibility. Container run-times and mounting host file systems Apptainer Charliecloud Podman The singularity --bind option allows host directories to be bind-mounted into the container. As shown below, by default the users' home directory is mounted. The initial working directory ( PWD ) behavior depends on where the container is launched from, and if that path has been bind-mounted into the container or not (Steps 1 & 3). The initial directory can be set explicitly with the --pwd flag (Step 2). # Step #1: host$ singularity run ./my_alpine.sif /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/glade/u/home/benkirk glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk ------------------------------ # Step #2: host$ singularity run --pwd /opt/container ./my_alpine.sif /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/opt/container glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk ------------------------------ # Step #3: host$ singularity run --bind /glade/u/home/benkirk --bind /glade/derecho/scratch --bind /glade/work --bind /glade/campaign --bind /glade/work:/random/path ./my_alpine.sif /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/glade/work/benkirk/repos/csg-utils/hpc-demos/containers/tutorial/apptainer glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk 54.5P 10.7P 43.3P 20% /glade/derecho/scratch csfs1 4.0P 983.2T 3.0P 24% /glade/work csfs1 120.7P 93.5P 27.2P 77% /glade/campaign csfs1 4.0P 983.2T 3.0P 24% /random/path ------------------------------ The ch-run --bind option allows host directories to be bind-mounted into the container. Note that Charliecloud will not create new directories for the bind-mount locations, so we need to ensure they are created within our Dockerfile : Dockerfile FROM alpine:latest COPY extras/speak.sh /opt/container/speak.sh COPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh RUN chmod +x /opt/container/*.sh \\ && mkdir -p /glade/u/home /glade/work /glade/campaign /glade/derecho/scratch /random/path ENV CONTAINER_VAR=\"bar\" \\ TOGGLE_VAR=\"set_from_INSIDE\" As shown below, by default the users' home directory is mounted. The initial working directory ( PWD ) is the container file system root ( / ). In Step 2 we bind some of the common GLADE file systems into the container using the directories created in the Dockerfile . Step 3 fails, showing Charliecloud refusing to bind-mount into /random/other since no such mount point exists in the base image. # Step #1: host$ ch-run ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/ ------------------------------ # Step #2: host$ ch-run --bind=/glade/derecho/scratch --bind=/glade/work --bind=/glade/campaign --bind=/glade/work:/random/path ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/ 54.5P 10.7P 43.3P 20% /glade/derecho/scratch csfs1 4.0P 983.1T 3.0P 24% /glade/work csfs1 120.7P 93.5P 27.2P 77% /glade/campaign csfs1 4.0P 983.1T 3.0P 24% /random/path ------------------------------ # Step #3: host$ ch-run --bind=/glade/derecho/scratch --bind=/glade/work --bind=/glade/campaign --bind=/glade/work:/random/other/path ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh ch-run[73181]: error: can't mkdir: /var/tmp/benkirk.ch/mnt/random/other: Read-only file system (ch_misc.c:409 30) Discussion Charliecloud will not create destination paths for bind-mounts inside the container. Make sure you create such mount points when you build the image. The Podman --volume option allows host directories to be bind-mounted into the container. As shown below, by default the users' is placed into /root as a home directory, The initial working directory ( PWD ) is the container file system root ( / ). # Step #1: host$ podman run my_alpine:latest /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/root PWD=/ ------------------------------ # Step #2: host$ podman run --volume /glade/u/home/benkirk --volume /glade/derecho/scratch --volume /glade/work --volume /glade/campaign --volume /glade/work:/random/path my_alpine:latest /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/root PWD=/ mergedroot 188.3G 2.0G 186.3G 1% /glade/work csfs1 4.0P 983.1T 3.0P 24% /random/path mergedroot 188.3G 2.0G 186.3G 1% /glade/campaign mergedroot 188.3G 2.0G 186.3G 1% /glade/derecho/scratch mergedroot 188.3G 2.0G 186.3G 1% /glade/u/home/benkirk ------------------------------ Summary Each run-time has a method for accessing host directories within the container. They are all different with how they treat the users' home and initial working directory inside the container. Do not assume a given behavior, rather be explicit with changing directories and specifying full paths as necessary. Full definitions of the test cases can be found here . Running containerized MPI applications \u00b6 The interaction between MPI implementations and container run-times is unfortunately the single biggest pain point of running containers in the HPC environment. If you're drawn to the \"simplicity\" promised by containerization but need to run on multiple nodes with MPI, we strongly encourage you to fully consider this section before going further. (The issues are well described here .) First let us distinguish two use cases: Running MPI inside a container on a single node, using an MPI stack defined within the container : While somewhat limited, this case is fairly easy to handle since the complexities of interfacing with a high-speed network are largely eliminated. Running MPI across multiple nodes, launching the run-time with MPI from the host : This is the general case, and also where complications arise. This is the focus of the remainder of this section. When using MPI on the host to launch an MPI application that was compiled within the container, it is imperative the host and container MPI implementations be compatible. In practice this means the pair should be from the same implementation (e.g. OpenMPI, or MPICH) and at similar versions - and the closer the versions the better. This means the container image can rarely be created without knowledge of the execution host environment. For Casper, where we deploy OpenMPI by default, this is not too terribly complicated since most containerized operating systems can easily install similar versions. For Derecho, however, the default MPI is Cray's MPICH, which is proprietary and therefore difficult in general to install into an arbitrary container image. In this case we must choose particular versions of MPI for the container image, knowing in advance they share heritage with the target host system. This allows us to build MPI applications inside the container with a compatible - but readily available - MPI. We then follow the \"bind model\" approach when running the container in order to \"replace\" the container MPI with the host MPI, gaining access to the high-speed network and vendor MPI optimizations. For a full demonstration of this process, see our containerized FastEddy example . Running containerized GPU applications \u00b6 Many GPU compute capabilities are directly available within containers to applications. CUDA, OpenMP and OpenACC offload codes will generally work without any special consideration. An exception is interfacing directly with the kernel driver. If you require such functionality, Apptainer provides the easiest support path through its --nv command line argument. See the Apptainer GPU documentation page for more details. # running nvidia-smi from within the container, try 1: casper$ singularity exec --cleanenv ./my_cuda_container.sif nvidia-smi Failed to initialize NVML: Driver/library version mismatch # running nvidia-smi from within the container, try 2: casper$ singularity exec --nv --cleanenv ./my_cuda_container.sif nvidia-smi NVIDIA-SMI 545.23.06 Driver Version: 545.23.06 CUDA Version: 12.3 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla V100-SXM2-32GB On | 00000000:89:00.0 Off | 0 | | N/A 30C P0 39W / 300W | 0MiB / 32768MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ [...] Sharing a container \u00b6 Under Development The best practices for sharing container images is currently under development. The availability and pricing models of external repositories is frequently changing, complicating a general recommendation. NCAR does not currently offer a custom centralized image repository for HPC user access. For sharing on the HPC systems, we currently recommend revision-controlled build processes and sharing resulting compressed imaged files directly. Alternatively, a popular model for external container building is to push the resulting images to Docker Hub, where they can be pulled into the HPC environment using the techniques outlined above.","title":"Working with Containers"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#general-container-usage","text":"The concepts of pull, build, and push are common regardless of container run-time, however the specifics vary. We will highlight these specific steps for each supported run time in the examples below. Example scenario In the examples below we will work incrementally through a simple but realistic example use case: building a container using the latest version of a different operating system to provide tools not available on the host. Specifically, we will: Begin with a basic Rocky Linux 9 container image fetched from Docker Hub, Demonstrate building our own derived container image with additional packages and tools, and Demonstrate sharing the resulting image.","title":"General container usage"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#pulling-a-container","text":"Pulling & converting a simple container image Apptainer Charliecloud Podman Pulling & listing images We will use the command singularity pull from the apptainer module to pull our image and save it in Singularity Image Format (SIF): singularity pull casper$ singularity pull ./rocky9.sif docker://rockylinux/rockylinux:9 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 4031b0359885 done Copying config 175264fac6 done Writing manifest to image destination Storing signatures 2023/11/27 15:12:32 info unpack layer: sha256:4031b03598854f77c4ae1e53c2fdca86fdb41eb95f1f051416ce2e363fc8cdd2 INFO: Creating SIF file... Prefer Apptainer's SIF image format SIF images are much better suited for use on large parallel file systems than large directory trees, and can easily be shared with other users. Like most run times, Apptainer supports several image storage formats, including unpacked directory tree \"sandboxes\" and compressed read-only image bundles in Singularity Image Format (SIF). We recommend using read-only, compressed SIF images for both performance and best practices reasons. While sandboxes may be tempting to create \"writable\" containers, they create sprawling directory trees of many small files, which slow container startup time and complicate management especially on shared, parallel file systems. Furthermore, writable container images undercut the encapsulation and repeatability benefits offered by containerization. It is possible to run containers on top of SIF images with temporary write layers if necessary. Running a simple command from the container We cover running containers in much more detail here , however below we will use the command ch-run to inspect the contents of the file /etc/os-release inside the container : singularity exec casper$ singularity exec ./rocky9.sif cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"9.2 (Blue Onyx)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"9.2\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\" ANSI_COLOR=\"0;32\" LOGO=\"fedora-logo-icon\" CPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" SUPPORT_END=\"2032-05-31\" ROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\" ROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\" REDHAT_SUPPORT_PRODUCT=\"Rocky Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\" This is functionally a hello-world type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment. apptainer vs. singularity As of version 3, the commands apptainer and singularity are synonymous. We will use the latter as there is a wide array of existing documentation referencing the singularity executable across the internet. Pulling & listing images We will use the command ch-image from the charliecloud module to pull and list images: ch-image pull & ch-image list # Pull the requested image, storing into Charliecloud ' s internal format casper$ ch-image pull rockylinux/rockylinux:9 pulling image: rockylinux/rockylinux:9 requesting arch: amd64 manifest list: downloading: 100% manifest: downloading: 100% config: downloading: 100% layer 1/1: 4031b03: downloading: 63.6/63.6 MiB (100%) pulled image: adding to build cache flattening image layer 1/1: 4031b03: listing validating tarball members layer 1/1: 4031b03: changed 34 absolute symbolic and/or hard links to relative resolving whiteouts layer 1/1: 4031b03: extracting image arch: amd64 # List all known images casper$ ch-image list rockylinux/rockylinux:9 See ch-image --help for more details and options. Prefer Charliecloud's bundled SquashFUSE image format After running the two commands above, the requested container has been downloaded and unpacked into Charliecloud's storage directory tree in its native format. This often is on temporary storage, and it is advisable to use ch-convert to convert the image to one of several other image formats before use. Converting the image On NCAR's HPC systems we strive to support the squash SquashFS file system archive, which allows the container to be converted to a single, compressed file. This is much better suited for use on large parallel file systems, and can easily be shared with other users. The command ch-convert can be used to convert images between Charliecloud's supported formats . ch-convert # Convert from Charliecloud ' s internal format to a compressed SquashFUSE image casper$ ch-convert rockylinux/rockylinux:9 ./rocky9.sqfs Parallel mksquashfs: Using 72 processors Creating 4.0 filesystem on ./rocky9.sqfs, block size 65536. [=====================================================================|] 8075/8075 100% Exportable Squashfs 4.0 filesystem, gzip compressed, data block size 65536 compressed data, compressed metadata, compressed fragments, compressed xattrs, compressed ids duplicates are removed Filesystem size 61709.90 Kbytes (60.26 Mbytes) 35.85% of uncompressed filesystem size (172122.07 Kbytes) Inode table size 80336 bytes (78.45 Kbytes) 27.36% of uncompressed inode table size (293634 bytes) Directory table size 88626 bytes (86.55 Kbytes) 43.45% of uncompressed directory table size (203971 bytes) Number of duplicate files found 1875 Number of inodes 8122 Number of files 6194 Number of fragments 693 Number of symbolic links 909 Number of device nodes 0 Number of fifo nodes 0 Number of socket nodes 0 Number of directories 1019 Number of hard-links 0 Number of ids (unique uids + gids) 1 Number of uids 1 root (0) Number of gids 1 root (0) See ch-convert --help for more details and options. Running a simple command from the container We cover running containers in much more detail here , however below we will use the command ch-run to inspect the contents of the file /etc/os-release inside the container : ch-run casper$ ch-run ./rocky9.sqfs -- cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"9.2 (Blue Onyx)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"9.2\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\" ANSI_COLOR=\"0;32\" LOGO=\"fedora-logo-icon\" CPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" SUPPORT_END=\"2032-05-31\" ROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\" ROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\" REDHAT_SUPPORT_PRODUCT=\"Rocky Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\" This is functionally a hello-world type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment. Pulling & listing images We will use the command podman pull and podman images from the podman module to pull and list images: podman image pull & podman images # Pull the requested image, storing into Podman ' s internal image format casper$ podman image pull docker://rockylinux/rockylinux:9 Trying to pull docker.io/rockylinux/rockylinux:9... Getting image source signatures Copying blob 4031b0359885 done Copying config 175264fac6 done Writing manifest to image destination Storing signatures 175264fac6da4392fb2a9761583c81f509745629daee81de29beb7051f360db7 # list known ( downloaded ) images casper$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/rockylinux/rockylinux 9 175264fac6da 6 months ago 181 MB Running a simple command from the container We cover running containers in much more detail here , however below we will use the command ch-run to inspect the contents of the file /etc/os-release inside the container : podman run casper$ podman run rockylinux/rockylinux:9 cat /etc/os-release NAME=\"Rocky Linux\" VERSION=\"9.2 (Blue Onyx)\" ID=\"rocky\" ID_LIKE=\"rhel centos fedora\" VERSION_ID=\"9.2\" PLATFORM_ID=\"platform:el9\" PRETTY_NAME=\"Rocky Linux 9.2 (Blue Onyx)\" ANSI_COLOR=\"0;32\" LOGO=\"fedora-logo-icon\" CPE_NAME=\"cpe:/o:rocky:rocky:9::baseos\" HOME_URL=\"https://rockylinux.org/\" BUG_REPORT_URL=\"https://bugs.rockylinux.org/\" SUPPORT_END=\"2032-05-31\" ROCKY_SUPPORT_PRODUCT=\"Rocky-Linux-9\" ROCKY_SUPPORT_PRODUCT_VERSION=\"9.2\" REDHAT_SUPPORT_PRODUCT=\"Rocky Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"9.2\" This is functionally a hello-world type demonstration, and can be compared to the same file on the host to show we are indeed running in a different environment. Podman vs. Docker Podman seeks to be functionally equivalent with Docker , so many Docker commands you may be familiar with will work the same.","title":"Pulling a container"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#building-a-container-from-a-definition-file","text":"In the examples above, we pulled a ready-made container image. For most practical applications we will want instead to build our own container image, often beginning with a base image from a public repository as shown above but extending it to meet a specific need. This process begins with a \"recipe\" file listing the steps required. By way of terminology, such recipes are typically referred to as Dockerfiles and usually follow a common format. Charliecloud and Podman support Dockerfiles directly. Apptainer is an outlier in this regard, and supports its own \"definition\" file format (commonly referred to as def -files). In this section we describe the general form of these build recipe files and provide simple build examples for the supported run-times.","title":"Building a container from a definition file"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#anatomy-of-build-recipes","text":"Dockerfiles and Apptainer Definition files Dockerfile Apptainer Definition files Following from the Docker documentation , a basic Dockerfile is Sample Dockerfile FROM rockylinux/rockylinux:9 RUN yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y groupinstall \"Development Tools\" \\ && dnf -y install \\ chrpath \\ bzip2 autoconf automake libtool \\ gcc gcc-c++ gcc-gfortran emacs make procps-ng openmpi-devel \\ && yum clean all RUN [...] ENV [...] Following from the Apptainer documentation , a basic definition file is Sample Definition File Bootstrap: docker From: docker.io/rockylinux/rockylinux:9 %post yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y install gimp \\ && dnf clean all --verbose %environment [...] We can now use the general form of these definition files to demonstrate constructing our own derived container image. Building a container from a recipe file Apptainer Charliecloud Podman Deffile: Bootstrap: docker From: docker.io/rockylinux/rockylinux:9 %post yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y install xpdf \\ && dnf clean all --verbose %environment export MY_CONTANER_VAR=\"foo\" We use the command singularity build to create a compressed SIF directly from the Deffile : casper$ TMPDIR=/var/tmp/ singularity build my_rocky9.sif Deffile [...] Dockerfile: FROM rockylinux/rockylinux:9 RUN yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y groupinstall \"Development Tools\" \\ && dnf -y install \\ chrpath \\ bzip2 autoconf automake libtool \\ gcc gcc-c++ gcc-gfortran emacs make procps-ng openmpi-devel \\ && dnf clean all --verbose We use the command ch-image build to build a container from the Dockerfile : casper$ ch-image build --force fakeroot --tag my_rocky9 . [...] Charliecloud builds in its internal format, which requires conversion before running. As shown above, we will convert the image to our preferred SquashFS format: casper$ ch-image list my_rocky9 rockylinux/rockylinux:9 benkirk@casper20(61)$ ch-convert my_rocky9 ./my_rocky9.sqfs input: ch-image my_rocky9 output: squash ./my_rocky9.sqfs packing ... [...] Dockerfile: FROM docker.io/rockylinux/rockylinux:9 RUN yum -y install dnf-plugins-core \\ && dnf -y update \\ && dnf config-manager --set-enabled crb \\ && dnf -y install epel-release \\ && dnf -y install xpdf \\ && dnf clean all --verbose We use the command podman build to build a container from the Dockerfile : casper$ podman build --tag my_rocky9 . [...] Container image builds directly on the HPC systems can be fragile As discussed previously, security concerns in the HPC environment restrict certain container image build operations that require elevated privileges. Simple operations such as compiling code within a container to augment with a tool, or customizing the execution environment will likely work fine. Additionally, installing most packages through an operating system package manager usually works as well. A common failure, however, is building containers that switch user IDs or change ownership of files within the build process. This can occur explicitly through a USER statement or through installation of some package. In either case, the underlying issue is that on the host the user has access to only a single user ID - their own. Many complex containers violate this restriction. We cannot support such build processes securely, even with so-called \"rootless\" container installations. An alternative and popular workflow is to build containers externally to the HPC environment on a resource where the user has elevated privileges, likely using Docker. The finalized images is then pushed to an image repository, and then pulled into the HPC environment. We will not demonstrate the approach here due to the variability of external environments, however the process is straightforward for the user familiar with the build steps discussed below.","title":"Anatomy of build recipes"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#interaction-with-environment-variables","text":"Different container run-times differ greatly in how they handle environment variables set externally on the host. Docker (and by similarity Podman) usually runs containers in a \"clean\" environment, that is, variables set on the host do not exist inside the container. Charliecloud by contrast passes nearly all environment variables from the host into the container. Apptainer takes a middle ground, by default passing along host environment variables, but this behavior can be changed with command-line arguments. The Docker/Podman approach makes sense when containerizing small services or pieces of code that you want to run identically everywhere, independent of the host environment. Conversely, the Apptainer & Charliecloud approach makes more sense when many useful variables are already defined in the environment. Regardless, at some point when working with containers you will undoubtedly find an issue that is traced either to a variable being inherited from the host when you don't want it to be, or missing in the container when you really want it. Each run-time allows for environment variables to be set explicitly from the run command line. Additionally, when building containers you can generally embed environment variables as well. Again, this behavior is run-time dependent as shown in the following example. Container run-times and environment variables Host Environment We set the following variables on the host system prior to launching the container: export HOST_VAR = \"foo\" export TOGGLE_VAR = \"set_from_OUTSIDE\" unset RANDOM_VAR Container Environment We then construct a tiny container image (<10MB) from the minimal Alpine Linux distribution. When we run the container it will report the value of several environment variables: HOST_VAR , CONTAINER_VAR , TOGGLE_VAR , and RANDOM_VAR (if set). Apptainer Charliecloud Podman The Apptainer Definition file %environment section allows us to define variables that exist inside the container. Definition File Bootstrap: docker From: docker.io/alpine:latest %files ./extras/speak.sh /opt/container/speak.sh ./extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh %post chmod +x /opt/container/*.sh %environment export CONTAINER_VAR=\"bar\" export TOGGLE_VAR=\"set_from_INSIDE\" Container Environment Examples Running the container shows that HOST_VAR is passed in by default, and TOGGLE_VAR retains its value from inside the container definition unless explicitly passed via the --env argument. The argument --cleanenv prevents external variables from being passed. # Step #1: host$ singularity run ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #2: host$ singularity run --cleanenv ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #3: host$ singularity run --cleanenv --env TOGGLE_VAR=set_from_OUTSIDE ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #4: host$ singularity run --cleanenv --env RANDOM_VAR=set_on_command-line ./my_alpine.sif /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE RANDOM_VAR=set_on_command-line ------------------------------ The Charliecloud Dockerfile ENV section allows us to define variables, but its use seems inconsistent in practice. Dockerfile FROM alpine:latest COPY extras/speak.sh /opt/container/speak.sh COPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh RUN chmod +x /opt/container/*.sh \\ && mkdir -p /glade/u/home /glade/work /glade/campaign /glade/derecho/scratch /random/path ENV CONTAINER_VAR=\"bar\" \\ TOGGLE_VAR=\"set_from_INSIDE\" Container Environment Examples Running the container shows that HOST_VAR is passed in by default, and TOGGLE_VAR takes its value from the host. The environment variables set from the ENV instruction are disregarded by default, but are honored when the --set-env arguments used. The argument --set-env=VAR=val allows variables to be passed to the container. The argument --unset-env can be passed to prevent certain specified host variables from passing into the container, with --unset-env=\"*\" preventing any. # Step #1: host$ ch-run ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR= TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #2: host$ ch-run --set-env ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #3: host$ ch-run '--unset-env=*' ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR= TOGGLE_VAR= ------------------------------ # Step #4: host$ ch-run --set-env=TOGGLE_VAR=set_from_OUTSIDE ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR= TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #5: host$ ch-run --set-env=RANDOM_VAR=set_on_command-line ./my_alpine.sqfs -- /opt/container/speak.sh ------------------------------ HOST_VAR=foo CONTAINER_VAR= TOGGLE_VAR=set_from_OUTSIDE RANDOM_VAR=set_on_command-line ------------------------------ Discussion Charliecloud honors the ENV Dockerfile instruction at build-time but disregards it at run-time by default, the argument --set-env is required to regain access to these variables. The Podman Dockerfile ENV section allows us to define variables that exist inside the container. Dockerfile FROM docker.io/alpine:latest COPY extras/speak.sh /opt/container/speak.sh COPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh RUN chmod +x /opt/container/*.sh ENV CONTAINER_VAR=\"bar\" \\ TOGGLE_VAR=\"set_from_INSIDE\" Container Environment Examples Running the container shows that HOST_VAR is not passed in by default, and TOGGLE_VAR retains its value from inside the container definition unless explicitly passed via the --env argument. # Step #1: host$ podman run my_alpine:latest /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE ------------------------------ # Step #2: host$ podman run --env TOGGLE_VAR=set_from_OUTSIDE my_alpine:latest /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_OUTSIDE ------------------------------ # Step #3: host$ podman run --env RANDOM_VAR=set_on_command-line my_alpine:latest /opt/container/speak.sh ------------------------------ HOST_VAR= CONTAINER_VAR=bar TOGGLE_VAR=set_from_INSIDE RANDOM_VAR=set_on_command-line ------------------------------ Summary Each container run-time allows you to pass environment variables in via command line arguments, and have different default behaviors with respect to host-defined variables. Some allow you to set default values that exist inside the container. Our best guidance is simply be aware of what is defined in your execution environment, pass critical values via command line arguments to avoid ambiguity, and perform error checking on environment variable values inside your image to be safe. Full definitions of the test cases can be found here .","title":"Interaction with environment variables"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#accessing-host-file-systems","text":"Similar to treatment of environment variables, each container run-time has unique behavior with respect to home and initial working directories inside containers. By default all provide minimal access to the host file systems, however they allow for host directories to be \"bind mounted\" into the container upon request. Following the same approach outlined above, we use our minimal container image to illustrate default and optional file system accessibility. Container run-times and mounting host file systems Apptainer Charliecloud Podman The singularity --bind option allows host directories to be bind-mounted into the container. As shown below, by default the users' home directory is mounted. The initial working directory ( PWD ) behavior depends on where the container is launched from, and if that path has been bind-mounted into the container or not (Steps 1 & 3). The initial directory can be set explicitly with the --pwd flag (Step 2). # Step #1: host$ singularity run ./my_alpine.sif /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/glade/u/home/benkirk glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk ------------------------------ # Step #2: host$ singularity run --pwd /opt/container ./my_alpine.sif /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/opt/container glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk ------------------------------ # Step #3: host$ singularity run --bind /glade/u/home/benkirk --bind /glade/derecho/scratch --bind /glade/work --bind /glade/campaign --bind /glade/work:/random/path ./my_alpine.sif /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/glade/work/benkirk/repos/csg-utils/hpc-demos/containers/tutorial/apptainer glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk glade_user 150.0T 77.7T 72.3T 52% /glade/u/home/benkirk 54.5P 10.7P 43.3P 20% /glade/derecho/scratch csfs1 4.0P 983.2T 3.0P 24% /glade/work csfs1 120.7P 93.5P 27.2P 77% /glade/campaign csfs1 4.0P 983.2T 3.0P 24% /random/path ------------------------------ The ch-run --bind option allows host directories to be bind-mounted into the container. Note that Charliecloud will not create new directories for the bind-mount locations, so we need to ensure they are created within our Dockerfile : Dockerfile FROM alpine:latest COPY extras/speak.sh /opt/container/speak.sh COPY extras/list_glade_filesystems.sh /opt/container/list_glade_filesystems.sh RUN chmod +x /opt/container/*.sh \\ && mkdir -p /glade/u/home /glade/work /glade/campaign /glade/derecho/scratch /random/path ENV CONTAINER_VAR=\"bar\" \\ TOGGLE_VAR=\"set_from_INSIDE\" As shown below, by default the users' home directory is mounted. The initial working directory ( PWD ) is the container file system root ( / ). In Step 2 we bind some of the common GLADE file systems into the container using the directories created in the Dockerfile . Step 3 fails, showing Charliecloud refusing to bind-mount into /random/other since no such mount point exists in the base image. # Step #1: host$ ch-run ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/ ------------------------------ # Step #2: host$ ch-run --bind=/glade/derecho/scratch --bind=/glade/work --bind=/glade/campaign --bind=/glade/work:/random/path ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/glade/u/home/benkirk PWD=/ 54.5P 10.7P 43.3P 20% /glade/derecho/scratch csfs1 4.0P 983.1T 3.0P 24% /glade/work csfs1 120.7P 93.5P 27.2P 77% /glade/campaign csfs1 4.0P 983.1T 3.0P 24% /random/path ------------------------------ # Step #3: host$ ch-run --bind=/glade/derecho/scratch --bind=/glade/work --bind=/glade/campaign --bind=/glade/work:/random/other/path ./my_alpine.sqfs -- /opt/container/list_glade_filesystems.sh ch-run[73181]: error: can't mkdir: /var/tmp/benkirk.ch/mnt/random/other: Read-only file system (ch_misc.c:409 30) Discussion Charliecloud will not create destination paths for bind-mounts inside the container. Make sure you create such mount points when you build the image. The Podman --volume option allows host directories to be bind-mounted into the container. As shown below, by default the users' is placed into /root as a home directory, The initial working directory ( PWD ) is the container file system root ( / ). # Step #1: host$ podman run my_alpine:latest /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/root PWD=/ ------------------------------ # Step #2: host$ podman run --volume /glade/u/home/benkirk --volume /glade/derecho/scratch --volume /glade/work --volume /glade/campaign --volume /glade/work:/random/path my_alpine:latest /opt/container/list_glade_filesystems.sh ------------------------------ HOME=/root PWD=/ mergedroot 188.3G 2.0G 186.3G 1% /glade/work csfs1 4.0P 983.1T 3.0P 24% /random/path mergedroot 188.3G 2.0G 186.3G 1% /glade/campaign mergedroot 188.3G 2.0G 186.3G 1% /glade/derecho/scratch mergedroot 188.3G 2.0G 186.3G 1% /glade/u/home/benkirk ------------------------------ Summary Each run-time has a method for accessing host directories within the container. They are all different with how they treat the users' home and initial working directory inside the container. Do not assume a given behavior, rather be explicit with changing directories and specifying full paths as necessary. Full definitions of the test cases can be found here .","title":"Accessing host file systems"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#running-containerized-mpi-applications","text":"The interaction between MPI implementations and container run-times is unfortunately the single biggest pain point of running containers in the HPC environment. If you're drawn to the \"simplicity\" promised by containerization but need to run on multiple nodes with MPI, we strongly encourage you to fully consider this section before going further. (The issues are well described here .) First let us distinguish two use cases: Running MPI inside a container on a single node, using an MPI stack defined within the container : While somewhat limited, this case is fairly easy to handle since the complexities of interfacing with a high-speed network are largely eliminated. Running MPI across multiple nodes, launching the run-time with MPI from the host : This is the general case, and also where complications arise. This is the focus of the remainder of this section. When using MPI on the host to launch an MPI application that was compiled within the container, it is imperative the host and container MPI implementations be compatible. In practice this means the pair should be from the same implementation (e.g. OpenMPI, or MPICH) and at similar versions - and the closer the versions the better. This means the container image can rarely be created without knowledge of the execution host environment. For Casper, where we deploy OpenMPI by default, this is not too terribly complicated since most containerized operating systems can easily install similar versions. For Derecho, however, the default MPI is Cray's MPICH, which is proprietary and therefore difficult in general to install into an arbitrary container image. In this case we must choose particular versions of MPI for the container image, knowing in advance they share heritage with the target host system. This allows us to build MPI applications inside the container with a compatible - but readily available - MPI. We then follow the \"bind model\" approach when running the container in order to \"replace\" the container MPI with the host MPI, gaining access to the high-speed network and vendor MPI optimizations. For a full demonstration of this process, see our containerized FastEddy example .","title":"Running containerized MPI applications"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#running-containerized-gpu-applications","text":"Many GPU compute capabilities are directly available within containers to applications. CUDA, OpenMP and OpenACC offload codes will generally work without any special consideration. An exception is interfacing directly with the kernel driver. If you require such functionality, Apptainer provides the easiest support path through its --nv command line argument. See the Apptainer GPU documentation page for more details. # running nvidia-smi from within the container, try 1: casper$ singularity exec --cleanenv ./my_cuda_container.sif nvidia-smi Failed to initialize NVML: Driver/library version mismatch # running nvidia-smi from within the container, try 2: casper$ singularity exec --nv --cleanenv ./my_cuda_container.sif nvidia-smi NVIDIA-SMI 545.23.06 Driver Version: 545.23.06 CUDA Version: 12.3 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla V100-SXM2-32GB On | 00000000:89:00.0 Off | 0 | | N/A 30C P0 39W / 300W | 0MiB / 32768MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ [...]","title":"Running containerized GPU applications"},{"location":"environment-and-software/user-environment/containers/working_with_containers/#sharing-a-container","text":"Under Development The best practices for sharing container images is currently under development. The availability and pricing models of external repositories is frequently changing, complicating a general recommendation. NCAR does not currently offer a custom centralized image repository for HPC user access. For sharing on the HPC systems, we currently recommend revision-controlled build processes and sharing resulting compressed imaged files directly. Alternatively, a popular model for external container building is to push the resulting images to Docker Hub, where they can be pulled into the HPC environment using the techniques outlined above.","title":"Sharing a container"},{"location":"environment-and-software/user-environment/spack/","text":"","title":"Index"},{"location":"environment-and-software/user-environment/spack/spack-downstream/","text":"","title":"Spack Downstream"},{"location":"environment-and-software/user-environment/spack/spack/","text":"","title":"Spack"},{"location":"esds/faq/","text":"Frequently Asked Questions \u00b6 A copy of ESDS FAQ page: https://ncar.github.io/esds/faq/ This contains relevant questions and answers from common workflow issues and questions posted on Zulip. Note This page is meant to be a list of FAQ regarding climate datasets, movivated by a variety of employees across UCAR/NCAR. I need help with this! \u00b6 Where do I go for help? \u00b6 Try one of the following resources. Xarray's How Do I do X? page Xarray Github Discussions Pangeo Discourse Forum NCAR Zulip under #python-questions, #python-dev, or #dask. Avoid personal emails and prefer a public forum. What do I do if my question is not answered on this page? \u00b6 If your question is related to conda environments and you're affiliated with UCAR/NCAR, you can open a help ticket on the NCAR Research Computing Helpdesk site . If your issue is related to data science packages and workflows, you can open an issue on our GitHub here or book an office hour appointment with an ESDS core member! Someone must have written the function I want. Where do I look? \u00b6 See the xarray ecosystem page. Also see the xarray-contrib and pangeo-data organizations. Some NCAR relevant projects include: GeoCAT-comp GeoCAT-viz cf_xarray climpred eofs MetPy rechunker xclim xesmf xgcm pop-tools xskillscore How do I use conda environments? \u00b6 General Advice \u00b6 Dealing with Python environments can be tricky... a good place to start is to checkout this guide on dealing with Python environments . If you just need a refresher on the various conda commands, this conda cheet sheet is a wonderful quick reference. Using conda on NCAR HPC resources \u00b6 Warning Since 12 December 2022, it is no longer recommended to install your own version of miniconda on the HPC system. To export your existing environments to the recommended installation of miniconda, refer to the \"How can I export my environments?\" section. The NCAR High Performance Computing (HPC) system has a conda installation for you to use. The most recent and detailed instructions can be found on this Using Conda and Python page. If you don't want the trouble of making your own conda environment, there are managed environments available. The NCAR Package Library (NPL) is an environment containing many common scientific Python packages such as Numpy, Xarray, and GeoCAT. You can access the NPL environment through the command line and the NCAR JupyterHub. NPL on the command line \u00b6 Open up a terminal in Casper or Cheyenne Load the NCAR conda module: $ module load conda/latest List the available NCAR managed environments: $ conda env list base * /glade/u/apps/opt/conda npl /glade/u/apps/opt/conda/envs/npl npl-2022b /glade/u/apps/opt/conda/envs/npl-2022b npl-2206 /glade/u/apps/opt/conda/envs/npl-2206 npl-2207 /glade/u/apps/opt/conda/envs/npl-2207 pygpu-dask /glade/u/apps/opt/conda/envs/pygpu-dask Activate the environment you want to use. Here we are using the npl environment as an example. npl can be replaced with any available environment name: $ conda activate npl Now when you run a script, the modules within the npl environment will be available to your program. NPL on the NCAR JupyterHub \u00b6 Log in to the Production NCAR JupyterHub Start a server With your Jupyter Notebook open, click on the kernel name in the upper right. A dialog will appear with all the various kernels available to you. These kernels will (generally) have the same name as the conda environment that it uses. This may not be the case if you are managing your own environments and kernels. Select the \"npl (conda)\" kernel from the list if you want to use the NCAR-managed NPL environment. Creating and accessing a new conda environment on the NCAR JupyterHub \u00b6 You may want to move past using NPL, and create a new conda environment! For detailed instructions, check out the Using Conda and Python page on the NCAR Advanced Research Computing site. Heres a summary of the basic steps: Create the environment If you are creating an environment from scratch, use the following: conda create --name my_environment where my_environment is the name of your environment Ff you have an environment file (ex. environment.yml ), use the following: conda env create -f environment.yml Activate your environment and install the ipykernel package conda activate my_environment.yml conda install ipykernel The [`ipykernel`](https://github.com/ipython/ipykernel) package is required for your environment to be available from the NCAR [JupyterHub](https://jupyterhub.hpc.ucar.edu/) Accessing your conda environment Your environment should now automatically show up as an available kernel in any Jupyter server on the NCAR HPC systems. If you want to give your kernel a name that is different from the environment name, you can use the following command: python -m ipykernel install --user --name = my-kernel Where my-kernel is the kernel name. Conda is taking too long to solve environment: use mamba \u00b6 This is a very common issue when installing a new package or trying to update a package in an existing conda environment. This issue is usually manifested in a conda message along these lines: environment Solving environment: failed with initial frozen solve. Retrying with flexible solve. One solution to this issue is to use mamba which is a drop-in replacement for conda. Mamba aims to greately speed up and improve conda functionality such as solving environment, installing packages, etc... Installing Mamba conda install -n base -c conda-forge mamba Set conda-forge and nodefaults channels conda config --add channels nodefaults conda config --add channels conda-forge To install a package with mamba, you just run mamba install package_name To create/update an environment from an environment file, run: mamba env update -f environment.yml We do not recommend using `mamba` to activate and deactivate environments as this can cause packages to misbehave/not load correctly. See mamba documentation for more. How can I export my environments? \u00b6 If you made an environment on one machine or using a different conda installation, you can export that environment and use it elsewhere. These are the basic steps: Export your environment With the environment you want to export activated, run the following command: conda env export --from-history > environment.yml where environment can be replaced with the file name of your choice. The --from-history flag allows you to recreate your environment on any system. It is the cross-platform compatible way of exporting an environment. Move the environment.yml to the system you want to use it on / activate the appropriate conda installation you wish to use. Use the .yml file to create your environment conda env create -f environment.yml Xarray and Dask \u00b6 General tips \u00b6 Read the xarray documentation on optimizing workflow with dask . Read the Best practices for dask array Keep track of chunk sizes throughout your workflow. This is especially important when reading in data using xr.open_mfdataset . Aim for 100-200MB size chunks. Choose chunking appropriate to your analysis. If you're working with time series then chunk more in space and less along time. Avoid indexing with .where as much as possible. In particulate .where(..., drop=True) will trigger a compute since it needs to know where NaNs are present to drop them. Instead see if you can write your statement as a .clip , .sel , .isel , or .query statement. How do I optimize reading multiple files using Xarray and Dask? \u00b6 A good first place to start when reading in multiple files is Xarray's multi-file documentation . For example, if you are trying to read in multiple files where you are interested in concatenating over the time dimension, here is an example of the xr.open_dataset line would look like: ds = xr . open_mfdataset ( files , # Name of the dimension to concatenate along. concat_dim = \"time\" , # Attempt to auto-magically combine the given datasets into one by using dimension coordinates. combine = \"by_coords\" , # Specify chunks for dask - explained later chunks = { \"lev\" : 1 , \"time\" : 500 }, # Only data variables in which the dimension already appears are included. data_vars = \"minimal\" , # Only coordinates in which the dimension already appears are included. coords = \"minimal\" , # Skip comparing and pick variable from first dataset. compat = \"override\" , parallel = True , ) Where can I find Xarray tutorials? \u00b6 See videos and notebooks . How do I debug my code when using dask? \u00b6 An option is to use .compute(scheduler=\"single-threaded\") . This will run your code as a serial for loop. When an error is raised you can use the %debug magic to drop in to the stack and debug from there. See this post for more debugging tips in a serial context. KilledWorker X{. What do I do? \u00b6 Keep an eye on the dask dashboard. If a lot of the bars in the Memory tab are orange, that means your workers are running out of memory. Reduce your chunk size. Help, my analysis is slow! \u00b6 Try subsetting for just the variable(s) you need for example, if you are reading in a dataset with ~25 variables, and you only need temperature , just read in temperature. You can specify which variables to read in by using the following syntax, following the example of the temperature variable. ds = xr . open_dataset ( file , data_vars = [ 'temperature' ]) Take a look at your chunk size, it might not be optimized. When reading a file in using Xarray with Dask, a \"general rule of thumb\" is to keep your chunk size down to around 100 mb. For example, let's say you trying to read in multiple files, each with ~600 time steps. This is case where each file is very large (several 10s of GB) and using Dask to help with data processing is essential . You can check the size of each chunk by subsetting a single DataArray (ex. ds['temperature'] ) If you have very large chunks, try modifying the number of chunks you specify within xr.open_mfdataset(files, ..., chunks={'lev':1, \"time\": 500}) where lev and time are vertical and time dimensions respectively. Check to see how large each chunk is after modifying the chunk size, and modify as necessary. You do not have enough dask workers If you have a few large files, having the number of workers equal to to the number of input files read in using xr.open_mfdataset would be a good practice If you have a large number of smaller files, you may not run into this issue, and it is suggest you look at the other potential solutions. I have to do lots of rechunking, but the rechunk step uses too much memory and kills my workers. \u00b6 Try the rechunker package . Writing to files in parallel \u00b6 Distributed writes to netCDF are hard. Try writing to zarr using Dataset.to_zarr . If you need to write to netCDF and your final dataset can fit in memory then use dataset.load().to_netcdf(...) . If you really must write a big dataset to netCDF try using save_mfdataset (see here ). My Dask workers are taking a long time to start. How can I monitor them? \u00b6 Dask worker requests are added to the job queues on Casper and Cheyenne with the cluster.scale() method. After this method is called, you can verify that they are waiting in the queue with this command: qstat -u <my_username> on Cheyenne, and the same command will work on Casper after April 2021. If you see no pending worker jobs, then verify that you have called cluster.scale() . Github \u00b6 Setting up Github Authentication \u00b6 Beginning August 13, 2021, Github will no longer accept account passwords when authenticating git operations. There are essentially two options, which Github provides proper documentation for getting setup: Setup two-factor authentication Connect to Github via SSH CESM Data \u00b6 Dealing with CESM monthly output - is there something wrong with time \u00b6 A well known issue of CESM data is that timestamps for fields saved as averages are placed at the end of the averaging period. For instance, in the following example, the January/1920 average has a timestamp of February/1920 : In [ 25 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 33 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: ds . time Out [ 34 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 2 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 4 , 1 , 0 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 11 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2006 , 1 , 1 , 0 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 02 - 01 00 : 00 : 00 ... 2006 - 01 - 01 00 : 00 : 00 Attributes : long_name : time bounds : time_bnds A temporary workaround is to fix the issue ourselves by computing new time axis by averaging the time bounds: In [ 29 ]: import xarray as xr In [ 30 ]: import cf_xarray # use cf-xarray so that we can use CF attributes In [ 31 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 32 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: attrs , encoding = ds . time . attrs . copy (), ds . time . encoding . copy () In [ 36 ]: time_bounds = ds . cf . get_bounds ( 'time' ) In [ 37 ]: time_bounds_dim_name = ds . cf . get_bounds_dim_name ( 'time' ) In [ 38 ]: ds = ds . assign_coords ( time = time_bounds . mean ( time_bounds_dim_name )) In [ 39 ]: ds . time . attrs , ds . time . encoding = attrs , encoding In [ 40 ]: ds . time Out [ 40 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 1 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 2 , 15 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 16 , 12 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 10 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 11 , 16 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 16 , 12 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 01 - 16 12 : 00 : 00 ... 2005 - 12 - 16 12 : 00 : 00 Attributes : long_name : time bounds : time_bnds cf-xarray can be installed via pip or conda. cf-xarray docs are available [here](https://cf-xarray.readthedocs.io/en/latest/).","title":"Frequently Asked Questions"},{"location":"esds/faq/#frequently-asked-questions","text":"A copy of ESDS FAQ page: https://ncar.github.io/esds/faq/ This contains relevant questions and answers from common workflow issues and questions posted on Zulip. Note This page is meant to be a list of FAQ regarding climate datasets, movivated by a variety of employees across UCAR/NCAR.","title":"Frequently Asked Questions"},{"location":"esds/faq/#i-need-help-with-this","text":"","title":"I need help with this!"},{"location":"esds/faq/#where-do-i-go-for-help","text":"Try one of the following resources. Xarray's How Do I do X? page Xarray Github Discussions Pangeo Discourse Forum NCAR Zulip under #python-questions, #python-dev, or #dask. Avoid personal emails and prefer a public forum.","title":"Where do I go for help?"},{"location":"esds/faq/#what-do-i-do-if-my-question-is-not-answered-on-this-page","text":"If your question is related to conda environments and you're affiliated with UCAR/NCAR, you can open a help ticket on the NCAR Research Computing Helpdesk site . If your issue is related to data science packages and workflows, you can open an issue on our GitHub here or book an office hour appointment with an ESDS core member!","title":"What do I do if my question is not answered on this page?"},{"location":"esds/faq/#someone-must-have-written-the-function-i-want-where-do-i-look","text":"See the xarray ecosystem page. Also see the xarray-contrib and pangeo-data organizations. Some NCAR relevant projects include: GeoCAT-comp GeoCAT-viz cf_xarray climpred eofs MetPy rechunker xclim xesmf xgcm pop-tools xskillscore","title":"Someone must have written the function I want. Where do I look?"},{"location":"esds/faq/#how-do-i-use-conda-environments","text":"","title":"How do I use conda environments?"},{"location":"esds/faq/#general-advice","text":"Dealing with Python environments can be tricky... a good place to start is to checkout this guide on dealing with Python environments . If you just need a refresher on the various conda commands, this conda cheet sheet is a wonderful quick reference.","title":"General Advice"},{"location":"esds/faq/#using-conda-on-ncar-hpc-resources","text":"Warning Since 12 December 2022, it is no longer recommended to install your own version of miniconda on the HPC system. To export your existing environments to the recommended installation of miniconda, refer to the \"How can I export my environments?\" section. The NCAR High Performance Computing (HPC) system has a conda installation for you to use. The most recent and detailed instructions can be found on this Using Conda and Python page. If you don't want the trouble of making your own conda environment, there are managed environments available. The NCAR Package Library (NPL) is an environment containing many common scientific Python packages such as Numpy, Xarray, and GeoCAT. You can access the NPL environment through the command line and the NCAR JupyterHub.","title":"Using conda on NCAR HPC resources"},{"location":"esds/faq/#npl-on-the-command-line","text":"Open up a terminal in Casper or Cheyenne Load the NCAR conda module: $ module load conda/latest List the available NCAR managed environments: $ conda env list base * /glade/u/apps/opt/conda npl /glade/u/apps/opt/conda/envs/npl npl-2022b /glade/u/apps/opt/conda/envs/npl-2022b npl-2206 /glade/u/apps/opt/conda/envs/npl-2206 npl-2207 /glade/u/apps/opt/conda/envs/npl-2207 pygpu-dask /glade/u/apps/opt/conda/envs/pygpu-dask Activate the environment you want to use. Here we are using the npl environment as an example. npl can be replaced with any available environment name: $ conda activate npl Now when you run a script, the modules within the npl environment will be available to your program.","title":"NPL on the command line"},{"location":"esds/faq/#npl-on-the-ncar-jupyterhub","text":"Log in to the Production NCAR JupyterHub Start a server With your Jupyter Notebook open, click on the kernel name in the upper right. A dialog will appear with all the various kernels available to you. These kernels will (generally) have the same name as the conda environment that it uses. This may not be the case if you are managing your own environments and kernels. Select the \"npl (conda)\" kernel from the list if you want to use the NCAR-managed NPL environment.","title":"NPL on the NCAR JupyterHub"},{"location":"esds/faq/#creating-and-accessing-a-new-conda-environment-on-the-ncar-jupyterhub","text":"You may want to move past using NPL, and create a new conda environment! For detailed instructions, check out the Using Conda and Python page on the NCAR Advanced Research Computing site. Heres a summary of the basic steps: Create the environment If you are creating an environment from scratch, use the following: conda create --name my_environment where my_environment is the name of your environment Ff you have an environment file (ex. environment.yml ), use the following: conda env create -f environment.yml Activate your environment and install the ipykernel package conda activate my_environment.yml conda install ipykernel The [`ipykernel`](https://github.com/ipython/ipykernel) package is required for your environment to be available from the NCAR [JupyterHub](https://jupyterhub.hpc.ucar.edu/) Accessing your conda environment Your environment should now automatically show up as an available kernel in any Jupyter server on the NCAR HPC systems. If you want to give your kernel a name that is different from the environment name, you can use the following command: python -m ipykernel install --user --name = my-kernel Where my-kernel is the kernel name.","title":"Creating and accessing a new conda environment on the NCAR JupyterHub"},{"location":"esds/faq/#conda-is-taking-too-long-to-solve-environment-use-mamba","text":"This is a very common issue when installing a new package or trying to update a package in an existing conda environment. This issue is usually manifested in a conda message along these lines: environment Solving environment: failed with initial frozen solve. Retrying with flexible solve. One solution to this issue is to use mamba which is a drop-in replacement for conda. Mamba aims to greately speed up and improve conda functionality such as solving environment, installing packages, etc... Installing Mamba conda install -n base -c conda-forge mamba Set conda-forge and nodefaults channels conda config --add channels nodefaults conda config --add channels conda-forge To install a package with mamba, you just run mamba install package_name To create/update an environment from an environment file, run: mamba env update -f environment.yml We do not recommend using `mamba` to activate and deactivate environments as this can cause packages to misbehave/not load correctly. See mamba documentation for more.","title":"Conda is taking too long to solve environment: use mamba"},{"location":"esds/faq/#how-can-i-export-my-environments","text":"If you made an environment on one machine or using a different conda installation, you can export that environment and use it elsewhere. These are the basic steps: Export your environment With the environment you want to export activated, run the following command: conda env export --from-history > environment.yml where environment can be replaced with the file name of your choice. The --from-history flag allows you to recreate your environment on any system. It is the cross-platform compatible way of exporting an environment. Move the environment.yml to the system you want to use it on / activate the appropriate conda installation you wish to use. Use the .yml file to create your environment conda env create -f environment.yml","title":"How can I export my environments?"},{"location":"esds/faq/#xarray-and-dask","text":"","title":"Xarray and Dask"},{"location":"esds/faq/#general-tips","text":"Read the xarray documentation on optimizing workflow with dask . Read the Best practices for dask array Keep track of chunk sizes throughout your workflow. This is especially important when reading in data using xr.open_mfdataset . Aim for 100-200MB size chunks. Choose chunking appropriate to your analysis. If you're working with time series then chunk more in space and less along time. Avoid indexing with .where as much as possible. In particulate .where(..., drop=True) will trigger a compute since it needs to know where NaNs are present to drop them. Instead see if you can write your statement as a .clip , .sel , .isel , or .query statement.","title":"General tips"},{"location":"esds/faq/#how-do-i-optimize-reading-multiple-files-using-xarray-and-dask","text":"A good first place to start when reading in multiple files is Xarray's multi-file documentation . For example, if you are trying to read in multiple files where you are interested in concatenating over the time dimension, here is an example of the xr.open_dataset line would look like: ds = xr . open_mfdataset ( files , # Name of the dimension to concatenate along. concat_dim = \"time\" , # Attempt to auto-magically combine the given datasets into one by using dimension coordinates. combine = \"by_coords\" , # Specify chunks for dask - explained later chunks = { \"lev\" : 1 , \"time\" : 500 }, # Only data variables in which the dimension already appears are included. data_vars = \"minimal\" , # Only coordinates in which the dimension already appears are included. coords = \"minimal\" , # Skip comparing and pick variable from first dataset. compat = \"override\" , parallel = True , )","title":"How do I optimize reading multiple files using Xarray and Dask?"},{"location":"esds/faq/#where-can-i-find-xarray-tutorials","text":"See videos and notebooks .","title":"Where can I find Xarray tutorials?"},{"location":"esds/faq/#how-do-i-debug-my-code-when-using-dask","text":"An option is to use .compute(scheduler=\"single-threaded\") . This will run your code as a serial for loop. When an error is raised you can use the %debug magic to drop in to the stack and debug from there. See this post for more debugging tips in a serial context.","title":"How do I debug my code when using dask?"},{"location":"esds/faq/#killedworker-x-what-do-i-do","text":"Keep an eye on the dask dashboard. If a lot of the bars in the Memory tab are orange, that means your workers are running out of memory. Reduce your chunk size.","title":"KilledWorker X{. What do I do?"},{"location":"esds/faq/#help-my-analysis-is-slow","text":"Try subsetting for just the variable(s) you need for example, if you are reading in a dataset with ~25 variables, and you only need temperature , just read in temperature. You can specify which variables to read in by using the following syntax, following the example of the temperature variable. ds = xr . open_dataset ( file , data_vars = [ 'temperature' ]) Take a look at your chunk size, it might not be optimized. When reading a file in using Xarray with Dask, a \"general rule of thumb\" is to keep your chunk size down to around 100 mb. For example, let's say you trying to read in multiple files, each with ~600 time steps. This is case where each file is very large (several 10s of GB) and using Dask to help with data processing is essential . You can check the size of each chunk by subsetting a single DataArray (ex. ds['temperature'] ) If you have very large chunks, try modifying the number of chunks you specify within xr.open_mfdataset(files, ..., chunks={'lev':1, \"time\": 500}) where lev and time are vertical and time dimensions respectively. Check to see how large each chunk is after modifying the chunk size, and modify as necessary. You do not have enough dask workers If you have a few large files, having the number of workers equal to to the number of input files read in using xr.open_mfdataset would be a good practice If you have a large number of smaller files, you may not run into this issue, and it is suggest you look at the other potential solutions.","title":"Help, my analysis is slow!"},{"location":"esds/faq/#i-have-to-do-lots-of-rechunking-but-the-rechunk-step-uses-too-much-memory-and-kills-my-workers","text":"Try the rechunker package .","title":"I have to do lots of rechunking, but the rechunk step uses too much memory and kills my workers."},{"location":"esds/faq/#writing-to-files-in-parallel","text":"Distributed writes to netCDF are hard. Try writing to zarr using Dataset.to_zarr . If you need to write to netCDF and your final dataset can fit in memory then use dataset.load().to_netcdf(...) . If you really must write a big dataset to netCDF try using save_mfdataset (see here ).","title":"Writing to files in parallel"},{"location":"esds/faq/#my-dask-workers-are-taking-a-long-time-to-start-how-can-i-monitor-them","text":"Dask worker requests are added to the job queues on Casper and Cheyenne with the cluster.scale() method. After this method is called, you can verify that they are waiting in the queue with this command: qstat -u <my_username> on Cheyenne, and the same command will work on Casper after April 2021. If you see no pending worker jobs, then verify that you have called cluster.scale() .","title":"My Dask workers are taking a long time to start. How can I monitor them?"},{"location":"esds/faq/#github","text":"","title":"Github"},{"location":"esds/faq/#setting-up-github-authentication","text":"Beginning August 13, 2021, Github will no longer accept account passwords when authenticating git operations. There are essentially two options, which Github provides proper documentation for getting setup: Setup two-factor authentication Connect to Github via SSH","title":"Setting up Github Authentication"},{"location":"esds/faq/#cesm-data","text":"","title":"CESM Data"},{"location":"esds/faq/#dealing-with-cesm-monthly-output-is-there-something-wrong-with-time","text":"A well known issue of CESM data is that timestamps for fields saved as averages are placed at the end of the averaging period. For instance, in the following example, the January/1920 average has a timestamp of February/1920 : In [ 25 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 33 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: ds . time Out [ 34 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 2 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 4 , 1 , 0 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 11 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2006 , 1 , 1 , 0 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 02 - 01 00 : 00 : 00 ... 2006 - 01 - 01 00 : 00 : 00 Attributes : long_name : time bounds : time_bnds A temporary workaround is to fix the issue ourselves by computing new time axis by averaging the time bounds: In [ 29 ]: import xarray as xr In [ 30 ]: import cf_xarray # use cf-xarray so that we can use CF attributes In [ 31 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 32 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: attrs , encoding = ds . time . attrs . copy (), ds . time . encoding . copy () In [ 36 ]: time_bounds = ds . cf . get_bounds ( 'time' ) In [ 37 ]: time_bounds_dim_name = ds . cf . get_bounds_dim_name ( 'time' ) In [ 38 ]: ds = ds . assign_coords ( time = time_bounds . mean ( time_bounds_dim_name )) In [ 39 ]: ds . time . attrs , ds . time . encoding = attrs , encoding In [ 40 ]: ds . time Out [ 40 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 1 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 2 , 15 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 16 , 12 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 10 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 11 , 16 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 16 , 12 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 01 - 16 12 : 00 : 00 ... 2005 - 12 - 16 12 : 00 : 00 Attributes : long_name : time bounds : time_bnds cf-xarray can be installed via pip or conda. cf-xarray docs are available [here](https://cf-xarray.readthedocs.io/en/latest/).","title":"Dealing with CESM monthly output - is there something wrong with time"},{"location":"getting-started/","text":"Getting started with NCAR HPC Resources \u00b6 About this page This document will guide you through the basics of using NCAR's supercomputers, storage systems, and services. Once you are authorized to use NCAR compute and storage resources, and you have an account and the necessary software , you can follow the procedures described below to log in. These pages provide information on compiling your code, submitting jobs, and performing other common tasks on all NCAR resources unless otherwise noted: Compiling Code on Derecho or Casper Understanding and Customizing your User and Software Environment Starting and Managing Jobs with PBS Managing Your Resource Allocation Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators. Logging In \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: Derecho Casper Cheyenne ssh -X username@derecho.hpc.ucar.edu ssh -X username@casper.hpc.ucar.edu ssh -X username@cheyenne.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. The -X is optional and requests simple X11 graphics forwarding to your client. You can omit username in the command above if your Casper username is the same as your username on your local computer. Tip Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding. New User Resources \u00b6 New User Orientation New User Training for HPC Systems Getting Started on Derecho Getting Started on Casper","title":"Getting started with NCAR HPC Resources"},{"location":"getting-started/#getting-started-with-ncar-hpc-resources","text":"About this page This document will guide you through the basics of using NCAR's supercomputers, storage systems, and services. Once you are authorized to use NCAR compute and storage resources, and you have an account and the necessary software , you can follow the procedures described below to log in. These pages provide information on compiling your code, submitting jobs, and performing other common tasks on all NCAR resources unless otherwise noted: Compiling Code on Derecho or Casper Understanding and Customizing your User and Software Environment Starting and Managing Jobs with PBS Managing Your Resource Allocation Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.","title":"Getting started with NCAR HPC Resources"},{"location":"getting-started/#logging-in","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: Derecho Casper Cheyenne ssh -X username@derecho.hpc.ucar.edu ssh -X username@casper.hpc.ucar.edu ssh -X username@cheyenne.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. The -X is optional and requests simple X11 graphics forwarding to your client. You can omit username in the command above if your Casper username is the same as your username on your local computer. Tip Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding.","title":"Logging In"},{"location":"getting-started/#new-user-resources","text":"New User Orientation New User Training for HPC Systems Getting Started on Derecho Getting Started on Casper","title":"New User Resources"},{"location":"getting-started/acknowledging-ncar-and-cisl/","text":"Acknowledging NCAR and CISL \u00b6 A requirement of all allocations and use of NCAR HPC resources managed by CISL, including the CMIP Analysis Platform and Research Data Archive, is to acknowledge NCAR and CISL support for your research. Our ability to identify supported scientific results helps ensure continued support from NSF and other sources for future HPC systems. Upon completion of your project, NCAR requires an accomplishment report containing: The research results obtained using NCAR resources, a list of scientific publications that resulted from this research, the names and affiliations of graduate students who used NCAR resources, and the title, author, and relevant citation data for any theses or dissertations produced using NCAR computational resources. The accomplishment report should be sent as a PDF to alloc@ucar.edu . Note A PI\u2019s or project lead\u2019s subsequent allocations may be delayed pending compliance with these reporting requirements. More details follow below for Derecho, Cheyenne, Yellowstone, and CMIP Analysis Platform projects, and for the use of Research Data Archive data sets. For Derecho projects \u00b6 If you are publishing a work that uses the Derecho resource, follow the guidance for Cheyenne below, but use the appropriate calendar year, \"Derecho: HPE Cray EX System\" for the system name, and the Derecho DOI ( https://doi.org/10.5065/qx9a-pg09 ). For Cheyenne projects \u00b6 Citation formats \u00b6 We prefer that you cite your use of the Cheyenne system (and/or the Casper data analysis and visualization cluster) with the following citation formats, based on the source of your allocation and modified as needed to conform with citation style guidelines. Be sure to use the digital object identifier (DOI) as shown; it is unique to Cheyenne. Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (Climate Simulation Laboratory). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (University Community Computing). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (NCAR Community Computing). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (Wyoming-NCAR Alliance). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Acknowledgments \u00b6 Acknowledgments are more difficult to track electronically due to the many possible variations, but you may choose to acknowledge support from Cheyenne if the citation method described above is inappropriate. You may modify the following examples as appropriate; however, please include at least the DOI in the acknowledgment text, as you would include funding agency award numbers. For University/CHAP and NCAR allocations \u00b6 We would like to acknowledge high-performance computing support from Cheyenne ( doi:10.5065/D6RX99HX ) provided by NCAR's Computational and Information Systems Laboratory, sponsored by the National Science Foundation. For Climate Simulation Lab (CSL) allocations \u00b6 Computing resources ( doi:10.5065/D6RX99HX ) were provided by the Climate Simulation Laboratory at NCAR's Computational and Information Systems Laboratory, sponsored by the National Science Foundation and other agencies. For Wyoming-NCAR Alliance allocations \u00b6 We would like to acknowledge the use of computational resources ( doi:10.5065/D6RX99HX ) at the NCAR-Wyoming Supercomputing Center provided by the National Science Foundation and the State of Wyoming, and supported by NCAR's Computational and Information Systems Laboratory. For Yellowstone projects \u00b6 If you are publishing a work that used the Yellowstone resource, follow the guidance for Cheyenne, but use 2016 for the citation year, \"Yellowstone: IBM iDataPlex System\" for the system name, and the Yellowstone ARK ( http://n2t.net/ark:/85065/d7wd3xhc ) for the DOI. For CMIP Analysis Platform projects \u00b6 See this CMIP Analysis Platform documentation for how to acknowledge NCAR/CISL support and for information on other requirements related to the use of CMIP data. For use of RDA data sets \u00b6 The data were provided by the Research Data Archive (RDA) of the Computational and Information Systems Laboratory at the National Center for Atmospheric Research. NCAR is supported by grants from the National Science Foundation. We ask that you cite the dataset(s) used. Citations templates are provided on the relevant RDA dataset home pages. For an example, see the \"How to Cite This Dataset\" section on https://rda.ucar.edu/datasets/ds094.0/citation .","title":"Acknowledging NCAR and CISL"},{"location":"getting-started/acknowledging-ncar-and-cisl/#acknowledging-ncar-and-cisl","text":"A requirement of all allocations and use of NCAR HPC resources managed by CISL, including the CMIP Analysis Platform and Research Data Archive, is to acknowledge NCAR and CISL support for your research. Our ability to identify supported scientific results helps ensure continued support from NSF and other sources for future HPC systems. Upon completion of your project, NCAR requires an accomplishment report containing: The research results obtained using NCAR resources, a list of scientific publications that resulted from this research, the names and affiliations of graduate students who used NCAR resources, and the title, author, and relevant citation data for any theses or dissertations produced using NCAR computational resources. The accomplishment report should be sent as a PDF to alloc@ucar.edu . Note A PI\u2019s or project lead\u2019s subsequent allocations may be delayed pending compliance with these reporting requirements. More details follow below for Derecho, Cheyenne, Yellowstone, and CMIP Analysis Platform projects, and for the use of Research Data Archive data sets.","title":"Acknowledging NCAR and CISL"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-derecho-projects","text":"If you are publishing a work that uses the Derecho resource, follow the guidance for Cheyenne below, but use the appropriate calendar year, \"Derecho: HPE Cray EX System\" for the system name, and the Derecho DOI ( https://doi.org/10.5065/qx9a-pg09 ).","title":"For Derecho projects"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-cheyenne-projects","text":"","title":"For Cheyenne projects"},{"location":"getting-started/acknowledging-ncar-and-cisl/#citation-formats","text":"We prefer that you cite your use of the Cheyenne system (and/or the Casper data analysis and visualization cluster) with the following citation formats, based on the source of your allocation and modified as needed to conform with citation style guidelines. Be sure to use the digital object identifier (DOI) as shown; it is unique to Cheyenne. Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (Climate Simulation Laboratory). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (University Community Computing). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (NCAR Community Computing). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX . Computational and Information Systems Laboratory. 2019. Cheyenne: HPE/SGI ICE XA System (Wyoming-NCAR Alliance). Boulder, CO: National Center for Atmospheric Research. doi:10.5065/D6RX99HX .","title":"Citation formats"},{"location":"getting-started/acknowledging-ncar-and-cisl/#acknowledgments","text":"Acknowledgments are more difficult to track electronically due to the many possible variations, but you may choose to acknowledge support from Cheyenne if the citation method described above is inappropriate. You may modify the following examples as appropriate; however, please include at least the DOI in the acknowledgment text, as you would include funding agency award numbers.","title":"Acknowledgments"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-universitychap-and-ncar-allocations","text":"We would like to acknowledge high-performance computing support from Cheyenne ( doi:10.5065/D6RX99HX ) provided by NCAR's Computational and Information Systems Laboratory, sponsored by the National Science Foundation.","title":"For University/CHAP and NCAR allocations"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-climate-simulation-lab-csl-allocations","text":"Computing resources ( doi:10.5065/D6RX99HX ) were provided by the Climate Simulation Laboratory at NCAR's Computational and Information Systems Laboratory, sponsored by the National Science Foundation and other agencies.","title":"For Climate Simulation Lab (CSL) allocations"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-wyoming-ncar-alliance-allocations","text":"We would like to acknowledge the use of computational resources ( doi:10.5065/D6RX99HX ) at the NCAR-Wyoming Supercomputing Center provided by the National Science Foundation and the State of Wyoming, and supported by NCAR's Computational and Information Systems Laboratory.","title":"For Wyoming-NCAR Alliance allocations"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-yellowstone-projects","text":"If you are publishing a work that used the Yellowstone resource, follow the guidance for Cheyenne, but use 2016 for the citation year, \"Yellowstone: IBM iDataPlex System\" for the system name, and the Yellowstone ARK ( http://n2t.net/ark:/85065/d7wd3xhc ) for the DOI.","title":"For Yellowstone projects"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-cmip-analysis-platform-projects","text":"See this CMIP Analysis Platform documentation for how to acknowledge NCAR/CISL support and for information on other requirements related to the use of CMIP data.","title":"For CMIP Analysis Platform projects"},{"location":"getting-started/acknowledging-ncar-and-cisl/#for-use-of-rda-data-sets","text":"The data were provided by the Research Data Archive (RDA) of the Computational and Information Systems Laboratory at the National Center for Atmospheric Research. NCAR is supported by grants from the National Science Foundation. We ask that you cite the dataset(s) used. Citations templates are provided on the relevant RDA dataset home pages. For an example, see the \"How to Cite This Dataset\" section on https://rda.ucar.edu/datasets/ds094.0/citation .","title":"For use of RDA data sets"},{"location":"getting-started/best-practices-for-supercomputer-users/","text":"Best practices for supercomputer users \u00b6 The practices described below will help you make the most of your computing and storage allocations. Using shared resources \u00b6 Be considerate of others in the user community when you work with these shared computing and storage resources. Here are a few key issues to keep in mind. Use login nodes only for their intended purposes \u00b6 You can run short, non-memory-intensive processes on the login nodes. These include tasks such as text editing or running small serial scripts or programs. Memory-intensive processes that slow login node performance for all users are killed automatically and the responsible parties are notified by email. See Appropriate use of login nodes 0 for more information. Use the Derecho and Casper nodes that best meet your needs \u00b6 The Derecho system and Casper nodes are configured for distinct purposes. Derecho is best used for running climate and weather models and simulations while the Casper cluster of nodes is for other specialized work. Most Casper nodes are used for analyzing and visualizing data while others feature large-memory, dense GPU configurations that support explorations in machine learning and deep learning. This documentation explains how to get jobs running on the most appropriate system for your work and on the individual types of nodes that will best meet your needs: Derecho Overview Casper Overview Submitting jobs with PBS Sample Derecho PBS job scripts Sample Casper PBS job scripts For expert assistance or guidance in using these resources, contact the NCAR Research Computing help desk . Don't monopolize compute resources \u00b6 Consider what impact you might have on the work of others and schedule jobs accordingly. For example, avoid writing job submission scripts that rapidly fill the scheduler with potentially concurrent compute resource requests. Contact the Consulting Services Group for guidance if your workload requires you to submit numerous jobs in a short time frame. CISL monitors the use of these resources and will kill jobs when necessary to ensure fair access for all users. Limit your use of shared licenses \u00b6 Users share a limited number of licenses for running IDL, MATLAB, Mathematica, and some other applications. Be familiar with and follow the established license-use guidelines to ensure fair access for all users. CISL reserves the right to kill jobs/tasks of users who monopolize these licenses. Managing allocations \u00b6 Monitor usage charges \u00b6 Check your usage charges frequently to help ensure that you are using CISL resources as efficiently as possible. Also make sure that others who are authorized to charge against your allocation understand how to use them efficiently. Understand how your choice of queues affects charges against your computing allocation and be aware of other allocation-related policies. See Managing your allocation . If you are authorized to charge your work against multiple projects, check your usage charges and storage holdings for each project on a regular basis. This will help ensure that you are charging jobs correctly and help you avoid overspending your allocations. Optimize on a single processor \u00b6 Minimize your use of computing resources and conserve your allocation by optimizing your code on a single processor before running larger jobs in production. Use optimizing libraries if your code lends itself to that. Remove unneeded data \u00b6 Periodically examine your GLADE and NCAR Campaign Storage holdings and remove unwanted, unneeded files. This reduces charges against your storage allocation and makes these systems more efficient for everyone. Contact CISL consultants \u00b6 Before you run a set of jobs that will consume a large portion of your allocation \u2013 a long experiment, for example \u2013 contact the NCAR Research Computing help desk to request a review of your job configuration. One of the consultants may be able to suggest an economical workflow that will help you conserve computing resources. This is especially important if you are unfamiliar with job configuration or with how to manage your allocation efficiently. Writing job scripts \u00b6 Avoid hardcoding in your job scripts \u00b6 Use relative paths and environment variables instead of hardcoding directory names in your job scripts. Hardcoding in scripts and elsewhere can make debugging your code more difficult and also complicate situations in which others need to copy your directories to build and run your code as themselves. Here\u2019s one simple example of what not to do in your script: cd /glade/derecho/scratch/joe/code/running_directory Instead, replace your hardcoded username with $USER : cd /glade/derecho/scratch/ $USER /code/running_directory Better yet, assume that you will launch the job from your working directory so you don\u2019t need to include the path in your script at all. Use comments in job scripts \u00b6 When setting a variable in your job scripts or startup files, include the date and a brief description of the variable's purpose. This practice may help prevent propagation of variables that are possibly inappropriate in carrying jobs and environments forward. One example is noting the use of a variable that is not set or appropriate in most other scripts. # yyyy-mm-dd Context: Cheyenne MPT peak_memusage job. # Variable MPI_SHEPHERD is set in this job in order to # enable peak_memusage. Do not propagate it to other MPT # jobs as it may cause significant slowdown or timeout. export MPI_SHEPHERD = \"true\" Prepare for debugging and troubleshooting \u00b6 Arrange the script, source code, and data used in your job in a few directories to make it easy for others to copy and debug if necessary. Also: Include a README file that details the environment needed to configure, build and run, and that identifies the required modules and environment variables. Ask a colleague or CISL Consulting Services Group consultant to copy and run the code themselves. Managing files \u00b6 Set permissions when you create files \u00b6 Set permissions when you create a file. While you can change file ownership and permissions after the fact, establishing them when you create the file will simplify your life and save you time and effort later. Configure jobs to avoid massive directories \u00b6 Ensemble runs, data assimilation runs, and other jobs generate tens or hundreds of thousands of output files, log files, and others over time. Such large numbers of files can be difficult to manage and remove from GLADE file spaces when they are no longer needed. Configuring jobs to place no more than 2,000 to 3,000 files in a single directory will make them easier to manage. See Removing large numbers of files for how to remove massive accumulations of files. Use scratch space for temporary files \u00b6 The GLADE scratch file space is a temporary space for data that will be analyzed and removed within a short amount of time. It is also the recommended space for temporary files that would otherwise reside in small /tmp or /var/tmp directories that many users share. See Storing temporary files with TMPDIR for more information. Use the most appropriate storage system \u00b6 Review and understand the intended uses of the GLADE file spaces and the NCAR Campaign Storage file system. For example, use your /glade/work space to work with data sets over time periods greater than what is permitted in the scratch space. Individual NCAR labs and project leads for universities that have Campaign Storage allocations establish their own workflows and storage policies. Store large files \u00b6 Storing large files, such as tar files, is more efficient than storing numerous small files. In the case of GLADE disk storage, this is because the system allocates a minimum amount of space for each file, no matter how small. That amount varies depending on which of several file spaces holds the file. See this GLADE documentation for details. Avoid sharing home spaces \u00b6 If you have an account for using the supercomputers, analysis, and visualization systems that CISL manages, you have your own /glade/u/home directory. Other users have their own home directories, too, so there is no need to share by giving others write permission. Sharing often leads to unnecessary confusion over file ownership as your work progresses. If you and your colleagues need to write files to a common space, consider using a work space or project space. Organize for efficiency \u00b6 Organize your files and keep them that way. Arrange them in same-purpose trees, for example. Say you have 20 TB of Mount Pinatubo volcanic aerosols data. Keep the files in a subdirectory such as /glade/u/home/$USER/pinatubo rather than scattered among unrelated files or in multiple directories. Specialized trees are easier to share with other users and to transfer to other users or projects as necessary. Back up critical files \u00b6 With the exception of users' /glade/u/home spaces, the GLADE and NCAR Campaign Storage file systems are not backed up. You are responsible for replicating any data that you feel should be stored at an additional location. Don't abandon files \u00b6 Don't leave orphaned files behind. Before your involvement in a project ends, transfer your files or arrange for someone else to take ownership of them. Transferring data \u00b6 Use Globus to transfer files \u00b6 CISL recommends using Globus to transfer large files or data sets between the GLADE centralized file service, the NCAR Campaign Storage file system, and remote destinations such as university facilities. In addition to web and command line interfaces, Globus offers a feature called Globus Connect Personal that enables users to move files easily to and from laptop or desktop computers and other systems. Secure Copy Protocol ( SCP ) works well for transferring a few relatively small files between most systems.","title":"Best Practices for Supercomputer Users"},{"location":"getting-started/best-practices-for-supercomputer-users/#best-practices-for-supercomputer-users","text":"The practices described below will help you make the most of your computing and storage allocations.","title":"Best practices for supercomputer users"},{"location":"getting-started/best-practices-for-supercomputer-users/#using-shared-resources","text":"Be considerate of others in the user community when you work with these shared computing and storage resources. Here are a few key issues to keep in mind.","title":"Using shared resources"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-login-nodes-only-for-their-intended-purposes","text":"You can run short, non-memory-intensive processes on the login nodes. These include tasks such as text editing or running small serial scripts or programs. Memory-intensive processes that slow login node performance for all users are killed automatically and the responsible parties are notified by email. See Appropriate use of login nodes 0 for more information.","title":"Use login nodes only for their intended purposes"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-the-derecho-and-casper-nodes-that-best-meet-your-needs","text":"The Derecho system and Casper nodes are configured for distinct purposes. Derecho is best used for running climate and weather models and simulations while the Casper cluster of nodes is for other specialized work. Most Casper nodes are used for analyzing and visualizing data while others feature large-memory, dense GPU configurations that support explorations in machine learning and deep learning. This documentation explains how to get jobs running on the most appropriate system for your work and on the individual types of nodes that will best meet your needs: Derecho Overview Casper Overview Submitting jobs with PBS Sample Derecho PBS job scripts Sample Casper PBS job scripts For expert assistance or guidance in using these resources, contact the NCAR Research Computing help desk .","title":"Use the Derecho and Casper nodes that best meet your needs"},{"location":"getting-started/best-practices-for-supercomputer-users/#dont-monopolize-compute-resources","text":"Consider what impact you might have on the work of others and schedule jobs accordingly. For example, avoid writing job submission scripts that rapidly fill the scheduler with potentially concurrent compute resource requests. Contact the Consulting Services Group for guidance if your workload requires you to submit numerous jobs in a short time frame. CISL monitors the use of these resources and will kill jobs when necessary to ensure fair access for all users.","title":"Don't monopolize compute resources"},{"location":"getting-started/best-practices-for-supercomputer-users/#limit-your-use-of-shared-licenses","text":"Users share a limited number of licenses for running IDL, MATLAB, Mathematica, and some other applications. Be familiar with and follow the established license-use guidelines to ensure fair access for all users. CISL reserves the right to kill jobs/tasks of users who monopolize these licenses.","title":"Limit your use of shared licenses"},{"location":"getting-started/best-practices-for-supercomputer-users/#managing-allocations","text":"","title":"Managing allocations"},{"location":"getting-started/best-practices-for-supercomputer-users/#monitor-usage-charges","text":"Check your usage charges frequently to help ensure that you are using CISL resources as efficiently as possible. Also make sure that others who are authorized to charge against your allocation understand how to use them efficiently. Understand how your choice of queues affects charges against your computing allocation and be aware of other allocation-related policies. See Managing your allocation . If you are authorized to charge your work against multiple projects, check your usage charges and storage holdings for each project on a regular basis. This will help ensure that you are charging jobs correctly and help you avoid overspending your allocations.","title":"Monitor usage charges"},{"location":"getting-started/best-practices-for-supercomputer-users/#optimize-on-a-single-processor","text":"Minimize your use of computing resources and conserve your allocation by optimizing your code on a single processor before running larger jobs in production. Use optimizing libraries if your code lends itself to that.","title":"Optimize on a single processor"},{"location":"getting-started/best-practices-for-supercomputer-users/#remove-unneeded-data","text":"Periodically examine your GLADE and NCAR Campaign Storage holdings and remove unwanted, unneeded files. This reduces charges against your storage allocation and makes these systems more efficient for everyone.","title":"Remove unneeded data"},{"location":"getting-started/best-practices-for-supercomputer-users/#contact-cisl-consultants","text":"Before you run a set of jobs that will consume a large portion of your allocation \u2013 a long experiment, for example \u2013 contact the NCAR Research Computing help desk to request a review of your job configuration. One of the consultants may be able to suggest an economical workflow that will help you conserve computing resources. This is especially important if you are unfamiliar with job configuration or with how to manage your allocation efficiently.","title":"Contact CISL consultants"},{"location":"getting-started/best-practices-for-supercomputer-users/#writing-job-scripts","text":"","title":"Writing job scripts"},{"location":"getting-started/best-practices-for-supercomputer-users/#avoid-hardcoding-in-your-job-scripts","text":"Use relative paths and environment variables instead of hardcoding directory names in your job scripts. Hardcoding in scripts and elsewhere can make debugging your code more difficult and also complicate situations in which others need to copy your directories to build and run your code as themselves. Here\u2019s one simple example of what not to do in your script: cd /glade/derecho/scratch/joe/code/running_directory Instead, replace your hardcoded username with $USER : cd /glade/derecho/scratch/ $USER /code/running_directory Better yet, assume that you will launch the job from your working directory so you don\u2019t need to include the path in your script at all.","title":"Avoid hardcoding in your job scripts"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-comments-in-job-scripts","text":"When setting a variable in your job scripts or startup files, include the date and a brief description of the variable's purpose. This practice may help prevent propagation of variables that are possibly inappropriate in carrying jobs and environments forward. One example is noting the use of a variable that is not set or appropriate in most other scripts. # yyyy-mm-dd Context: Cheyenne MPT peak_memusage job. # Variable MPI_SHEPHERD is set in this job in order to # enable peak_memusage. Do not propagate it to other MPT # jobs as it may cause significant slowdown or timeout. export MPI_SHEPHERD = \"true\"","title":"Use comments in job scripts"},{"location":"getting-started/best-practices-for-supercomputer-users/#prepare-for-debugging-and-troubleshooting","text":"Arrange the script, source code, and data used in your job in a few directories to make it easy for others to copy and debug if necessary. Also: Include a README file that details the environment needed to configure, build and run, and that identifies the required modules and environment variables. Ask a colleague or CISL Consulting Services Group consultant to copy and run the code themselves.","title":"Prepare for debugging and troubleshooting"},{"location":"getting-started/best-practices-for-supercomputer-users/#managing-files","text":"","title":"Managing files"},{"location":"getting-started/best-practices-for-supercomputer-users/#set-permissions-when-you-create-files","text":"Set permissions when you create a file. While you can change file ownership and permissions after the fact, establishing them when you create the file will simplify your life and save you time and effort later.","title":"Set permissions when you create files"},{"location":"getting-started/best-practices-for-supercomputer-users/#configure-jobs-to-avoid-massive-directories","text":"Ensemble runs, data assimilation runs, and other jobs generate tens or hundreds of thousands of output files, log files, and others over time. Such large numbers of files can be difficult to manage and remove from GLADE file spaces when they are no longer needed. Configuring jobs to place no more than 2,000 to 3,000 files in a single directory will make them easier to manage. See Removing large numbers of files for how to remove massive accumulations of files.","title":"Configure jobs to avoid massive directories"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-scratch-space-for-temporary-files","text":"The GLADE scratch file space is a temporary space for data that will be analyzed and removed within a short amount of time. It is also the recommended space for temporary files that would otherwise reside in small /tmp or /var/tmp directories that many users share. See Storing temporary files with TMPDIR for more information.","title":"Use scratch space for temporary files"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-the-most-appropriate-storage-system","text":"Review and understand the intended uses of the GLADE file spaces and the NCAR Campaign Storage file system. For example, use your /glade/work space to work with data sets over time periods greater than what is permitted in the scratch space. Individual NCAR labs and project leads for universities that have Campaign Storage allocations establish their own workflows and storage policies.","title":"Use the most appropriate storage system"},{"location":"getting-started/best-practices-for-supercomputer-users/#store-large-files","text":"Storing large files, such as tar files, is more efficient than storing numerous small files. In the case of GLADE disk storage, this is because the system allocates a minimum amount of space for each file, no matter how small. That amount varies depending on which of several file spaces holds the file. See this GLADE documentation for details.","title":"Store large files"},{"location":"getting-started/best-practices-for-supercomputer-users/#avoid-sharing-home-spaces","text":"If you have an account for using the supercomputers, analysis, and visualization systems that CISL manages, you have your own /glade/u/home directory. Other users have their own home directories, too, so there is no need to share by giving others write permission. Sharing often leads to unnecessary confusion over file ownership as your work progresses. If you and your colleagues need to write files to a common space, consider using a work space or project space.","title":"Avoid sharing home spaces"},{"location":"getting-started/best-practices-for-supercomputer-users/#organize-for-efficiency","text":"Organize your files and keep them that way. Arrange them in same-purpose trees, for example. Say you have 20 TB of Mount Pinatubo volcanic aerosols data. Keep the files in a subdirectory such as /glade/u/home/$USER/pinatubo rather than scattered among unrelated files or in multiple directories. Specialized trees are easier to share with other users and to transfer to other users or projects as necessary.","title":"Organize for efficiency"},{"location":"getting-started/best-practices-for-supercomputer-users/#back-up-critical-files","text":"With the exception of users' /glade/u/home spaces, the GLADE and NCAR Campaign Storage file systems are not backed up. You are responsible for replicating any data that you feel should be stored at an additional location.","title":"Back up critical files"},{"location":"getting-started/best-practices-for-supercomputer-users/#dont-abandon-files","text":"Don't leave orphaned files behind. Before your involvement in a project ends, transfer your files or arrange for someone else to take ownership of them.","title":"Don't abandon files"},{"location":"getting-started/best-practices-for-supercomputer-users/#transferring-data","text":"","title":"Transferring data"},{"location":"getting-started/best-practices-for-supercomputer-users/#use-globus-to-transfer-files","text":"CISL recommends using Globus to transfer large files or data sets between the GLADE centralized file service, the NCAR Campaign Storage file system, and remote destinations such as university facilities. In addition to web and command line interfaces, Globus offers a feature called Globus Connect Personal that enables users to move files easily to and from laptop or desktop computers and other systems. Secure Copy Protocol ( SCP ) works well for transferring a few relatively small files between most systems.","title":"Use Globus to transfer files"},{"location":"getting-started/data-retention-policies/","text":"General data retention policies and procedures \u00b6 The following general guidelines apply to project and user data. For a complete listing of all shared file systems, including purge details for scratch space, see GLADE file spaces . Project data \u00b6 When a sponsored project approaches expiry, there are several steps in the process that affect the accessibility of associated data: 30 days before expiration, the project PIs will receive an email reminding them of the pending expiration. The project team should assess remaining files and disposition appropriately in preparation for group deactivation. 90 days after project expiration, the UNIX group associated with the project is removed. At this point users with accounts remaining on the system will likely no longer have access permissions to the projects' data, as the primary group no longer exists. It is therefore imperative that any remaining project data be relocated and ownership permissions assessed prior to this group deactivation. Finally, files are removed as scheduled on the timeline described above for the relevant file system. Restoring access to project data \u00b6 CISL has limited ability to modify access to project data after the 90-day post-expiry window. Such modifications require the approval of the original project owner. CISL has no ability to restore data after the purge or removal policies stated above have taken effect. User accounts \u00b6 User accounts are deactivated when they are no longer associated with an active project. When a user account is deactivated, several steps in the process affect the accessibility of the users' data: 30 days after a user account is deactivated, a final home directory backup is performed and the home directory is removed. The user\u2019s work directory is removed. No backup is performed. Finally, additional scratch files are removed as scheduled on the timeline described above for the relevant file system. Restoring access to collaborators' data \u00b6 A typical request for data access comes not from the departing user, but from remaining collaborators. Colleagues occasionally request access to a departed users' files, sometimes many months after the account is terminated, often when they realize the original owner set permissions that limit their access. While CISL has a limited ability to help in these situations, there are also legal limits to what we can do. For example, CISL cannot share files beyond the clear intent of the original owner as inferred from the UNIX file permissions. If a collaborator would like access to a file that was previously group- or world-readable, we may be able to help. If the original file was restricted to user-only read, however, we cannot override those intentions. The only exceptions to this policy are in compliance with broader UCAR IT records or investigation policies as described in UCAR's 1-7 Information Security Policy.","title":"Data Retention Policies"},{"location":"getting-started/data-retention-policies/#general-data-retention-policies-and-procedures","text":"The following general guidelines apply to project and user data. For a complete listing of all shared file systems, including purge details for scratch space, see GLADE file spaces .","title":"General data retention policies and procedures"},{"location":"getting-started/data-retention-policies/#project-data","text":"When a sponsored project approaches expiry, there are several steps in the process that affect the accessibility of associated data: 30 days before expiration, the project PIs will receive an email reminding them of the pending expiration. The project team should assess remaining files and disposition appropriately in preparation for group deactivation. 90 days after project expiration, the UNIX group associated with the project is removed. At this point users with accounts remaining on the system will likely no longer have access permissions to the projects' data, as the primary group no longer exists. It is therefore imperative that any remaining project data be relocated and ownership permissions assessed prior to this group deactivation. Finally, files are removed as scheduled on the timeline described above for the relevant file system.","title":"Project data"},{"location":"getting-started/data-retention-policies/#restoring-access-to-project-data","text":"CISL has limited ability to modify access to project data after the 90-day post-expiry window. Such modifications require the approval of the original project owner. CISL has no ability to restore data after the purge or removal policies stated above have taken effect.","title":"Restoring access to project data"},{"location":"getting-started/data-retention-policies/#user-accounts","text":"User accounts are deactivated when they are no longer associated with an active project. When a user account is deactivated, several steps in the process affect the accessibility of the users' data: 30 days after a user account is deactivated, a final home directory backup is performed and the home directory is removed. The user\u2019s work directory is removed. No backup is performed. Finally, additional scratch files are removed as scheduled on the timeline described above for the relevant file system.","title":"User accounts"},{"location":"getting-started/data-retention-policies/#restoring-access-to-collaborators-data","text":"A typical request for data access comes not from the departing user, but from remaining collaborators. Colleagues occasionally request access to a departed users' files, sometimes many months after the account is terminated, often when they realize the original owner set permissions that limit their access. While CISL has a limited ability to help in these situations, there are also legal limits to what we can do. For example, CISL cannot share files beyond the clear intent of the original owner as inferred from the UNIX file permissions. If a collaborator would like access to a file that was previously group- or world-readable, we may be able to help. If the original file was restricted to user-only read, however, we cannot override those intentions. The only exceptions to this policy are in compliance with broader UCAR IT records or investigation policies as described in UCAR's 1-7 Information Security Policy.","title":"Restoring access to collaborators' data"},{"location":"getting-started/managing-your-allocation/","text":"Managing your allocation \u00b6 Actively managing and monitoring your allocation of computing and storage resources will help ensure that you use these resources as efficiently as possible. Allocations are made to the project lead, who is designated in the allocation request. The project lead often is the principal investigator (PI) for the associated funding awards. The sections below describe how to perform basic administrative tasks and use the Systems Accounting Manager ( SAM ) to monitor charges against your allocation. All authorized users on a project can manage their own preferences and review their own usage reports in SAM. To get the most from your allocation, be sure to fully utilize the compute resources you request and use available memory efficiently. Also be aware that event failed jobs use some of your allocation, so be proactive in identifying reasons for failures (and contact the NCAR Research Computing help desk if you need assistance). Guidelines and allocation management \u00b6 The project lead may authorize a project administrator (project admin) to perform some tasks on behalf of the project, such as adding or removing users. To designate a project admin, submit a request through the NCAR Research Computing help desk . Here are some guidelines for managing user access to an allocation: Establish user accounts only for appropriate personnel and make users aware of relevant responsibilities and best practices . Request removal of users' accounts when those individuals are no longer associated with your project. Adding and removing user accounts \u00b6 Some users who are authorized to share access to a project's allocation are identified when the allocation is requested. PIs, project leads, and project admins can also add other users to their projects any time after receiving an allocation award. To add new user accounts for a project, send a request through the help desk link above or call 303-497-2400. Include the user's name, email address, phone number, and a full shipping address for delivery of the new user's authentication token if a physical token is required. You do not need to include a shipping address if the user already has a CISL token. A PI, project lead, or project admin can deauthorize or remove a user from a project by contacting the help desk. Specifying project to be charged \u00b6 A user account must be associated with at least one allocated project and may be associated with more than one. You will have an alphanumeric project code for each such project. These project codes are used to charge your use of computing and storage resources against the appropriate allocations. Take care to specify the correct project code when you submit jobs. Even if you have only one project code, you must specify the project code to be charged when you submit a job. How to do this is described in the documentation for each HPC system. Charges for computing \u00b6 Projects must have allocations of both CPU core-hours and GPU hours in order to use both types of nodes. Depending on which computing resources are being used, charges are assessed in adjusted core-hours or GPU hours or both in the case of hybrid jobs. These metrics are defined as the number of processors requested multiplied by the duration of the job in hours and, for Derecho and Cheyenne jobs, modified by job priority . Priority level Charging factor Economy 0.7x Regular 1.0x Premium 1.5x If you are concerned about your usage rate, contact the NCAR Research Computing help desk for guidance on running jobs more efficiently and conserving your allocation. Sometimes jobs can be configured to make better use of the compute resources, and you may be able to save allocation by using a less-expensive priority level. Seek help if you notice anything amiss with your allocation. Charging formula \u00b6 All jobs on Derecho and Casper are charged according to the following formula: wall-clock hours \u00d7 requested resource (ncpus or ngpus) \u00d7 queue factor Consider the impact of your choice of nodes before you submit a job on these systems. When using exclusive nodes in the \"main\" submission queue on Derecho, you will be charged for all CPUs or GPUs on those nodes regardless of how many you request . For example, if you request two nodes with ncpus = 18 on each node, you will still be charged for 256 CPUs when running in main because those nodes are for your exclusive use when your job is running. Conversely, when using shared nodes in the \"develop\" queue on Derecho or the \"casper\" queue on Casper, you will be charged only for the CPUs or GPUs that you requested. Note When using GPU nodes, you are not charged for the CPU wall-time used on those nodes. However, it is possible for a job to incur both CPU and GPU costs if you use a heterogeneous mix of node types, with each node being charged according to the dominant resource type (GPUs take precedence). Tracking usage \u00b6 Individuals can track their HPC system usage in the Systems Accounting Manager (see below). SAM reports show usage data and charges against your allocations. Charges for computing are calculated and updated daily; storage use reports are updated weekly. Using the Systems Accounting Manager \u00b6 User preferences \u00b6 Log in at sam.ucar.edu and select User , then Preferences . From there, you can: Change your primary group if you belong to more than one UNIX group for using NCAR computing resources. Specify your default login shell for the systems to which you have access. See what your home directory is on each system. SAM reports \u00b6 Log in to sam.ucar.edu and you will see the following choices on the Reports menu: My Account Statements My 30/90 Day Use Project Search If you are authorized to charge usage to multiple projects, you will see them listed when you select either of the first two report types. Select one of the projects listed to get information. NCAR divisional users often have access to numerous projects, while individual university users most often have just one or a few. In either case, use Project Search to: go directly to a report on a specified project, or search by username to see all projects with which you are associated. My Account Statements \u00b6 Your account statement includes an overall report on the status of your project\u2019s computing and storage allocation and the usage associated with it. If you are authorized to charge usage to more than one project, you will have an account statement for each project. The overall usage report on project activity shows your allocations\u2019 start and end dates, amounts allocated, and remaining balances. The Activity link at the end of each line reveals more details: the project\u2019s allocation history, a monthly summary of job charges, and other activity, such as refunds. You can select a month and then view or download the individual job records. Another table includes additional information regarding your project\u2019s status in relation to any 30- and 90-day usage thresholds that apply and to any related projects. This is most common for NCAR users on divisional projects. Your own statement may show lines for multiple projects or subprojects, as is often the case for NCAR divisional allocations. My 30/90 Day Use \u00b6 This selection lets you focus on your usage in relation to any 30- and 90-day usage thresholds that apply. Again, this is most common for NCAR divisional projects. Project Search \u00b6 You can search by individual project code and get an account statement as described above. If you search by your username, you will see a list of any projects you are associated with and you can select any of them to get an account statement.","title":"Managing Your Allocation"},{"location":"getting-started/managing-your-allocation/#managing-your-allocation","text":"Actively managing and monitoring your allocation of computing and storage resources will help ensure that you use these resources as efficiently as possible. Allocations are made to the project lead, who is designated in the allocation request. The project lead often is the principal investigator (PI) for the associated funding awards. The sections below describe how to perform basic administrative tasks and use the Systems Accounting Manager ( SAM ) to monitor charges against your allocation. All authorized users on a project can manage their own preferences and review their own usage reports in SAM. To get the most from your allocation, be sure to fully utilize the compute resources you request and use available memory efficiently. Also be aware that event failed jobs use some of your allocation, so be proactive in identifying reasons for failures (and contact the NCAR Research Computing help desk if you need assistance).","title":"Managing your allocation"},{"location":"getting-started/managing-your-allocation/#guidelines-and-allocation-management","text":"The project lead may authorize a project administrator (project admin) to perform some tasks on behalf of the project, such as adding or removing users. To designate a project admin, submit a request through the NCAR Research Computing help desk . Here are some guidelines for managing user access to an allocation: Establish user accounts only for appropriate personnel and make users aware of relevant responsibilities and best practices . Request removal of users' accounts when those individuals are no longer associated with your project.","title":"Guidelines and allocation management"},{"location":"getting-started/managing-your-allocation/#adding-and-removing-user-accounts","text":"Some users who are authorized to share access to a project's allocation are identified when the allocation is requested. PIs, project leads, and project admins can also add other users to their projects any time after receiving an allocation award. To add new user accounts for a project, send a request through the help desk link above or call 303-497-2400. Include the user's name, email address, phone number, and a full shipping address for delivery of the new user's authentication token if a physical token is required. You do not need to include a shipping address if the user already has a CISL token. A PI, project lead, or project admin can deauthorize or remove a user from a project by contacting the help desk.","title":"Adding and removing user accounts"},{"location":"getting-started/managing-your-allocation/#specifying-project-to-be-charged","text":"A user account must be associated with at least one allocated project and may be associated with more than one. You will have an alphanumeric project code for each such project. These project codes are used to charge your use of computing and storage resources against the appropriate allocations. Take care to specify the correct project code when you submit jobs. Even if you have only one project code, you must specify the project code to be charged when you submit a job. How to do this is described in the documentation for each HPC system.","title":"Specifying project to be charged"},{"location":"getting-started/managing-your-allocation/#charges-for-computing","text":"Projects must have allocations of both CPU core-hours and GPU hours in order to use both types of nodes. Depending on which computing resources are being used, charges are assessed in adjusted core-hours or GPU hours or both in the case of hybrid jobs. These metrics are defined as the number of processors requested multiplied by the duration of the job in hours and, for Derecho and Cheyenne jobs, modified by job priority . Priority level Charging factor Economy 0.7x Regular 1.0x Premium 1.5x If you are concerned about your usage rate, contact the NCAR Research Computing help desk for guidance on running jobs more efficiently and conserving your allocation. Sometimes jobs can be configured to make better use of the compute resources, and you may be able to save allocation by using a less-expensive priority level. Seek help if you notice anything amiss with your allocation.","title":"Charges for computing"},{"location":"getting-started/managing-your-allocation/#charging-formula","text":"All jobs on Derecho and Casper are charged according to the following formula: wall-clock hours \u00d7 requested resource (ncpus or ngpus) \u00d7 queue factor Consider the impact of your choice of nodes before you submit a job on these systems. When using exclusive nodes in the \"main\" submission queue on Derecho, you will be charged for all CPUs or GPUs on those nodes regardless of how many you request . For example, if you request two nodes with ncpus = 18 on each node, you will still be charged for 256 CPUs when running in main because those nodes are for your exclusive use when your job is running. Conversely, when using shared nodes in the \"develop\" queue on Derecho or the \"casper\" queue on Casper, you will be charged only for the CPUs or GPUs that you requested. Note When using GPU nodes, you are not charged for the CPU wall-time used on those nodes. However, it is possible for a job to incur both CPU and GPU costs if you use a heterogeneous mix of node types, with each node being charged according to the dominant resource type (GPUs take precedence).","title":"Charging formula"},{"location":"getting-started/managing-your-allocation/#tracking-usage","text":"Individuals can track their HPC system usage in the Systems Accounting Manager (see below). SAM reports show usage data and charges against your allocations. Charges for computing are calculated and updated daily; storage use reports are updated weekly.","title":"Tracking usage"},{"location":"getting-started/managing-your-allocation/#using-the-systems-accounting-manager","text":"","title":"Using the Systems Accounting Manager"},{"location":"getting-started/managing-your-allocation/#user-preferences","text":"Log in at sam.ucar.edu and select User , then Preferences . From there, you can: Change your primary group if you belong to more than one UNIX group for using NCAR computing resources. Specify your default login shell for the systems to which you have access. See what your home directory is on each system.","title":"User preferences"},{"location":"getting-started/managing-your-allocation/#sam-reports","text":"Log in to sam.ucar.edu and you will see the following choices on the Reports menu: My Account Statements My 30/90 Day Use Project Search If you are authorized to charge usage to multiple projects, you will see them listed when you select either of the first two report types. Select one of the projects listed to get information. NCAR divisional users often have access to numerous projects, while individual university users most often have just one or a few. In either case, use Project Search to: go directly to a report on a specified project, or search by username to see all projects with which you are associated.","title":"SAM reports"},{"location":"getting-started/managing-your-allocation/#my-account-statements","text":"Your account statement includes an overall report on the status of your project\u2019s computing and storage allocation and the usage associated with it. If you are authorized to charge usage to more than one project, you will have an account statement for each project. The overall usage report on project activity shows your allocations\u2019 start and end dates, amounts allocated, and remaining balances. The Activity link at the end of each line reveals more details: the project\u2019s allocation history, a monthly summary of job charges, and other activity, such as refunds. You can select a month and then view or download the individual job records. Another table includes additional information regarding your project\u2019s status in relation to any 30- and 90-day usage thresholds that apply and to any related projects. This is most common for NCAR users on divisional projects. Your own statement may show lines for multiple projects or subprojects, as is often the case for NCAR divisional allocations.","title":"My Account Statements"},{"location":"getting-started/managing-your-allocation/#my-3090-day-use","text":"This selection lets you focus on your usage in relation to any 30- and 90-day usage thresholds that apply. Again, this is most common for NCAR divisional projects.","title":"My 30/90 Day Use"},{"location":"getting-started/managing-your-allocation/#project-search","text":"You can search by individual project code and get an account statement as described above. If you search by your username, you will see a list of any projects you are associated with and you can select any of them to get an account statement.","title":"Project Search"},{"location":"getting-started/system-usage-policies/","text":"System use policies \u00b6 Appropriate use of login nodes \u00b6 Users may run short, non-memory-intensive processes interactively on the Derecho system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster. Fair share policy \u00b6 CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, the Antarctic Mesoscale Prediction System (AMPS), and the Wyoming community. The fair-share policy takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions. Job scheduling priorities \u00b6 The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following: Job priority (user selectable) Fair share factor Eligible time in queue Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: The necessary resources are not yet available. The system has been reserved for a scheduled outage. The job has been placed on hold. You have reached your concurrent core-usage limit when using the develop queue. A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job. PBS sorting factors \u00b6 Stakeholder shares and fair-share factor \u00b6 CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half. Job priority \u00b6 Users can set job priority to one of three values. Jobs with higher priority are charged against the user's allocation at higher rates than others. Job priority Priority order Priority factor Description premium 1 1.5 Jobs are charged at 150% of the regular rate. regular 2 1 All production jobs default to this priority. economy 3 0.7 Production batch jobs are charged at 70% of regular rate. preempt 4 0 Automatically selected when job is submitted to preempt queue. Job size \u00b6 Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start. GPU usage \u00b6 In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected. Backfilling \u00b6 When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs. Preemption \u00b6 Derecho has a preemption routing queue that can be used to submit jobs that will run when the required resources are not in use for higher-priority work in the main or develop execution queues. In order to take advantage of preemption, submit your job to the preempt routing queue and it job will run when the necessary resources are available. When resources for any preempt jobs are needed by higher-priority work, the scheduler sends a SIGTERM signal that can be detected by your job. After the SIGTERM signal is sent to the job, there is a five-minute window in which the job has a chance to checkpoint or save any work that was accomplished. After the five-minute window, the job will be killed by the scheduler and deleted.","title":"System Usage Policies"},{"location":"getting-started/system-usage-policies/#system-use-policies","text":"","title":"System use policies"},{"location":"getting-started/system-usage-policies/#appropriate-use-of-login-nodes","text":"Users may run short, non-memory-intensive processes interactively on the Derecho system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster.","title":"Appropriate use of login nodes"},{"location":"getting-started/system-usage-policies/#fair-share-policy","text":"CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, the Antarctic Mesoscale Prediction System (AMPS), and the Wyoming community. The fair-share policy takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions.","title":"Fair share policy"},{"location":"getting-started/system-usage-policies/#job-scheduling-priorities","text":"The PBS Pro workload management system scheduling policy for running jobs in the Derecho environment requires balancing several factors. Jobs generally are sorted based on the following: Job priority (user selectable) Fair share factor Eligible time in queue Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: The necessary resources are not yet available. The system has been reserved for a scheduled outage. The job has been placed on hold. You have reached your concurrent core-usage limit when using the develop queue. A high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues indefinitely, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.","title":"Job scheduling priorities"},{"location":"getting-started/system-usage-policies/#pbs-sorting-factors","text":"","title":"PBS sorting factors"},{"location":"getting-started/system-usage-policies/#stakeholder-shares-and-fair-share-factor","text":"CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the CESM community, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In such a case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Shares are evaluated based on usage over the past week with usage the prior week being decayed by half.","title":"Stakeholder shares and fair-share factor"},{"location":"getting-started/system-usage-policies/#job-priority","text":"Users can set job priority to one of three values. Jobs with higher priority are charged against the user's allocation at higher rates than others. Job priority Priority order Priority factor Description premium 1 1.5 Jobs are charged at 150% of the regular rate. regular 2 1 All production jobs default to this priority. economy 3 0.7 Production batch jobs are charged at 70% of regular rate. preempt 4 0 Automatically selected when job is submitted to preempt queue.","title":"Job priority"},{"location":"getting-started/system-usage-policies/#job-size","text":"Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs or GPUs to start.","title":"Job size"},{"location":"getting-started/system-usage-policies/#gpu-usage","text":"In order to submit jobs that will use GPUs, you must be associated with a project that has an allocation of GPU hours. If you submit a job with a project code that does not have an allocation of GPU hours, your job will be rejected.","title":"GPU usage"},{"location":"getting-started/system-usage-policies/#backfilling","text":"When a job cannot start immediately, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.","title":"Backfilling"},{"location":"getting-started/system-usage-policies/#preemption","text":"Derecho has a preemption routing queue that can be used to submit jobs that will run when the required resources are not in use for higher-priority work in the main or develop execution queues. In order to take advantage of preemption, submit your job to the preempt routing queue and it job will run when the necessary resources are available. When resources for any preempt jobs are needed by higher-priority work, the scheduler sends a SIGTERM signal that can be detected by your job. After the SIGTERM signal is sent to the job, there is a five-minute window in which the job has a chance to checkpoint or save any work that was accomplished. After the five-minute window, the job will be killed by the scheduler and deleted.","title":"Preemption"},{"location":"getting-started/user-responsibilities/","text":"User responsibilities \u00b6 When you are granted access to use NCAR resources, you accept the responsibilities listed below. You will use these computer and information systems in an ethical and legal manner. You agree not to duplicate or use copyrighted or proprietary software without proper authorization. You may not use NCAR computer and information systems in any manner for any business, professional, or other activity that is unrelated to the purpose of your allocation or terms of employment except as otherwise stated in Access to and Use of Information Systems and Technology Infrastructure (staff login required). You are required to acknowledge the use of NCAR resources, including the CMIP Analysis Platform and Research Data Archive, in any resulting publications, in a manner consistent with the examples provided here . You are responsible for protecting your personal identification number, authentication token, and/or passwords. You may not share your account privileges with anyone or knowingly permit any unauthorized access to a computer, computer privileges, systems, networks, or programs. The accounts of those involved will be disabled if sharing is detected. While NCAR storage systems are highly reliable, you are responsible for backing up critical data to protect it against loss or corruption. You also are responsible for understanding the usage and data retention policies for the file system and data archive resources used. You agree to report potential security breaches as soon as possible by calling the Research Computing Help Desk at 303-497-2400. You are responsible for ensuring that NCAR has your current contact information, including your phone number, email address, and mailing address. If your name, phone number, email address, or other information changes, notify CISL through support.ucar.edu . If CISL personnel can\u2019t reach you when they need to \u2013 about a problem caused by a job you are running, for example \u2013 the job may be killed and you will be locked out of the system. Project leads (including instructors associated with classroom allocations) are responsible for ensuring that users on their projects are aware of these responsibilities and for ensuring that authentication tokens are returned when users complete their work or students finish their classes. Project leads are responsible for any token replacement fees. Please contact the NCAR Research Computing help desk if you have questions.","title":"User Responsibilities"},{"location":"getting-started/user-responsibilities/#user-responsibilities","text":"When you are granted access to use NCAR resources, you accept the responsibilities listed below. You will use these computer and information systems in an ethical and legal manner. You agree not to duplicate or use copyrighted or proprietary software without proper authorization. You may not use NCAR computer and information systems in any manner for any business, professional, or other activity that is unrelated to the purpose of your allocation or terms of employment except as otherwise stated in Access to and Use of Information Systems and Technology Infrastructure (staff login required). You are required to acknowledge the use of NCAR resources, including the CMIP Analysis Platform and Research Data Archive, in any resulting publications, in a manner consistent with the examples provided here . You are responsible for protecting your personal identification number, authentication token, and/or passwords. You may not share your account privileges with anyone or knowingly permit any unauthorized access to a computer, computer privileges, systems, networks, or programs. The accounts of those involved will be disabled if sharing is detected. While NCAR storage systems are highly reliable, you are responsible for backing up critical data to protect it against loss or corruption. You also are responsible for understanding the usage and data retention policies for the file system and data archive resources used. You agree to report potential security breaches as soon as possible by calling the Research Computing Help Desk at 303-497-2400. You are responsible for ensuring that NCAR has your current contact information, including your phone number, email address, and mailing address. If your name, phone number, email address, or other information changes, notify CISL through support.ucar.edu . If CISL personnel can\u2019t reach you when they need to \u2013 about a problem caused by a job you are running, for example \u2013 the job may be killed and you will be locked out of the system. Project leads (including instructors associated with classroom allocations) are responsible for ensuring that users on their projects are aware of these responsibilities and for ensuring that authentication tokens are returned when users complete their work or students finish their classes. Project leads are responsible for any token replacement fees. Please contact the NCAR Research Computing help desk if you have questions.","title":"User responsibilities"},{"location":"getting-started/vpn-access/","text":"VPN access \u00b6 NCAR and UCAR employees who are working remotely, and some other individuals who work closely with NCAR, may need to use the virtual private network (VPN) to connect with internal resources. If you need VPN access, see your lab's system administrator about downloading the necessary client software. When you have the client software and the required authentication token or app, you can connect to the VPN as described below. Connecting with GlobalProtect \u00b6 Start by clicking the GlobalProtect icon on your desktop or taskbar. Enter your username. (Remove CIT\\ if it appears in the password field. Enter only your username.) Follow the documented procedures for using your authentication token or app (for example, CIT password and Duo push). Use gp.ucar.edu as the portal name if it doesn't appear already. A box like the one below will confirm that you are connected to the VPN. Click Disconnect when you are finished.","title":"VPN access"},{"location":"getting-started/vpn-access/#vpn-access","text":"NCAR and UCAR employees who are working remotely, and some other individuals who work closely with NCAR, may need to use the virtual private network (VPN) to connect with internal resources. If you need VPN access, see your lab's system administrator about downloading the necessary client software. When you have the client software and the required authentication token or app, you can connect to the VPN as described below.","title":"VPN access"},{"location":"getting-started/vpn-access/#connecting-with-globalprotect","text":"Start by clicking the GlobalProtect icon on your desktop or taskbar. Enter your username. (Remove CIT\\ if it appears in the password field. Enter only your username.) Follow the documented procedures for using your authentication token or app (for example, CIT password and Duo push). Use gp.ucar.edu as the portal name if it doesn't appear already. A box like the one below will confirm that you are connected to the VPN. Click Disconnect when you are finished.","title":"Connecting with GlobalProtect"},{"location":"getting-started/accounts/","text":"Authentication and security \u00b6 Individuals who are granted access to the computing and storage resources that CISL manages use their assigned user names and one of the authentication methods that are described below to log in to those systems. Passwords, apps, tokens, and PINs must be protected and may not be shared with anyone. If sharing is detected, CISL will disable the accounts of those involved. The same applies to passwords that give users access to internal UCAR systems. UCAR and NCAR computers, computing systems, and associated communications systems are to be used for official business only. By signing the required authentication acknowledgement form, you agree not to misuse these resources, and you accept responsibility for activity associated with your username and token. You also agree not to duplicate or use copyrighted or proprietary software without proper authorization. Duo two-factor authentication \u00b6 Logging in with the Duo two-factor authentication (2FA) service requires the user to enter a CIT password in conjunction with the Duo Mobile app or a landline phone. See Authenticating with Duo for details. Security overview \u00b6 All users must comply with UCAR computer security policies and procedures. See Access to and Use of Information Systems and Technology Infrastructure (staff login required). We strive to maximize the availability and value of our computer and network systems by protecting them from unauthorized access. Good security practices help prevent data loss or corruption, malicious activity, and loss of computer time. As a user, you have an important role in ensuring the security of these resources. In addition to protecting the passwords, PINS, and tokens that give you access to our systems, we ask that you do the following: Loss, theft, or compromise \u00b6 Loss, theft, or compromise of a YubiKey must be reported within 48 hours to the Research Computing Help Desk at x2400 (303-497-2400). Quick reporting will help the organization minimize security risk. Protecting your Duo app or YubiKey token \u00b6 You must protect your Duo or YubiKey solution by agreeing to the following: Your YubiKey token or Duo application will remain in your custody and is for your use only; it may not be shared. You will immediately (within 48 hours) report loss of custody of your hardware authentication token to the Research Computing Help Desk at x2400 (303-497-2400). Loss of custody may be due to loss or theft. Your PIN number or CIT password may not be shared or made available in unencrypted electronic form. Compromise (disclosure of PIN number or CIT password) must be reported to the Research Computing Help Desk at x2400 (303-497-2400) and/or to the UCAR Security Operations Center at x4300 (307-996-4300). Protect your PIN \u00b6 Do not leave your PIN where others may view it, and do not affix it to your workstation or your token. Do not use the same PIN that you use for debit cards or credit cards. Try to memorize your PIN instead of writing it down. You may write it down, but do not store it with the token. If you do write it down, keep it where others cannot access it, such as in a locked desk drawer or file cabinet that only you can access. Use encryption for logging in and transferring files \u00b6 Our systems require this, but it also is good practice to use encryption for other computers and systems. Patch your systems and use anti-virus software \u00b6 This applies to any computer from which you log in to UCAR and NCAR systems. If you are using your own personal computer or another non-UCAR or non-NCAR computer, be sure that it is kept up to date with the latest software patches and anti-virus protection. If you are planning to visit UCAR and bringing your own computer, discuss wireless and guest network access with your UCAR contact before you arrive. Procedures regarding guest network access also apply to personally owned computers that UCAR and NCAR staff bring in. Be careful \u00b6 Be aware of email scams and so-called \"social engineering\" methods that hackers and fraudsters use to gain access to passwords and other information. Never give anyone your password. UCAR and NCAR system administrators will not ask you for your password via phone or email. Other cautions \u00b6 Don't run strange binaries or executables. Don't log in to sites that you receive in email or other messages, especially if the message seems urgent and you are not familiar with the site. Some malware is spread via USB flash drives, so make sure any flash drives that you use are from trusted sources.","title":"Authentication and security"},{"location":"getting-started/accounts/#authentication-and-security","text":"Individuals who are granted access to the computing and storage resources that CISL manages use their assigned user names and one of the authentication methods that are described below to log in to those systems. Passwords, apps, tokens, and PINs must be protected and may not be shared with anyone. If sharing is detected, CISL will disable the accounts of those involved. The same applies to passwords that give users access to internal UCAR systems. UCAR and NCAR computers, computing systems, and associated communications systems are to be used for official business only. By signing the required authentication acknowledgement form, you agree not to misuse these resources, and you accept responsibility for activity associated with your username and token. You also agree not to duplicate or use copyrighted or proprietary software without proper authorization.","title":"Authentication and security"},{"location":"getting-started/accounts/#duo-two-factor-authentication","text":"Logging in with the Duo two-factor authentication (2FA) service requires the user to enter a CIT password in conjunction with the Duo Mobile app or a landline phone. See Authenticating with Duo for details.","title":"Duo two-factor authentication"},{"location":"getting-started/accounts/#security-overview","text":"All users must comply with UCAR computer security policies and procedures. See Access to and Use of Information Systems and Technology Infrastructure (staff login required). We strive to maximize the availability and value of our computer and network systems by protecting them from unauthorized access. Good security practices help prevent data loss or corruption, malicious activity, and loss of computer time. As a user, you have an important role in ensuring the security of these resources. In addition to protecting the passwords, PINS, and tokens that give you access to our systems, we ask that you do the following:","title":"Security overview"},{"location":"getting-started/accounts/#loss-theft-or-compromise","text":"Loss, theft, or compromise of a YubiKey must be reported within 48 hours to the Research Computing Help Desk at x2400 (303-497-2400). Quick reporting will help the organization minimize security risk.","title":"Loss, theft, or compromise"},{"location":"getting-started/accounts/#protecting-your-duo-app-or-yubikey-token","text":"You must protect your Duo or YubiKey solution by agreeing to the following: Your YubiKey token or Duo application will remain in your custody and is for your use only; it may not be shared. You will immediately (within 48 hours) report loss of custody of your hardware authentication token to the Research Computing Help Desk at x2400 (303-497-2400). Loss of custody may be due to loss or theft. Your PIN number or CIT password may not be shared or made available in unencrypted electronic form. Compromise (disclosure of PIN number or CIT password) must be reported to the Research Computing Help Desk at x2400 (303-497-2400) and/or to the UCAR Security Operations Center at x4300 (307-996-4300).","title":"Protecting your Duo app or YubiKey token"},{"location":"getting-started/accounts/#protect-your-pin","text":"Do not leave your PIN where others may view it, and do not affix it to your workstation or your token. Do not use the same PIN that you use for debit cards or credit cards. Try to memorize your PIN instead of writing it down. You may write it down, but do not store it with the token. If you do write it down, keep it where others cannot access it, such as in a locked desk drawer or file cabinet that only you can access.","title":"Protect your PIN"},{"location":"getting-started/accounts/#use-encryption-for-logging-in-and-transferring-files","text":"Our systems require this, but it also is good practice to use encryption for other computers and systems.","title":"Use encryption for logging in and transferring files"},{"location":"getting-started/accounts/#patch-your-systems-and-use-anti-virus-software","text":"This applies to any computer from which you log in to UCAR and NCAR systems. If you are using your own personal computer or another non-UCAR or non-NCAR computer, be sure that it is kept up to date with the latest software patches and anti-virus protection. If you are planning to visit UCAR and bringing your own computer, discuss wireless and guest network access with your UCAR contact before you arrive. Procedures regarding guest network access also apply to personally owned computers that UCAR and NCAR staff bring in.","title":"Patch your systems and use anti-virus software"},{"location":"getting-started/accounts/#be-careful","text":"Be aware of email scams and so-called \"social engineering\" methods that hackers and fraudsters use to gain access to passwords and other information. Never give anyone your password. UCAR and NCAR system administrators will not ask you for your password via phone or email.","title":"Be careful"},{"location":"getting-started/accounts/#other-cautions","text":"Don't run strange binaries or executables. Don't log in to sites that you receive in email or other messages, especially if the message seems urgent and you are not familiar with the site. Some malware is spread via USB flash drives, so make sure any flash drives that you use are from trusted sources.","title":"Other cautions"},{"location":"getting-started/accounts/cit-passwords/","text":"CIT passwords \u00b6 CIT Password Portal \u00b6 If you enrolled in the password management portal and have forgotten or need to reset your password, you can reset it here: CIT Password Portal . Non-staff \u00b6 Users who need to get a CIT password must call 303-497-2400 for assistance. If you have already submitted a password request but have not yet returned the required token authorization form, please do so to complete the process. UCAR/NCAR staff \u00b6 Either log into the CIT Password Portal or contact your divisional or laboratory sysadmin for assistance.","title":"CIT Passwords"},{"location":"getting-started/accounts/cit-passwords/#cit-passwords","text":"","title":"CIT passwords"},{"location":"getting-started/accounts/cit-passwords/#cit-password-portal","text":"If you enrolled in the password management portal and have forgotten or need to reset your password, you can reset it here: CIT Password Portal .","title":"CIT Password Portal"},{"location":"getting-started/accounts/cit-passwords/#non-staff","text":"Users who need to get a CIT password must call 303-497-2400 for assistance. If you have already submitted a password request but have not yet returned the required token authorization form, please do so to complete the process.","title":"Non-staff"},{"location":"getting-started/accounts/cit-passwords/#ucarncar-staff","text":"Either log into the CIT Password Portal or contact your divisional or laboratory sysadmin for assistance.","title":"UCAR/NCAR staff"},{"location":"getting-started/accounts/systems-accounting-manager/","text":"Systems Accounting Manager \u00b6 The Systems Accounting Manager ( SAM ) enables users to manage their system preferences and get reports on charges for using the computational and storage resources that CISL manages. User preferences \u00b6 Log in at sam.ucar.edu and select User , then Preferences . From there, you can: Change your primary group if you belong to more than one UNIX group for using NCAR supercomputing resources. Specify your default login shell for the systems to which you have access. See what your home directory is on each system. SAM reports \u00b6 Log in to sam.ucar.edu and you will see the following choices on the Reports menu: My Account Statements My 30/90 Day Use Project Search If you are authorized to charge usage to multiple projects, you will see them listed when you select either of the first two report types. Select one of the projects listed to get information. NCAR divisional users often have access to numerous projects, while individual university users most often have just one or a few. In either case, use Project Search to: go directly to a report on a specified project, or search by username to see all projects with which you are associated. My Account Statements \u00b6 Your Account Statement includes an overall report on the status of your project\u2019s computing and storage allocation and the usage associated with it. If you are authorized to charge usage to more than one project, you will have an Account Statement for each project. The overall usage report on project activity shows your allocations\u2019 start and end dates, amounts allocated, and remaining balances. The \"Activity\" link at the end of each line reveals more details: the project\u2019s allocation history, a monthly summary of job charges, and other activity, such as refunds. You can select a month and then view or download the individual job records. Another table includes additional information regarding your project\u2019s status in relation to any 30- and 90-day usage thresholds that apply and to any related projects. This is most common for NCAR users on divisional projects. Your own statement may show lines for multiple projects or subprojects, as is often the case for NCAR divisional allocations. My 30/90 Day Use \u00b6 This selection lets you focus on your usage in relation to any 30- and 90-day usage thresholds that apply. Again, this is most common for NCAR divisional projects. Project Search \u00b6 You can search by individual project code and get an Account Statement as described above. If you search by your username, you will see a list of any projects you are associated with and you can select any of them to get an Account Statement.","title":"SAM"},{"location":"getting-started/accounts/systems-accounting-manager/#systems-accounting-manager","text":"The Systems Accounting Manager ( SAM ) enables users to manage their system preferences and get reports on charges for using the computational and storage resources that CISL manages.","title":"Systems Accounting Manager"},{"location":"getting-started/accounts/systems-accounting-manager/#user-preferences","text":"Log in at sam.ucar.edu and select User , then Preferences . From there, you can: Change your primary group if you belong to more than one UNIX group for using NCAR supercomputing resources. Specify your default login shell for the systems to which you have access. See what your home directory is on each system.","title":"User preferences"},{"location":"getting-started/accounts/systems-accounting-manager/#sam-reports","text":"Log in to sam.ucar.edu and you will see the following choices on the Reports menu: My Account Statements My 30/90 Day Use Project Search If you are authorized to charge usage to multiple projects, you will see them listed when you select either of the first two report types. Select one of the projects listed to get information. NCAR divisional users often have access to numerous projects, while individual university users most often have just one or a few. In either case, use Project Search to: go directly to a report on a specified project, or search by username to see all projects with which you are associated.","title":"SAM reports"},{"location":"getting-started/accounts/systems-accounting-manager/#my-account-statements","text":"Your Account Statement includes an overall report on the status of your project\u2019s computing and storage allocation and the usage associated with it. If you are authorized to charge usage to more than one project, you will have an Account Statement for each project. The overall usage report on project activity shows your allocations\u2019 start and end dates, amounts allocated, and remaining balances. The \"Activity\" link at the end of each line reveals more details: the project\u2019s allocation history, a monthly summary of job charges, and other activity, such as refunds. You can select a month and then view or download the individual job records. Another table includes additional information regarding your project\u2019s status in relation to any 30- and 90-day usage thresholds that apply and to any related projects. This is most common for NCAR users on divisional projects. Your own statement may show lines for multiple projects or subprojects, as is often the case for NCAR divisional allocations.","title":"My Account Statements"},{"location":"getting-started/accounts/systems-accounting-manager/#my-3090-day-use","text":"This selection lets you focus on your usage in relation to any 30- and 90-day usage thresholds that apply. Again, this is most common for NCAR divisional projects.","title":"My 30/90 Day Use"},{"location":"getting-started/accounts/systems-accounting-manager/#project-search","text":"You can search by individual project code and get an Account Statement as described above. If you search by your username, you will see a list of any projects you are associated with and you can select any of them to get an Account Statement.","title":"Project Search"},{"location":"getting-started/accounts/duo/","text":"Authenticating with Duo \u00b6 Danger NCAR Duo Authentication is export-controlled. Taking the app on your phone to Cuba, Iran, Syria, North Korea, or Sudan is strictly prohibited. Overview \u00b6 Logging in with Duo two-factor authentication (2FA) requires you to enter a CIT password and then use a Duo-configured device to confirm your identity. Best practice recommendation: Use a screen lock on your mobile device to increase security. Three ways of logging in with Duo: Push notification (preferred) The app sends a request (a \"push\" notification) to your phone or tablet, asking you to approve or deny the login request. Rolling passcode When you can't receive a push notification, enter both your CIT password and a numerical passcode from the Duo app, separated by a comma. Example: password,passcode Phone callback Enter your CIT password and the word \"phone,\" separated by a comma, then follow instructions you receive in a phone call. Example: password,phone The examples below use the push notification method of authentication. See How to Use Append Mode for more information on other methods. Getting started with Duo \u00b6 To get started, contact the NCAR Research Computing help desk to request enrollment (and to get a CIT password if you don't already have one). CISL will send you a link for setting up a Duo account. During setup, Duo asks some questions about the device you want to use. Smartphone and tablet users are asked to download this free Duo Mobile app . When your setup is complete, follow the instructions below to log in to the system, such as Cheyenne, the NCAR virtual private network, or others that accept Duo 2FA. Logging in with Duo \u00b6 HPC and SSH logins \u00b6 To log in to a system like Derecho: Enter your ssh command . Enter your CIT password where a token response is requested. The Duo App will send a \"push\" notification to your phone or tablet, asking you to approve or deny the login request. When you approve the request, you will be logged in. Other application logins \u00b6 Duo authentication with other systems is somewhat different. Logging on to the NCAR virtual private network ( VPN ) is one example. You will: Enter your username. Enter your CIT password. You may get an automatic Duo Push, or select Send Me a Push from the Duo screen. The Duo App will send a push notification to your phone or tablet, asking you to approve or deny the login request. When you approve the request, you will be logged in. Duo Device Portal \u00b6 The Duo Device Portal is where you can change device settings, add new devices (a new smartphone, tablet or landline), or update your preferred contact methods. You can also choose to have Duo send you a push automatically after you enter your CIT password. Look for \"When I log in\" after you sign in to the portal. Changing smartphone \u00b6 When you replace your smartphone and need to use it to authenticate, use one of the following methods to get your new phone up and running with Duo Mobile: Recommended: Duo Instant Restore , a feature for recovering Duo-protected accounts. Alternative: Go to the Duo Device Portal . Choose Call Me . Even if your phone number hasn't changed, Duo needs to call your new phone to complete the setup process. User guides and related links \u00b6 For additional information, see the following links or contact the NCAR Research Computing help desk for assistance: Common issues Duo Guide to Two-Factor Authentication Duo Travel Guide Duo Quick Sheet","title":"Authenticating with Duo"},{"location":"getting-started/accounts/duo/#authenticating-with-duo","text":"Danger NCAR Duo Authentication is export-controlled. Taking the app on your phone to Cuba, Iran, Syria, North Korea, or Sudan is strictly prohibited.","title":"Authenticating with Duo"},{"location":"getting-started/accounts/duo/#overview","text":"Logging in with Duo two-factor authentication (2FA) requires you to enter a CIT password and then use a Duo-configured device to confirm your identity. Best practice recommendation: Use a screen lock on your mobile device to increase security. Three ways of logging in with Duo: Push notification (preferred) The app sends a request (a \"push\" notification) to your phone or tablet, asking you to approve or deny the login request. Rolling passcode When you can't receive a push notification, enter both your CIT password and a numerical passcode from the Duo app, separated by a comma. Example: password,passcode Phone callback Enter your CIT password and the word \"phone,\" separated by a comma, then follow instructions you receive in a phone call. Example: password,phone The examples below use the push notification method of authentication. See How to Use Append Mode for more information on other methods.","title":"Overview"},{"location":"getting-started/accounts/duo/#getting-started-with-duo","text":"To get started, contact the NCAR Research Computing help desk to request enrollment (and to get a CIT password if you don't already have one). CISL will send you a link for setting up a Duo account. During setup, Duo asks some questions about the device you want to use. Smartphone and tablet users are asked to download this free Duo Mobile app . When your setup is complete, follow the instructions below to log in to the system, such as Cheyenne, the NCAR virtual private network, or others that accept Duo 2FA.","title":"Getting started with Duo"},{"location":"getting-started/accounts/duo/#logging-in-with-duo","text":"","title":"Logging in with Duo"},{"location":"getting-started/accounts/duo/#hpc-and-ssh-logins","text":"To log in to a system like Derecho: Enter your ssh command . Enter your CIT password where a token response is requested. The Duo App will send a \"push\" notification to your phone or tablet, asking you to approve or deny the login request. When you approve the request, you will be logged in.","title":"HPC and SSH logins"},{"location":"getting-started/accounts/duo/#other-application-logins","text":"Duo authentication with other systems is somewhat different. Logging on to the NCAR virtual private network ( VPN ) is one example. You will: Enter your username. Enter your CIT password. You may get an automatic Duo Push, or select Send Me a Push from the Duo screen. The Duo App will send a push notification to your phone or tablet, asking you to approve or deny the login request. When you approve the request, you will be logged in.","title":"Other application logins"},{"location":"getting-started/accounts/duo/#duo-device-portal","text":"The Duo Device Portal is where you can change device settings, add new devices (a new smartphone, tablet or landline), or update your preferred contact methods. You can also choose to have Duo send you a push automatically after you enter your CIT password. Look for \"When I log in\" after you sign in to the portal.","title":"Duo Device Portal"},{"location":"getting-started/accounts/duo/#changing-smartphone","text":"When you replace your smartphone and need to use it to authenticate, use one of the following methods to get your new phone up and running with Duo Mobile: Recommended: Duo Instant Restore , a feature for recovering Duo-protected accounts. Alternative: Go to the Duo Device Portal . Choose Call Me . Even if your phone number hasn't changed, Duo needs to call your new phone to complete the setup process.","title":"Changing smartphone"},{"location":"getting-started/accounts/duo/#user-guides-and-related-links","text":"For additional information, see the following links or contact the NCAR Research Computing help desk for assistance: Common issues Duo Guide to Two-Factor Authentication Duo Travel Guide Duo Quick Sheet","title":"User guides and related links"},{"location":"getting-started/accounts/duo/enrolling/","text":"Enrolling your phone or tablet for Duo 2FA \u00b6 Duo Mobile is an app that runs on your smartphone and helps you authenticate quickly and easily with Duo two-factor authentication (2FA). You will still be able to log in using a phone call or text message without the app, but for the best experience, we recommend using Duo Mobile with a smartphone. You can also enroll a landline telephone or iOS/Android tablets. After you request enrollment and receive an email from Duo Security, follow these steps to enroll your device. Step 1: Click Start setup \u00b6 Click the personalized enrollment link in your email from Duo Security, then click Start setup . Step 2: Enter your phone number \u00b6 Select your country from the drop-down list and enter your phone number. Use the number of the smartphone or cell phone that you'll have with you when you're logging in to a Duo-protected service. You can enter your desk phone number if you don\u2019t have a cell phone. Double-check the phone number to make sure you entered it correctly, check the box to confirm that the number is correct, and click Continue . (If you are enrolling a tablet, you aren't prompted to enter a phone number.) Step 3: Choose your platform \u00b6 Choose the type of phone you have and click Continue . Step 4: Install the Duo Mobile app \u00b6 Follow the platform-specific instructions on the screen to install Duo Mobile. After installing the app, return to the enrollment window and click I have Duo Mobile installed . Step 5: Activate Duo Mobile \u00b6 Activating the app links it to your account so you can use it for authentication. To activate Duo Mobile on an iPhone, Android, or Windows Phone, scan the barcode with the app's built-in barcode scanner. Follow the platform-specific instructions for your device. The \"Continue\" button is clickable after you scan the barcode. If you can\u2019t scan the barcode, click Email me an activation link instead and follow the instructions. Step 6: Configure device options (optional) \u00b6 By default Duo will ask you to choose how you want to authenticate each time you log in \u2013 with a push, phone call, or passcode, for example. The default is recommended, but you can change the setting so you automatically receive a push or a call instead of being asked every time. To do that, make your selection from the dropdown menu and click Saved . Step 7: Finish \u00b6 Choose an optional authentication method from the \u201cWhen I log in\u201d dropdown menu, then click Finish Enrollment . Duo automatically sends an authentication request via a push notification to the Duo Mobile app on your smartphone or a phone call to your device (depending on your selection). Congratulations! \u00b6 Your device is ready to approve Duo push authentication requests. Click Send me a Push to give it a try. All you need to do is tap Approve on the Duo login request you get on your phone.","title":"Enrolling with Duo"},{"location":"getting-started/accounts/duo/enrolling/#enrolling-your-phone-or-tablet-for-duo-2fa","text":"Duo Mobile is an app that runs on your smartphone and helps you authenticate quickly and easily with Duo two-factor authentication (2FA). You will still be able to log in using a phone call or text message without the app, but for the best experience, we recommend using Duo Mobile with a smartphone. You can also enroll a landline telephone or iOS/Android tablets. After you request enrollment and receive an email from Duo Security, follow these steps to enroll your device.","title":"Enrolling your phone or tablet for Duo 2FA"},{"location":"getting-started/accounts/duo/enrolling/#step-1-click-start-setup","text":"Click the personalized enrollment link in your email from Duo Security, then click Start setup .","title":"Step 1: Click Start setup"},{"location":"getting-started/accounts/duo/enrolling/#step-2-enter-your-phone-number","text":"Select your country from the drop-down list and enter your phone number. Use the number of the smartphone or cell phone that you'll have with you when you're logging in to a Duo-protected service. You can enter your desk phone number if you don\u2019t have a cell phone. Double-check the phone number to make sure you entered it correctly, check the box to confirm that the number is correct, and click Continue . (If you are enrolling a tablet, you aren't prompted to enter a phone number.)","title":"Step 2: Enter your phone number"},{"location":"getting-started/accounts/duo/enrolling/#step-3-choose-your-platform","text":"Choose the type of phone you have and click Continue .","title":"Step 3: Choose your platform"},{"location":"getting-started/accounts/duo/enrolling/#step-4-install-the-duo-mobile-app","text":"Follow the platform-specific instructions on the screen to install Duo Mobile. After installing the app, return to the enrollment window and click I have Duo Mobile installed .","title":"Step 4: Install the Duo Mobile app"},{"location":"getting-started/accounts/duo/enrolling/#step-5-activate-duo-mobile","text":"Activating the app links it to your account so you can use it for authentication. To activate Duo Mobile on an iPhone, Android, or Windows Phone, scan the barcode with the app's built-in barcode scanner. Follow the platform-specific instructions for your device. The \"Continue\" button is clickable after you scan the barcode. If you can\u2019t scan the barcode, click Email me an activation link instead and follow the instructions.","title":"Step 5: Activate Duo Mobile"},{"location":"getting-started/accounts/duo/enrolling/#step-6-configure-device-options-optional","text":"By default Duo will ask you to choose how you want to authenticate each time you log in \u2013 with a push, phone call, or passcode, for example. The default is recommended, but you can change the setting so you automatically receive a push or a call instead of being asked every time. To do that, make your selection from the dropdown menu and click Saved .","title":"Step 6: Configure device options (optional)"},{"location":"getting-started/accounts/duo/enrolling/#step-7-finish","text":"Choose an optional authentication method from the \u201cWhen I log in\u201d dropdown menu, then click Finish Enrollment . Duo automatically sends an authentication request via a push notification to the Duo Mobile app on your smartphone or a phone call to your device (depending on your selection).","title":"Step 7: Finish"},{"location":"getting-started/accounts/duo/enrolling/#congratulations","text":"Your device is ready to approve Duo push authentication requests. Click Send me a Push to give it a try. All you need to do is tap Approve on the Duo login request you get on your phone.","title":"Congratulations!"},{"location":"nhug/","text":"Welcome to the NCAR HPC User Group (NHUG) \u00b6 The NCAR HPC Users\u2019 Group, NHUG, welcomes participation from all NCAR HPC users! The NCAR HPC Users\u2019 Group, NHUG, is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community. NHUG holds monthly meetings featuring different HPC-related topics. The goal of these meetings is to foster a community around HPC-related issues and increase the collaboration among all of the NCAR HPC user community. We encourage you to join our monthly meetings. NHUG Communication Channels Join NHUG email list to receive calendar invites for upcoming monthly meetings and other important announcements. We also encourage you to join NCAR HPC User Group NCAR HPC User Group Slack channel channel which we will use for focussed interactions during and outside the monthly meetings. Join NHUG Email List Join NHUG Slack Channel Contribute to Future NHUG Topics If you have suggestions for future meeting topics or ideas that can enhance our community, we'd love to hear from you. Please submit your ideas using the form below to help shape our upcoming sessions. Submit Your Ideas","title":"Welcome to the NCAR HPC User Group (NHUG)"},{"location":"nhug/#welcome-to-the-ncar-hpc-user-group-nhug","text":"The NCAR HPC Users\u2019 Group, NHUG, welcomes participation from all NCAR HPC users! The NCAR HPC Users\u2019 Group, NHUG, is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community. NHUG holds monthly meetings featuring different HPC-related topics. The goal of these meetings is to foster a community around HPC-related issues and increase the collaboration among all of the NCAR HPC user community. We encourage you to join our monthly meetings. NHUG Communication Channels Join NHUG email list to receive calendar invites for upcoming monthly meetings and other important announcements. We also encourage you to join NCAR HPC User Group NCAR HPC User Group Slack channel channel which we will use for focussed interactions during and outside the monthly meetings. Join NHUG Email List Join NHUG Slack Channel Contribute to Future NHUG Topics If you have suggestions for future meeting topics or ideas that can enhance our community, we'd love to hear from you. Please submit your ideas using the form below to help shape our upcoming sessions. Submit Your Ideas","title":"Welcome to the NCAR HPC User Group (NHUG)"},{"location":"nhug/upcoming-events/","text":"NHUG Upcoming Events \u00b6","title":"NHUG Upcoming Events"},{"location":"nhug/upcoming-events/#nhug-upcoming-events","text":"","title":"NHUG Upcoming Events"},{"location":"nhug/archive/","text":"","title":"Index"},{"location":"pbs/","text":"Starting and managing jobs with PBS \u00b6 About this page This documentation provides information for how to use PBS Pro to submit and manage interactive jobs and batch jobs on NCAR systems. The basic PBS commands are the same on each cluster, but refer to these system-specific pages for details that are unique to each of them, including hardware specifications, software, and job-submission queues and procedures: Derecho Casper Computerized batch processing is a method of running software programs called jobs in batches automatically. While users are required to submit the jobs, no other interaction by the user is required to process the batch. Batches may automatically be run at scheduled times as well as being run contingent on the availability of computer resources. For additional background, see Batch Computng Overview . NCAR's HPC resources use the Portable Batch System as implemented in Altair's PBS Pro across shared resources. Job scripts \u00b6 Job scripts form the basis of batch jobs . A job script is simply a text file with instructions of the work to execute. Job scripts are usually written in bash or tcsh and thus mimic commands a user would execute interactively through a shell; but instead are executed on specific resources allocated by the scheduler when available. Scripts can also be written in other languages - commonly Python. See our job scripts page for a detailed discussion of job scripts and examples. Submitting jobs \u00b6 In the examples that follow, script_name , job.pbs etc... represent a job script files submitted for batch execution. PBS Pro is used to schedule both interactive jobs and batch compute jobs. Detailed examples of how to start both types of jobs are included in the documentation (see links above) for each individual system. Commands for starting interactive jobs are specific to individual systems. The basic command for starting a batch job, however, is the same. To submit a batch job, use the qsub command followed by the name of your PBS batch script file. qsub script_name Propagating environment settings \u00b6 Some users find it useful to set environment variables in their login environment that can be temporarily used for multiple batch jobs without modifying the job script. This practice can be particularly useful during iterative development and debugging work. PBS has two approaches to propagation: Specific variables can be forwarded to the job upon request. The entire environment can be forwarded to the job. In general, the first approach is preferred because the second may have unintended consequences. These settings are controlled by qsub arguments that can be used at the command line or as directives within job scripts. Here are examples of both approaches: # Selectively forward runtime variables to the job (lower-case v) qsub -v DEBUG = true,CASE_NAME job.pbs When you use the selective option (lower-case v ), you can either specify only the variable name to propagate the current value (as in CASE_NAME in the example), or you can explicitly set it to a given value at submission time (as in DEBUG ). # Forward the entire environment to the job (upper-case V) qsub -V job.pbs Do not use full propagation when peer-scheduling jobs. Doing so will cause libraries and binaries to be inherited via variables like PATH and LD_LIBRARY_PATH . These inherited settings will cause applications to break, and may render the job completely unusable. Managing jobs \u00b6 Here are some of the most useful commands for managing and monitoring jobs that have been launched with PBS. Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Derecho if you want to interact with a job on Derecho. Run any command followed by -h to get help, as in qhist -h . qdel \u00b6 Run qdel with the job ID to kill a pending or running job. qdel jobID Kill all of your own pending or running jobs. (Be sure to use backticks as shown.) qdel ` qselect -u $USER ` qhist \u00b6 Run qhist for information on finished jobs. qhist -u $USER Your output will include jobs that finished on the current day unless you specify the number ( N ) of days to include. qhist -u $USER -d N Your output will be similar to this, with Mem(GB) and CPU(%) indicating approximate total memory usage per job and average CPU usage per core per job: Job ID User Queue Nodes NCPUs Finish RMem(GB) Mem(GB) CPU(%) Elap(h) 2426690 stormyk regular 1 1 05-1527 - 0.3 75.0 0.09 2426693 stormyk regular 1 1 05-1527 - 0.1 90.0 0.09 2426541 stormyk regular 1 1 05-1523 - 0.1 83.0 0.03 2426542 stormyk regular 1 1 05-1524 - 0.1 70.0 0.04 2426683 stormyk regular 1 1 05-1523 - 0.1 0.0 0.02 2426444 stormyk regular 1 1 05-1522 - 0.1 19.0 0.02 2426435 stormyk regular 1 1 05-1522 - 0.1 13.0 0.02 You can obtain additional job details using qhist -w for wide output, or customize the output - see qhist --format=help for a list of options. The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed. qhist -u $USER -r x0 qstat \u00b6 Run this to see the status of all of your own unfinished jobs. qstat -u $USER Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 NDS for nodes, TSK for tasks, and so on. In the status (S) column, most jobs are either queued (Q) or running (R) . Sometimes jobs are held (H) , which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job. Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ------ -------- ----- ------- ------ --- --- ------ ----- - ---- 657237.chadmin apatelsm economy ens603 46100 60 216 -- 02:30 R 01:24 657238.chadmin apatelsm regular ens605 -- 1 36 -- 00:05 H -- 657466.chadmin apatelsm economy ens701 5189 60 216 -- 02:30 R 00:46 657467.chadmin apatelsm regular ens703 -- 1 36 -- 00:10 H -- Following are examples of qstat with some other commonly used options and arguments. Get a long-form summary of the status of an unfinished job. qstat -f jobID Warning Use the above command only sparingly; it places a high load on PBS. Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours). qstat -x jobID Get information about unfinished jobs in a specified execution queue. qstat queue_name See job activity by queue (e.g., pending, running) in terms of numbers of jobs. qstat -Q Display information for all of your pending, running, and finished jobs. qstat -x -u $USER Query jobs running on one system by specifying the system as shown here. (Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s ) qstat -w -u $USER @derecho Tip Query jobs running on one system by specifying @derecho , @cheyenne , or @casper from either system as shown here. qstat -w -u $USER @derecho qstat -w -u $USER @casper Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s Job Dependencies \u00b6 It is possible to schedule jobs to run based on the status of other jobs. For example, you might schedule a preprocessing job to run; start a computation job when the preprocessing job is complete; then start a post-processing job when the computation is done. One way to schedule such a series or chain of jobs is to use qsub -W [job-dependency-expression] to specify the job dependencies you need. This can also be accomplished by submitting subsequent jobs from inside another job. PBS job dependencies ( -W depend ) \u00b6 Let's say you have you have three scripts to submit and run consecutively: pre.pbs : a preprocessing job main.pbs : a computation job post.pbs : a post-processing job The main job can be run only when the preprocessing job finishes, and the post-processing job can be run only when the computation job finishes. Submit the first job, placing it on hold. (If it starts before the dependent jobs are submitted, the dependent jobs might never start.): JOBID1 = $( qsub -h pre.pbs ) Make starting of the second job dependent on successful completion of the first: JOBID2 = $( qsub -W depend = afterok: $JOBID1 main.pbs ) Make starting of the post-processing job dependent on successful completion of the second job: JOBID3 = $( qsub -W depend = afterok: $JOBID2 post.pbs ) Release the first job to initiate the sequence: qrls $JOBID1 (Strictly speaking, JOBID3 is not used in these examples and can be omitted if desired, but the sequence listed above is easily extensible to another dependent job.) The complete sequence is shown below for additional clarity: bash tcsh JOBID1 = $( qsub -h pre.pbs ) JOBID2 = $( qsub -W depend = afterok: $JOBID1 main.pbs ) JOBID3 = $( qsub -W depend = afterok: $JOBID2 post.pbs ) qrls $JOBID1 set JOBID1 = ` qsub -h pre.pbs ` set JOBID2 = ` qsub -W depend = afterok: $JOBID1 main.pbs ` set JOBID3 = ` qsub -W depend = afterok: $JOBID2 post.pbs ` qrls $JOBID1 In the example above, the clause afterok is used to indicate the subsequent jobs should only begin after the preceding job completes successfully. This is likely the most common use case, however other clauses are available, and jobs may depend on multiple predecessors. Some of the more common dependency clauses are listed below, where <arg_list> is usually a single PBS job id as shown above, but can be a colon-separated list of IDs if appropriate. See man qsub for a full listing and additional details. Clause Effect after:<arg_list> This job may be scheduled for execution at any point after all jobs in <arg_list> have started execution. afterok:<arg_list> This job may be scheduled for execution only after all jobs in <arg_list> have terminated with no errors. afternotok:<arg_list> This job may be scheduled for execution only after all jobs in <arg_list> have terminated with errors. afterany:<arg_list> This job may be scheduled for execution after all jobs in <arg_list> have finished execution, with any exit status (with or without errors.) This job will not run if a job in the <arg_list> was deleted without ever having been run. before:<arg_list> Jobs in <arg_list> may begin execution once this job has begun execution. beforeok:<arg_list> Jobs in <arg_list> may begin execution once this job terminates without errors. beforenotok:<arg_list> If this job terminates execution with errors, jobs in <arg_list> may begin. beforeany:<arg_list> Jobs in <arg_list> may begin execution once this job terminates execution, with or without errors. Nested jobs \u00b6 Peer Scheduling scheduling between systems \u00b6 Peer Scheduling scheduling between HPC Systems Cheyenne, Derecho, and Casper use the PBS scheduler, and each system has its own dedicated PBS server to manage job submission and execution. These \"peer\" servers can share data between each other, and jobs can be submitted from one system to another . It is also possible to create dependencies between jobs on each server, enabling simulation-analysis workflows that target the appropriate system for each task. Submitting a job to a peer system \u00b6 To submit a job to a queue on a peer server, you need to append the name of the server to the queue directive in your job script. The names of the PBS servers are: System PBS server name Cheyenne chadmin1.ib0.cheyenne.ucar.edu Casper casper-pbs Derecho desched1 Examples \u00b6 Casper to Derecho Casper to Cheyenne Derecho to Casper You want to submit to the Derecho \"main\" queue from a Casper login node or compute node. Append the Derecho server name as follows in your job script when specifying the queue: #PBS -q main@desched1 You want to submit to the Cheyenne \"regular\" queue from a Casper login node or compute node. Append the Cheyenne server name as follows in your job script when specifying the queue: #PBS -q regular@chadmin1.ib0.cheyenne.ucar.edu You want to submit a job to Casper from Derecho. Append the Casper server name as follows in your job script when specifying the destination: #PBS -q casper@casper-pbs The server-specific queue names will be understood by both PBS servers, so if you will want to submit the same script at times from either Cheyenne or Casper, always append the server name to your queue . The qinteractive and execcasper scripts, which start interactive jobs on Cheyenne and Casper respectively, will adjust the queue name for you to include the server, so you do not need to append the server name manually for interactive jobs . Querying jobs \u00b6 You can use qstat to query jobs from peer servers by including the server name in your field of interest. You can also use the system names noted above when running qstat. Note that the separator character differs for jobs ( . ) and queues ( @ ). qstat 123456.chadmin1.ib0.cheyenne.ucar.edu qstat regular@cheyenne qstat 654321.casper qstat @casper-pbs qstat @desched1 Creating dependencies between peer-scheduled jobs \u00b6 Creating job dependencies between submissions on peer servers is straightforward; there is nothing unique about this workflow in PBS. As with all jobs, pay close attention to specifying the destination server in your queue designations. The job IDs returned by PBS include the server name, so you do not need to append a server to the job ID you specify in your dependency argument. Here is an example of a workflow that runs a simulation on Cheyenne and, if successful, then runs a post-processing job on Casper. Thanks to peer scheduling, these jobs can be submitted from either Cheyenne or Casper login nodes. bash tcsh JID = $( qsub -q economy@chadmin1.ib0.cheyenne.ucar.edu run_model.pbs ) qsub -q casper@casper-pbs -W depend = afterok: $JID run_postprocess.pbs set JID = ` qsub -q economy@chadmin1.ib0.cheyenne.ucar.edu run_model.pbs ` qsub -q casper@casper-pbs -W depend = afterok: $JID run_postprocess.pbs","title":"Starting and managing jobs with PBS"},{"location":"pbs/#starting-and-managing-jobs-with-pbs","text":"About this page This documentation provides information for how to use PBS Pro to submit and manage interactive jobs and batch jobs on NCAR systems. The basic PBS commands are the same on each cluster, but refer to these system-specific pages for details that are unique to each of them, including hardware specifications, software, and job-submission queues and procedures: Derecho Casper Computerized batch processing is a method of running software programs called jobs in batches automatically. While users are required to submit the jobs, no other interaction by the user is required to process the batch. Batches may automatically be run at scheduled times as well as being run contingent on the availability of computer resources. For additional background, see Batch Computng Overview . NCAR's HPC resources use the Portable Batch System as implemented in Altair's PBS Pro across shared resources.","title":"Starting and managing jobs with PBS"},{"location":"pbs/#job-scripts","text":"Job scripts form the basis of batch jobs . A job script is simply a text file with instructions of the work to execute. Job scripts are usually written in bash or tcsh and thus mimic commands a user would execute interactively through a shell; but instead are executed on specific resources allocated by the scheduler when available. Scripts can also be written in other languages - commonly Python. See our job scripts page for a detailed discussion of job scripts and examples.","title":"Job scripts"},{"location":"pbs/#submitting-jobs","text":"In the examples that follow, script_name , job.pbs etc... represent a job script files submitted for batch execution. PBS Pro is used to schedule both interactive jobs and batch compute jobs. Detailed examples of how to start both types of jobs are included in the documentation (see links above) for each individual system. Commands for starting interactive jobs are specific to individual systems. The basic command for starting a batch job, however, is the same. To submit a batch job, use the qsub command followed by the name of your PBS batch script file. qsub script_name","title":"Submitting jobs"},{"location":"pbs/#propagating-environment-settings","text":"Some users find it useful to set environment variables in their login environment that can be temporarily used for multiple batch jobs without modifying the job script. This practice can be particularly useful during iterative development and debugging work. PBS has two approaches to propagation: Specific variables can be forwarded to the job upon request. The entire environment can be forwarded to the job. In general, the first approach is preferred because the second may have unintended consequences. These settings are controlled by qsub arguments that can be used at the command line or as directives within job scripts. Here are examples of both approaches: # Selectively forward runtime variables to the job (lower-case v) qsub -v DEBUG = true,CASE_NAME job.pbs When you use the selective option (lower-case v ), you can either specify only the variable name to propagate the current value (as in CASE_NAME in the example), or you can explicitly set it to a given value at submission time (as in DEBUG ). # Forward the entire environment to the job (upper-case V) qsub -V job.pbs Do not use full propagation when peer-scheduling jobs. Doing so will cause libraries and binaries to be inherited via variables like PATH and LD_LIBRARY_PATH . These inherited settings will cause applications to break, and may render the job completely unusable.","title":"Propagating environment settings"},{"location":"pbs/#managing-jobs","text":"Here are some of the most useful commands for managing and monitoring jobs that have been launched with PBS. Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Derecho if you want to interact with a job on Derecho. Run any command followed by -h to get help, as in qhist -h .","title":"Managing jobs"},{"location":"pbs/#qdel","text":"Run qdel with the job ID to kill a pending or running job. qdel jobID Kill all of your own pending or running jobs. (Be sure to use backticks as shown.) qdel ` qselect -u $USER `","title":"qdel"},{"location":"pbs/#qhist","text":"Run qhist for information on finished jobs. qhist -u $USER Your output will include jobs that finished on the current day unless you specify the number ( N ) of days to include. qhist -u $USER -d N Your output will be similar to this, with Mem(GB) and CPU(%) indicating approximate total memory usage per job and average CPU usage per core per job: Job ID User Queue Nodes NCPUs Finish RMem(GB) Mem(GB) CPU(%) Elap(h) 2426690 stormyk regular 1 1 05-1527 - 0.3 75.0 0.09 2426693 stormyk regular 1 1 05-1527 - 0.1 90.0 0.09 2426541 stormyk regular 1 1 05-1523 - 0.1 83.0 0.03 2426542 stormyk regular 1 1 05-1524 - 0.1 70.0 0.04 2426683 stormyk regular 1 1 05-1523 - 0.1 0.0 0.02 2426444 stormyk regular 1 1 05-1522 - 0.1 19.0 0.02 2426435 stormyk regular 1 1 05-1522 - 0.1 13.0 0.02 You can obtain additional job details using qhist -w for wide output, or customize the output - see qhist --format=help for a list of options. The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed. qhist -u $USER -r x0","title":"qhist"},{"location":"pbs/#qstat","text":"Run this to see the status of all of your own unfinished jobs. qstat -u $USER Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 NDS for nodes, TSK for tasks, and so on. In the status (S) column, most jobs are either queued (Q) or running (R) . Sometimes jobs are held (H) , which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job. Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ------ -------- ----- ------- ------ --- --- ------ ----- - ---- 657237.chadmin apatelsm economy ens603 46100 60 216 -- 02:30 R 01:24 657238.chadmin apatelsm regular ens605 -- 1 36 -- 00:05 H -- 657466.chadmin apatelsm economy ens701 5189 60 216 -- 02:30 R 00:46 657467.chadmin apatelsm regular ens703 -- 1 36 -- 00:10 H -- Following are examples of qstat with some other commonly used options and arguments. Get a long-form summary of the status of an unfinished job. qstat -f jobID Warning Use the above command only sparingly; it places a high load on PBS. Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours). qstat -x jobID Get information about unfinished jobs in a specified execution queue. qstat queue_name See job activity by queue (e.g., pending, running) in terms of numbers of jobs. qstat -Q Display information for all of your pending, running, and finished jobs. qstat -x -u $USER Query jobs running on one system by specifying the system as shown here. (Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s ) qstat -w -u $USER @derecho Tip Query jobs running on one system by specifying @derecho , @cheyenne , or @casper from either system as shown here. qstat -w -u $USER @derecho qstat -w -u $USER @casper Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s","title":"qstat"},{"location":"pbs/#job-dependencies","text":"It is possible to schedule jobs to run based on the status of other jobs. For example, you might schedule a preprocessing job to run; start a computation job when the preprocessing job is complete; then start a post-processing job when the computation is done. One way to schedule such a series or chain of jobs is to use qsub -W [job-dependency-expression] to specify the job dependencies you need. This can also be accomplished by submitting subsequent jobs from inside another job.","title":"Job Dependencies"},{"location":"pbs/#pbs-job-dependencies-w-depend","text":"Let's say you have you have three scripts to submit and run consecutively: pre.pbs : a preprocessing job main.pbs : a computation job post.pbs : a post-processing job The main job can be run only when the preprocessing job finishes, and the post-processing job can be run only when the computation job finishes. Submit the first job, placing it on hold. (If it starts before the dependent jobs are submitted, the dependent jobs might never start.): JOBID1 = $( qsub -h pre.pbs ) Make starting of the second job dependent on successful completion of the first: JOBID2 = $( qsub -W depend = afterok: $JOBID1 main.pbs ) Make starting of the post-processing job dependent on successful completion of the second job: JOBID3 = $( qsub -W depend = afterok: $JOBID2 post.pbs ) Release the first job to initiate the sequence: qrls $JOBID1 (Strictly speaking, JOBID3 is not used in these examples and can be omitted if desired, but the sequence listed above is easily extensible to another dependent job.) The complete sequence is shown below for additional clarity: bash tcsh JOBID1 = $( qsub -h pre.pbs ) JOBID2 = $( qsub -W depend = afterok: $JOBID1 main.pbs ) JOBID3 = $( qsub -W depend = afterok: $JOBID2 post.pbs ) qrls $JOBID1 set JOBID1 = ` qsub -h pre.pbs ` set JOBID2 = ` qsub -W depend = afterok: $JOBID1 main.pbs ` set JOBID3 = ` qsub -W depend = afterok: $JOBID2 post.pbs ` qrls $JOBID1 In the example above, the clause afterok is used to indicate the subsequent jobs should only begin after the preceding job completes successfully. This is likely the most common use case, however other clauses are available, and jobs may depend on multiple predecessors. Some of the more common dependency clauses are listed below, where <arg_list> is usually a single PBS job id as shown above, but can be a colon-separated list of IDs if appropriate. See man qsub for a full listing and additional details. Clause Effect after:<arg_list> This job may be scheduled for execution at any point after all jobs in <arg_list> have started execution. afterok:<arg_list> This job may be scheduled for execution only after all jobs in <arg_list> have terminated with no errors. afternotok:<arg_list> This job may be scheduled for execution only after all jobs in <arg_list> have terminated with errors. afterany:<arg_list> This job may be scheduled for execution after all jobs in <arg_list> have finished execution, with any exit status (with or without errors.) This job will not run if a job in the <arg_list> was deleted without ever having been run. before:<arg_list> Jobs in <arg_list> may begin execution once this job has begun execution. beforeok:<arg_list> Jobs in <arg_list> may begin execution once this job terminates without errors. beforenotok:<arg_list> If this job terminates execution with errors, jobs in <arg_list> may begin. beforeany:<arg_list> Jobs in <arg_list> may begin execution once this job terminates execution, with or without errors.","title":"PBS job dependencies (-W depend)"},{"location":"pbs/#nested-jobs","text":"","title":"Nested jobs"},{"location":"pbs/#peer-scheduling-scheduling-between-systems","text":"Peer Scheduling scheduling between HPC Systems Cheyenne, Derecho, and Casper use the PBS scheduler, and each system has its own dedicated PBS server to manage job submission and execution. These \"peer\" servers can share data between each other, and jobs can be submitted from one system to another . It is also possible to create dependencies between jobs on each server, enabling simulation-analysis workflows that target the appropriate system for each task.","title":"Peer Scheduling scheduling between systems"},{"location":"pbs/#submitting-a-job-to-a-peer-system","text":"To submit a job to a queue on a peer server, you need to append the name of the server to the queue directive in your job script. The names of the PBS servers are: System PBS server name Cheyenne chadmin1.ib0.cheyenne.ucar.edu Casper casper-pbs Derecho desched1","title":"Submitting a job to a peer system"},{"location":"pbs/#examples","text":"Casper to Derecho Casper to Cheyenne Derecho to Casper You want to submit to the Derecho \"main\" queue from a Casper login node or compute node. Append the Derecho server name as follows in your job script when specifying the queue: #PBS -q main@desched1 You want to submit to the Cheyenne \"regular\" queue from a Casper login node or compute node. Append the Cheyenne server name as follows in your job script when specifying the queue: #PBS -q regular@chadmin1.ib0.cheyenne.ucar.edu You want to submit a job to Casper from Derecho. Append the Casper server name as follows in your job script when specifying the destination: #PBS -q casper@casper-pbs The server-specific queue names will be understood by both PBS servers, so if you will want to submit the same script at times from either Cheyenne or Casper, always append the server name to your queue . The qinteractive and execcasper scripts, which start interactive jobs on Cheyenne and Casper respectively, will adjust the queue name for you to include the server, so you do not need to append the server name manually for interactive jobs .","title":"Examples"},{"location":"pbs/#querying-jobs","text":"You can use qstat to query jobs from peer servers by including the server name in your field of interest. You can also use the system names noted above when running qstat. Note that the separator character differs for jobs ( . ) and queues ( @ ). qstat 123456.chadmin1.ib0.cheyenne.ucar.edu qstat regular@cheyenne qstat 654321.casper qstat @casper-pbs qstat @desched1","title":"Querying jobs"},{"location":"pbs/#creating-dependencies-between-peer-scheduled-jobs","text":"Creating job dependencies between submissions on peer servers is straightforward; there is nothing unique about this workflow in PBS. As with all jobs, pay close attention to specifying the destination server in your queue designations. The job IDs returned by PBS include the server name, so you do not need to append a server to the job ID you specify in your dependency argument. Here is an example of a workflow that runs a simulation on Cheyenne and, if successful, then runs a post-processing job on Casper. Thanks to peer scheduling, these jobs can be submitted from either Cheyenne or Casper login nodes. bash tcsh JID = $( qsub -q economy@chadmin1.ib0.cheyenne.ucar.edu run_model.pbs ) qsub -q casper@casper-pbs -W depend = afterok: $JID run_postprocess.pbs set JID = ` qsub -q economy@chadmin1.ib0.cheyenne.ucar.edu run_model.pbs ` qsub -q casper@casper-pbs -W depend = afterok: $JID run_postprocess.pbs","title":"Creating dependencies between peer-scheduled jobs"},{"location":"pbs/charging/","text":"Job-submission queues and charges \u00b6 Derecho queue details \u00b6 The main queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs on Derecho. On Derecho users request a specific job priority via the #PBS -l job_priority=<regular|premium|economy> resource directive , instead of through distinct queues as was the case previously on Cheyenne. Queue name PBS job_priority Wall clock limit (hours) Charging factor Derecho queue description main premium 12 1.5 Jobs are charged at 150% of the regular rate. regular 1 Most production batch jobs run in this queue; also accepts interactive jobs. economy 0.7 Production batch jobs are charged at 70% of the regular rate. preempt 24 0.2 Jobs will only run on resources otherwise idle. Jobs may be preempted with a short grace period to make room for higher priority jobs. develop 6 1 Interactive and serial batch use for debugging and other tasks on shared 256-GB nodes. Jobs in this queue should specify memory required via -l select=...:mem=5GB up to 235GB. An individual job can use up to 256 cores or 8 GPUs across two nodes. A user can run multiple jobs in the share queue concurrently if the total number of cores or GPUs used is no more than 256 or 8 respectively. Some additional queues on the system are for dedicated purposes and accessible only to authorized users. Calculating charges \u00b6 Exclusive nodes \u00b6 Charges for use of Derecho are calculated in terms of core-hours. Jobs run in Derecho queues other than \"share\" are charged for exclusive use of the nodes by this formula: wall-clock hours \u00d7 nodes used \u00d7 cores per node \u00d7 charging factor Derecho node charging Your batch script indicates how many Derecho nodes your job will use. Fully Subscribed CPU Node Fully Subscribed CPU Node (Hybrid) Under-subscribed CPU Node In this example, you have selected 2 nodes, each of which has 128 cores, all of which will be used as MPI \"ranks.\" #PBS -l select=2:ncpus=128:mpiprocs=128 Your job will be charged for the use of 256 cores. In this example, you have selected 4 nodes, each of which has 128 cores. Each node will have 32 MPI ranks and 4 OpenMP threads. #PBS -l select=4:ncpus=128:mpiprocs=32:ompthreads=4 Your job will be charged for the use of 512 cores. In this example, you have selected 2 nodes, each of which has 128 cores. To double the available memory per rank, you elect to \"under-subscribe\" the node, that is, only use 64 cores as MPI ranks. #PBS -l select=2:ncpus=128:mpiprocs=64:mem=235GB Your job will be charged for all 256 cores, even though you are only using 128 (assuming your code has no hybrid parallel capability - e.g OpenMP or other threading). Exclusive nodes are charged by resource allocation, not utilization Most Derecho batch queues are for exclusive use, so jobs submitted to Derecho queues are charged for all 128 CPU cores on each node that is used regardless of how many CPUs are used. Requesting a Derecho CPU node for 1 hour will result in a 128 core hour charge, even if left idle by the user. Shared nodes (Casper) \u00b6 Charges for jobs that you run on a shared node, including Casper nodes, are calculated by this formula: core-seconds/3600 (core-hours) Checking and managing charges \u00b6 Users can check computing and storage charges through the CISL Systems Accounting Manager. (Go to SAM documentation or to SAM app .) If you have concerns about using your allocation most efficiently, contact the NCAR Research Computing help desk for guidance. Sometimes jobs can be configured to make better use of the processors, and you may be able to save by using a less expensive queue. CISL can refund core-hours if system failures cause jobs to fail and the failed jobs are reported promptly. Use this core-hours refund request form (login required) if you think a refund is warranted. Technical limitations prevent us from verifying refund eligibility for jobs that are more than seven days old.","title":"Queues and Charging"},{"location":"pbs/charging/#job-submission-queues-and-charges","text":"","title":"Job-submission queues and charges"},{"location":"pbs/charging/#derecho-queue-details","text":"The main queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs on Derecho. On Derecho users request a specific job priority via the #PBS -l job_priority=<regular|premium|economy> resource directive , instead of through distinct queues as was the case previously on Cheyenne. Queue name PBS job_priority Wall clock limit (hours) Charging factor Derecho queue description main premium 12 1.5 Jobs are charged at 150% of the regular rate. regular 1 Most production batch jobs run in this queue; also accepts interactive jobs. economy 0.7 Production batch jobs are charged at 70% of the regular rate. preempt 24 0.2 Jobs will only run on resources otherwise idle. Jobs may be preempted with a short grace period to make room for higher priority jobs. develop 6 1 Interactive and serial batch use for debugging and other tasks on shared 256-GB nodes. Jobs in this queue should specify memory required via -l select=...:mem=5GB up to 235GB. An individual job can use up to 256 cores or 8 GPUs across two nodes. A user can run multiple jobs in the share queue concurrently if the total number of cores or GPUs used is no more than 256 or 8 respectively. Some additional queues on the system are for dedicated purposes and accessible only to authorized users.","title":"Derecho queue details"},{"location":"pbs/charging/#calculating-charges","text":"","title":"Calculating charges"},{"location":"pbs/charging/#exclusive-nodes","text":"Charges for use of Derecho are calculated in terms of core-hours. Jobs run in Derecho queues other than \"share\" are charged for exclusive use of the nodes by this formula: wall-clock hours \u00d7 nodes used \u00d7 cores per node \u00d7 charging factor Derecho node charging Your batch script indicates how many Derecho nodes your job will use. Fully Subscribed CPU Node Fully Subscribed CPU Node (Hybrid) Under-subscribed CPU Node In this example, you have selected 2 nodes, each of which has 128 cores, all of which will be used as MPI \"ranks.\" #PBS -l select=2:ncpus=128:mpiprocs=128 Your job will be charged for the use of 256 cores. In this example, you have selected 4 nodes, each of which has 128 cores. Each node will have 32 MPI ranks and 4 OpenMP threads. #PBS -l select=4:ncpus=128:mpiprocs=32:ompthreads=4 Your job will be charged for the use of 512 cores. In this example, you have selected 2 nodes, each of which has 128 cores. To double the available memory per rank, you elect to \"under-subscribe\" the node, that is, only use 64 cores as MPI ranks. #PBS -l select=2:ncpus=128:mpiprocs=64:mem=235GB Your job will be charged for all 256 cores, even though you are only using 128 (assuming your code has no hybrid parallel capability - e.g OpenMP or other threading). Exclusive nodes are charged by resource allocation, not utilization Most Derecho batch queues are for exclusive use, so jobs submitted to Derecho queues are charged for all 128 CPU cores on each node that is used regardless of how many CPUs are used. Requesting a Derecho CPU node for 1 hour will result in a 128 core hour charge, even if left idle by the user.","title":"Exclusive nodes"},{"location":"pbs/charging/#shared-nodes-casper","text":"Charges for jobs that you run on a shared node, including Casper nodes, are calculated by this formula: core-seconds/3600 (core-hours)","title":"Shared nodes (Casper)"},{"location":"pbs/charging/#checking-and-managing-charges","text":"Users can check computing and storage charges through the CISL Systems Accounting Manager. (Go to SAM documentation or to SAM app .) If you have concerns about using your allocation most efficiently, contact the NCAR Research Computing help desk for guidance. Sometimes jobs can be configured to make better use of the processors, and you may be able to save by using a less expensive queue. CISL can refund core-hours if system failures cause jobs to fail and the failed jobs are reported promptly. Use this core-hours refund request form (login required) if you think a refund is warranted. Technical limitations prevent us from verifying refund eligibility for jobs that are more than seven days old.","title":"Checking and managing charges"},{"location":"pbs/preemption/","text":"Job preemption with PBS \u00b6 Derecho Only! This page describes functionality only present on Derecho. Job Preemption on Derecho Some jobs are suitable for running in incrementally on resources that would otherwise be idle. Derecho supports running such jobs in the preempt queue, which allows them to be preempted with minimal impact when a higher-priority job requires the use of those resources. Suitable workflows include those with short or fairly unpredictable run times; for example, data processing, file movement, or running analysis tools that have an efficient checkpoint/restart capability. Using the preempt queue \u00b6 The preempt queue is similar to the main Derecho queue in that it serves to route jobs to the system's CPU or GPU nodes. It is different, however, in that: Jobs will start only when they can make use of resources that would otherwise be idle. They will be terminated \u2013 with a 10-minute grace period \u2013 if the resources being used are required by a higher priority job in a different queue. The start time of a job in the preempt queue will be unpredictable because of the idle resource prerequisite. Once it starts, it is guaranteed at least 10 minutes of run time but potentially much more. The duration depends on jobs other users submit to PBS after your job begins. To submit a job to the preempt queue, simply specify preempt as the queue name in your PBS script header as follows: Requesting the preempt queue #!/bin/bash #PBS -A <project_code> #PBS -N preemptable_job #PBS -q preempt #PBS -r n #PBS -l walltime=04:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 ... The walltime specification is the job duration upper limit - as discussed above, the actual execution could be shorter if the job is preempted. Use the specifier #PBS -r to indicate if the job should be rerun if it is preempted; valid options are y or n for yes or no. All other aspects of the PBS script are unchanged by the use of preemption. Abrupt termination may be entirely acceptable for some workflows. This could be the case for batch removal of a large number of files , for example, or if the application writes frequent checkpoint files and can restart successfully after being interrupted. In other cases, it may be beneficial for the application to take a specific action within the 10-minute grace window. Such an approach is possible with minor changes to the application as described below. Practical implications for preempt throughput \u00b6 Idle resources are a prerequisite for jobs in the preempt queue to start. The smaller the resource request, the more likely there will be an idle portion of the machine on which to schedule the job. Conversely, large jobs in the queue are likely to wait for long periods of time, if they execute at all. The ideal use case is small-to-medium sized jobs that are robust to interruption and that can make meaningful progress in short periods of time. Charging and allocations \u00b6 Jobs run in the preempt queue are charged at a queue factor of only 0.2, vs. 1.0 for regular jobs. Jobs that do not run to completion because of preemption are not charged against your allocation. Terminating proactively with signal handling \u00b6 When a job running in the preempt queue is targeted for preemption, PBS notifies the running process through a UNIX signal . PBS then waits 10 minutes before killing the process. A properly configured application can receive the notification signal, act upon it (typically through an existing checkpoint functionality), and then terminate gracefully rather than be terminated abruptly at the end of the grace period. The steps required to configure an application in this manner are: Provide a signal handler function to receive the termination request. Register the signal handler function with the operating system. Invoke or add checkpoints to be triggered by the signal handler. Steps 1 and 2 are fairly common across applications and even programming languages. Step 3 is application-specific, and usually involves writing the application state to disk so that it can be restarted later. For some applications, however, an even simpler approach may be possible. For example, if the target application is a data-processing pipeline, it may suffice to receive the termination notification, complete the current processing step, and simply exit without beginning additional steps in the pipeline. Signal handling and registration \u00b6 C/C++ and Fortran \u00b6 For traditional compiled languages such as C/C++ and Fortran, signal handling is most readily accomplished through some minimal C functions, even inside a predominantly Fortran application. This is because the operating system application interface is C based. The following shows the minimal required steps. Sample C program to catch signals sent from the operating system #include <stdio.h> #include <unistd.h> #include <time.h> #include <signal.h> static int checkpoint_requested = 0 ; void my_sig_handler ( int signum ) { time_t now ; time ( & now ); switch ( signum ) { case SIGINT : case SIGTERM : case SIGUSR1 : checkpoint_requested = 1 ; printf ( \"...caught signal %d at %s\" , signum , ctime ( & now )); break ; default : printf ( \"...caught other unknown signal: %d at %s\" , signum , ctime ( & now )); printf ( \" see \\\" man 7 signal \\\" for a list of known signals \\n \" ); break ; } } int main ( int argc , char ** argv ) { /* register our user-defined signal handlers */ signal ( SIGINT , my_sig_handler ); signal ( SIGTERM , my_sig_handler ); signal ( SIGUSR1 , my_sig_handler ); return 0 ; } First, we declare a C function my_sig_handler , which takes the signal identifier as input. In this example we construct a switch statement that allows for processing different types of signals in the same code block. It is evident from the listing that if the function is called with a SIGINT , SIGTERM , or SIGUSR1 signal then we set the flag checkpoint_requested and print an informative statement. For completeness, if called with any other signal, we print a diagnostic message as well but take no other action. Second, we call the system routine signal() to register our function for the specific signals we want the application to process. In this case, we are asking the operating system to call our function my_sig_handler() any time a SIGINT , SIGTERM , or SIGUSR1 is encountered. The third step is application specific and not listed, but the general idea is elsewhere in the application (for example, the main time step loop) we would check the value of the checkpoint_requested flag and take appropriate action to save state and exit gracefully. Expand this box for a complete MPI/C++ example #include <iostream> #include <iomanip> #include <stdio.h> #include <unistd.h> #include <signal.h> #include <mpi.h> namespace { int numranks , rank ; char hn [ 256 ]; int checkpoint_req = 0 ; void done_checkpoint () { checkpoint_req = 0 ; } } int checkpoint_requested ( MPI_Comm comm ) { int local_checkpoint_req = checkpoint_req ; MPI_Allreduce ( & local_checkpoint_req , & checkpoint_req , 1 , MPI_INT , MPI_MAX , comm ); return checkpoint_req ; } void my_sig_handler ( int signum ) { time_t now ; time ( & now ); if ( 0 == rank ) printf ( \"...inside handler function \\n \" ); switch ( signum ) { case SIGSTOP : case SIGINT : case SIGTERM : case SIGUSR1 : checkpoint_req = 1 ; if ( 0 == rank ) printf ( \"...caught signal %d at %s\" , signum , ctime ( & now )); break ; default : if ( 0 == rank ) { printf ( \"...caught other unknown signal: %d at %s\" , signum , ctime ( & now )); printf ( \" see \\\" man 7 signal \\\" for a list of known signals \\n \" ); } break ; } // re-register default signal handler for action //if (0 == rank) printf(\" --> Restoring default handler for signal %d\\n\", signum); //signal(signum, SIG_DFL); return ; } void register_sig_handler () { if ( 0 == rank ) printf ( \"Registering user-specified signal handlers for PID %d \\n \" , getpid ()); signal ( SIGSTOP , my_sig_handler ); signal ( SIGINT , my_sig_handler ); signal ( SIGTERM , my_sig_handler ); signal ( SIGUSR1 , my_sig_handler ); signal ( SIGUSR2 , my_sig_handler ); } void do_checkpoint ( MPI_Comm comm ) { for ( int i = 1 ; i <= 10 ; i ++ ) { if ( 0 == rank ) { time_t now ; time ( & now ); printf ( \" \\t %2d : Inside checkpoint function : %s\" , i , ctime ( & now )); fflush ( stdout ); sleep ( 5 ); } MPI_Barrier ( comm ); } done_checkpoint (); if ( 0 == rank ) { time_t now ; time ( & now ); printf ( \" --> Gracefully exiting after checkpoint : %s\" , ctime ( & now )); fflush ( stdout ); } MPI_Finalize (); exit ( EXIT_SUCCESS ); return ; } int main ( int argc , char ** argv ) { gethostname ( hn , sizeof ( hn ) / sizeof ( char )); MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numranks ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); std :: cout << \"Hello from \" << rank << \" / \" << std :: string ( hn ) << \", running \" << argv [ 0 ] << \" on \" << numranks << \" ranks\" << std :: endl ; // register our user-defined signal handlers, on every rank register_sig_handler (); for ( int i = 1 ; i <= 5000 ; i ++ ) { if ( 0 == rank ) { time_t now ; time ( & now ); printf ( \"%2d : Main function loop : %s\" , i , ctime ( & now )); fflush ( stdout ); sleep ( 5 ); } MPI_Barrier ( MPI_COMM_WORLD ); // this function needs to perform a reduction to see if any rank received // a signal, hence it is blocking. if ( checkpoint_requested ( MPI_COMM_WORLD )) do_checkpoint ( MPI_COMM_WORLD ); } MPI_Finalize (); return 0 ; } You can see this example on Github here . To integrate such an approach into a Fortran application, it is simplest to create a C function taking no arguments that encapsulates the signal registration process and calling it from within your Fortran main application. Please contact CISL help for further assistance. While the most common use case is compiled languages as shown above, it is also possible to catch and act upon signals when your main application is a shell script or in Python, as shown below. Shell scripts \u00b6 In shell scripting, the process is generally similar, with some very slight changes in terminology. Notably, in shell scripts traps can be used to intercept signals and call a user-specified function, in much the same way a signal handler can be installed in a compiled language. A complete example follows. Sample bash script to catch signals sent from the operating system #!/usr/bin/env bash checkpoint_requested = false function my_signal_handler () { case ${ sig } in SIGINT ) SIGTERM ) SIGUSR1 ) printf \"...caught %s at %s\\n\" ${ sig } \" $( date ) \" checkpoint_requested = true ;; * ) printf \"...caught unknown signal: %s\\n\" ${ sig } exit 1 ;; esac } function register_signal_handler () { printf \"Registering user-specified signal handlers for PID %d\\n\" $$ trap \"sig=SIGINT; my_signal_handler\" SIGINT trap \"sig=SIGTERM; my_signal_handler\" SIGTERM trap \"sig=USR1; my_signal_handler\" SIGUSR1 } function do_checkpoint () { for i in $( seq 1 10 ) ; do printf \"\\t%2d : Inside checkpoint function\\n\" $i sleep 5s done checkpoint_requested = false ; } # main function follows register_signal_handler for i in $( seq 1 50 ) ; do printf \"%2d : Main function\\n\" $i sleep 5s if ${ checkpoint_requested } ; then do_checkpoint fi done Running the previous code will enter a \"Main Function\" loop that executes a number of steps. Sending Control+C to the running program effectively sends a SIGINT and invokes the desired signal handling function through the bash trap mechanism. Python applications \u00b6 Finally, Python provides the signal module, which can be used in a user application to catch signals from the operating system as shown here: Sample python script to catch signals sent from the operating system #!/usr/bin/env python3 import signal import sys import time import os from datetime import datetime checkpoint_requested = False def my_signal_handler ( sig , frame ): global checkpoint_requested if signal . SIGINT == sig or signal . SIGTERM == sig or signal . SIGUSR1 == sig : print ( \"...caught signal {} at \" . format ( sig ) + datetime . now () . strftime ( \"%H:%M:%S %B %d , %Y\" )) checkpoint_requested = True else : print ( \"...caught unknown signal: {} \" . format ( sig )) sys . exit ( 1 ) return def register_signal_handler (): print ( \"Registering user-specified signal handlers for PID {} \" . format ( os . getpid ())) signal . signal ( signal . SIGINT , my_signal_handler ) signal . signal ( signal . SIGTERM , my_signal_handler ) signal . signal ( signal . SIGUSR1 , my_signal_handler ) return def do_checkpoint (): global checkpoint_requested for i in range ( 1 , 11 ): print ( \" \\t {:2d} : Inside checkpoint function\" . format ( i )) sys . stdout . flush () time . sleep ( 5 ) checkpoint_requested = False ; return if __name__ == \"__main__\" : register_signal_handler (); for i in range ( 1 , 51 ): print ( \" {:2d} : Main function\" . format ( i )) sys . stdout . flush () time . sleep ( 5 ) if checkpoint_requested : do_checkpoint () Additional resources \u00b6 All the sample scripts above are available through the NCAR hpc-demos GitHub repository in the PBS/preemp t subdirectory. More detail on signal handling in C More detail on using traps in shell scripts More detail on signal handling in Python","title":"Using Preemption"},{"location":"pbs/preemption/#job-preemption-with-pbs","text":"Derecho Only! This page describes functionality only present on Derecho. Job Preemption on Derecho Some jobs are suitable for running in incrementally on resources that would otherwise be idle. Derecho supports running such jobs in the preempt queue, which allows them to be preempted with minimal impact when a higher-priority job requires the use of those resources. Suitable workflows include those with short or fairly unpredictable run times; for example, data processing, file movement, or running analysis tools that have an efficient checkpoint/restart capability.","title":"Job preemption with PBS"},{"location":"pbs/preemption/#using-the-preempt-queue","text":"The preempt queue is similar to the main Derecho queue in that it serves to route jobs to the system's CPU or GPU nodes. It is different, however, in that: Jobs will start only when they can make use of resources that would otherwise be idle. They will be terminated \u2013 with a 10-minute grace period \u2013 if the resources being used are required by a higher priority job in a different queue. The start time of a job in the preempt queue will be unpredictable because of the idle resource prerequisite. Once it starts, it is guaranteed at least 10 minutes of run time but potentially much more. The duration depends on jobs other users submit to PBS after your job begins. To submit a job to the preempt queue, simply specify preempt as the queue name in your PBS script header as follows: Requesting the preempt queue #!/bin/bash #PBS -A <project_code> #PBS -N preemptable_job #PBS -q preempt #PBS -r n #PBS -l walltime=04:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 ... The walltime specification is the job duration upper limit - as discussed above, the actual execution could be shorter if the job is preempted. Use the specifier #PBS -r to indicate if the job should be rerun if it is preempted; valid options are y or n for yes or no. All other aspects of the PBS script are unchanged by the use of preemption. Abrupt termination may be entirely acceptable for some workflows. This could be the case for batch removal of a large number of files , for example, or if the application writes frequent checkpoint files and can restart successfully after being interrupted. In other cases, it may be beneficial for the application to take a specific action within the 10-minute grace window. Such an approach is possible with minor changes to the application as described below.","title":"Using the preempt queue"},{"location":"pbs/preemption/#practical-implications-for-preempt-throughput","text":"Idle resources are a prerequisite for jobs in the preempt queue to start. The smaller the resource request, the more likely there will be an idle portion of the machine on which to schedule the job. Conversely, large jobs in the queue are likely to wait for long periods of time, if they execute at all. The ideal use case is small-to-medium sized jobs that are robust to interruption and that can make meaningful progress in short periods of time.","title":"Practical implications for preempt throughput"},{"location":"pbs/preemption/#charging-and-allocations","text":"Jobs run in the preempt queue are charged at a queue factor of only 0.2, vs. 1.0 for regular jobs. Jobs that do not run to completion because of preemption are not charged against your allocation.","title":"Charging and allocations"},{"location":"pbs/preemption/#terminating-proactively-with-signal-handling","text":"When a job running in the preempt queue is targeted for preemption, PBS notifies the running process through a UNIX signal . PBS then waits 10 minutes before killing the process. A properly configured application can receive the notification signal, act upon it (typically through an existing checkpoint functionality), and then terminate gracefully rather than be terminated abruptly at the end of the grace period. The steps required to configure an application in this manner are: Provide a signal handler function to receive the termination request. Register the signal handler function with the operating system. Invoke or add checkpoints to be triggered by the signal handler. Steps 1 and 2 are fairly common across applications and even programming languages. Step 3 is application-specific, and usually involves writing the application state to disk so that it can be restarted later. For some applications, however, an even simpler approach may be possible. For example, if the target application is a data-processing pipeline, it may suffice to receive the termination notification, complete the current processing step, and simply exit without beginning additional steps in the pipeline.","title":"Terminating proactively with signal handling"},{"location":"pbs/preemption/#signal-handling-and-registration","text":"","title":"Signal handling and registration"},{"location":"pbs/preemption/#cc-and-fortran","text":"For traditional compiled languages such as C/C++ and Fortran, signal handling is most readily accomplished through some minimal C functions, even inside a predominantly Fortran application. This is because the operating system application interface is C based. The following shows the minimal required steps. Sample C program to catch signals sent from the operating system #include <stdio.h> #include <unistd.h> #include <time.h> #include <signal.h> static int checkpoint_requested = 0 ; void my_sig_handler ( int signum ) { time_t now ; time ( & now ); switch ( signum ) { case SIGINT : case SIGTERM : case SIGUSR1 : checkpoint_requested = 1 ; printf ( \"...caught signal %d at %s\" , signum , ctime ( & now )); break ; default : printf ( \"...caught other unknown signal: %d at %s\" , signum , ctime ( & now )); printf ( \" see \\\" man 7 signal \\\" for a list of known signals \\n \" ); break ; } } int main ( int argc , char ** argv ) { /* register our user-defined signal handlers */ signal ( SIGINT , my_sig_handler ); signal ( SIGTERM , my_sig_handler ); signal ( SIGUSR1 , my_sig_handler ); return 0 ; } First, we declare a C function my_sig_handler , which takes the signal identifier as input. In this example we construct a switch statement that allows for processing different types of signals in the same code block. It is evident from the listing that if the function is called with a SIGINT , SIGTERM , or SIGUSR1 signal then we set the flag checkpoint_requested and print an informative statement. For completeness, if called with any other signal, we print a diagnostic message as well but take no other action. Second, we call the system routine signal() to register our function for the specific signals we want the application to process. In this case, we are asking the operating system to call our function my_sig_handler() any time a SIGINT , SIGTERM , or SIGUSR1 is encountered. The third step is application specific and not listed, but the general idea is elsewhere in the application (for example, the main time step loop) we would check the value of the checkpoint_requested flag and take appropriate action to save state and exit gracefully. Expand this box for a complete MPI/C++ example #include <iostream> #include <iomanip> #include <stdio.h> #include <unistd.h> #include <signal.h> #include <mpi.h> namespace { int numranks , rank ; char hn [ 256 ]; int checkpoint_req = 0 ; void done_checkpoint () { checkpoint_req = 0 ; } } int checkpoint_requested ( MPI_Comm comm ) { int local_checkpoint_req = checkpoint_req ; MPI_Allreduce ( & local_checkpoint_req , & checkpoint_req , 1 , MPI_INT , MPI_MAX , comm ); return checkpoint_req ; } void my_sig_handler ( int signum ) { time_t now ; time ( & now ); if ( 0 == rank ) printf ( \"...inside handler function \\n \" ); switch ( signum ) { case SIGSTOP : case SIGINT : case SIGTERM : case SIGUSR1 : checkpoint_req = 1 ; if ( 0 == rank ) printf ( \"...caught signal %d at %s\" , signum , ctime ( & now )); break ; default : if ( 0 == rank ) { printf ( \"...caught other unknown signal: %d at %s\" , signum , ctime ( & now )); printf ( \" see \\\" man 7 signal \\\" for a list of known signals \\n \" ); } break ; } // re-register default signal handler for action //if (0 == rank) printf(\" --> Restoring default handler for signal %d\\n\", signum); //signal(signum, SIG_DFL); return ; } void register_sig_handler () { if ( 0 == rank ) printf ( \"Registering user-specified signal handlers for PID %d \\n \" , getpid ()); signal ( SIGSTOP , my_sig_handler ); signal ( SIGINT , my_sig_handler ); signal ( SIGTERM , my_sig_handler ); signal ( SIGUSR1 , my_sig_handler ); signal ( SIGUSR2 , my_sig_handler ); } void do_checkpoint ( MPI_Comm comm ) { for ( int i = 1 ; i <= 10 ; i ++ ) { if ( 0 == rank ) { time_t now ; time ( & now ); printf ( \" \\t %2d : Inside checkpoint function : %s\" , i , ctime ( & now )); fflush ( stdout ); sleep ( 5 ); } MPI_Barrier ( comm ); } done_checkpoint (); if ( 0 == rank ) { time_t now ; time ( & now ); printf ( \" --> Gracefully exiting after checkpoint : %s\" , ctime ( & now )); fflush ( stdout ); } MPI_Finalize (); exit ( EXIT_SUCCESS ); return ; } int main ( int argc , char ** argv ) { gethostname ( hn , sizeof ( hn ) / sizeof ( char )); MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numranks ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); std :: cout << \"Hello from \" << rank << \" / \" << std :: string ( hn ) << \", running \" << argv [ 0 ] << \" on \" << numranks << \" ranks\" << std :: endl ; // register our user-defined signal handlers, on every rank register_sig_handler (); for ( int i = 1 ; i <= 5000 ; i ++ ) { if ( 0 == rank ) { time_t now ; time ( & now ); printf ( \"%2d : Main function loop : %s\" , i , ctime ( & now )); fflush ( stdout ); sleep ( 5 ); } MPI_Barrier ( MPI_COMM_WORLD ); // this function needs to perform a reduction to see if any rank received // a signal, hence it is blocking. if ( checkpoint_requested ( MPI_COMM_WORLD )) do_checkpoint ( MPI_COMM_WORLD ); } MPI_Finalize (); return 0 ; } You can see this example on Github here . To integrate such an approach into a Fortran application, it is simplest to create a C function taking no arguments that encapsulates the signal registration process and calling it from within your Fortran main application. Please contact CISL help for further assistance. While the most common use case is compiled languages as shown above, it is also possible to catch and act upon signals when your main application is a shell script or in Python, as shown below.","title":"C/C++ and Fortran"},{"location":"pbs/preemption/#shell-scripts","text":"In shell scripting, the process is generally similar, with some very slight changes in terminology. Notably, in shell scripts traps can be used to intercept signals and call a user-specified function, in much the same way a signal handler can be installed in a compiled language. A complete example follows. Sample bash script to catch signals sent from the operating system #!/usr/bin/env bash checkpoint_requested = false function my_signal_handler () { case ${ sig } in SIGINT ) SIGTERM ) SIGUSR1 ) printf \"...caught %s at %s\\n\" ${ sig } \" $( date ) \" checkpoint_requested = true ;; * ) printf \"...caught unknown signal: %s\\n\" ${ sig } exit 1 ;; esac } function register_signal_handler () { printf \"Registering user-specified signal handlers for PID %d\\n\" $$ trap \"sig=SIGINT; my_signal_handler\" SIGINT trap \"sig=SIGTERM; my_signal_handler\" SIGTERM trap \"sig=USR1; my_signal_handler\" SIGUSR1 } function do_checkpoint () { for i in $( seq 1 10 ) ; do printf \"\\t%2d : Inside checkpoint function\\n\" $i sleep 5s done checkpoint_requested = false ; } # main function follows register_signal_handler for i in $( seq 1 50 ) ; do printf \"%2d : Main function\\n\" $i sleep 5s if ${ checkpoint_requested } ; then do_checkpoint fi done Running the previous code will enter a \"Main Function\" loop that executes a number of steps. Sending Control+C to the running program effectively sends a SIGINT and invokes the desired signal handling function through the bash trap mechanism.","title":"Shell scripts"},{"location":"pbs/preemption/#python-applications","text":"Finally, Python provides the signal module, which can be used in a user application to catch signals from the operating system as shown here: Sample python script to catch signals sent from the operating system #!/usr/bin/env python3 import signal import sys import time import os from datetime import datetime checkpoint_requested = False def my_signal_handler ( sig , frame ): global checkpoint_requested if signal . SIGINT == sig or signal . SIGTERM == sig or signal . SIGUSR1 == sig : print ( \"...caught signal {} at \" . format ( sig ) + datetime . now () . strftime ( \"%H:%M:%S %B %d , %Y\" )) checkpoint_requested = True else : print ( \"...caught unknown signal: {} \" . format ( sig )) sys . exit ( 1 ) return def register_signal_handler (): print ( \"Registering user-specified signal handlers for PID {} \" . format ( os . getpid ())) signal . signal ( signal . SIGINT , my_signal_handler ) signal . signal ( signal . SIGTERM , my_signal_handler ) signal . signal ( signal . SIGUSR1 , my_signal_handler ) return def do_checkpoint (): global checkpoint_requested for i in range ( 1 , 11 ): print ( \" \\t {:2d} : Inside checkpoint function\" . format ( i )) sys . stdout . flush () time . sleep ( 5 ) checkpoint_requested = False ; return if __name__ == \"__main__\" : register_signal_handler (); for i in range ( 1 , 51 ): print ( \" {:2d} : Main function\" . format ( i )) sys . stdout . flush () time . sleep ( 5 ) if checkpoint_requested : do_checkpoint ()","title":"Python applications"},{"location":"pbs/preemption/#additional-resources","text":"All the sample scripts above are available through the NCAR hpc-demos GitHub repository in the PBS/preemp t subdirectory. More detail on signal handling in C More detail on using traps in shell scripts More detail on signal handling in Python","title":"Additional resources"},{"location":"pbs/storing-temporary-files/","text":"Storing temporary files with TMPDIR \u00b6 /tmp , /var/tmp , or similar shared directories to hold temporary files can increase the risk of your own programs and other users' programs failing when no more space is available. Compilers and utilities often create such temporary files without your knowledge of their size or number. Specifying your own directory for temporary files can help you avoid the problem. Interactive use \u00b6 In interactive use on the login nodes, the default TMPDIR is /glade/derecho/scratch/$USER . You can change that by running the commands shown below on your command line when you log in or by setting the TMPDIR variable in your start file . Batch use \u00b6 For batch use , CISL recommends setting TMPDIR within each batch script for all batch jobs. Include these commands as the first two executable lines of your batch script after the #PBS directives. bash/zsh tcsh export TMPDIR = $SCRATCH /temp && mkdir -p $TMPDIR setenv TMPDIR $SCRATCH /temp && mkdir -p $TMPDIR Using /local_scratch/ on Casper nodes \u00b6","title":"Storing temporary files"},{"location":"pbs/storing-temporary-files/#storing-temporary-files-with-tmpdir","text":"/tmp , /var/tmp , or similar shared directories to hold temporary files can increase the risk of your own programs and other users' programs failing when no more space is available. Compilers and utilities often create such temporary files without your knowledge of their size or number. Specifying your own directory for temporary files can help you avoid the problem.","title":"Storing temporary files with TMPDIR"},{"location":"pbs/storing-temporary-files/#interactive-use","text":"In interactive use on the login nodes, the default TMPDIR is /glade/derecho/scratch/$USER . You can change that by running the commands shown below on your command line when you log in or by setting the TMPDIR variable in your start file .","title":"Interactive use"},{"location":"pbs/storing-temporary-files/#batch-use","text":"For batch use , CISL recommends setting TMPDIR within each batch script for all batch jobs. Include these commands as the first two executable lines of your batch script after the #PBS directives. bash/zsh tcsh export TMPDIR = $SCRATCH /temp && mkdir -p $TMPDIR setenv TMPDIR $SCRATCH /temp && mkdir -p $TMPDIR","title":"Batch use"},{"location":"pbs/storing-temporary-files/#using-local_scratch-on-casper-nodes","text":"","title":"Using /local_scratch/ on Casper nodes"},{"location":"pbs/checking-memory-use/","text":"Checking memory use \u00b6 All compute nodes have a limited amount of physical memory \u2013 RAM \u2013 available to your application. If your program exceeds the available memory on a Derecho node, it will either be killed by system monitors or crash and dump core, often with a \"bus error\" message in your logs. On the Casper cluster, nodes can swap data to NVMe storage, which typically prevents failures because of running out of memory in all but the most intensive workflows. Computing on swapped data, however, is much slower than processing data that is resident in memory, so jobs may can still fail from exhausting the requested wall-clock. Applications that offload computation to GPUs must also consider the available memory on each GPU, which is known as the VRAM . Asking for a large amount of memory in every job might seem like a good idea, but it will typically result in slow dispatch times and long queue waits. Tip Always try to match your memory request to your needs. This allows the scheduler to best fit all jobs onto the available resources. A good match will also prevent you from needlessly waiting for more memory than you actually require. As detailed below, you can query the PBS scheduler for bulk memory statistics from any completed job. You can also observe memory usage via either instrumenting your application or live monitoring. Available memory by system \u00b6 System Usable memory per compute node Derecho 240 GB (2,488 CPU nodes) 490 GB (82 GPU nodes) Cheyenne 45 GB (3,168 nodes) 109 GB (864 nodes) Casper 365 GB (22 nodes) 738 GB (4 nodes) 1115 GB (6 nodes) If your job approaches the usable memory per node threshold shown in the table, you may experience unexpected issues or job failures. Leave a margin of 2 or 3 percent. Querying memory usage from PBS \u00b6 The qhist tool can be used to display information about completed PBS jobs. By default, it will print the node memory the job consumed (the high-water mark usage), as in this example: $ qhist -u $USER -d 3 Job ID User Queue Nodes NCPUs NGPUs Finish Mem(GB) CPU(%) Elap(h) 3632559 benkirk htc 1 1 0 27-1802 21.8 99.0 8.01 3632475 benkirk htc 1 1 0 27-0949 30.1 86.0 0.26 ... The output shows all jobs run in the previous three days, and it lists the amount of memory consumed. Running qhist with the -w argument specifies \"wide\" output and provides additional details, including the memory requested, as in this example: $ qhist -u $USER -d 3 -w Job ID User Queue Nodes NCPUs NGPUs Finish Time Req Mem Used Mem(GB) Avg CPU (%) Elapsed (h) Job Name 3632559 benkirk htc 1 1 0 07-27T18:02 40.0 21.8 99.0 8.01 STDIN 3632475 benkirk htc 1 1 0 07-27T09:49 256.0 30.1 86.0 0.26 regrid_bmr_casper ... The output in the first job shows that it requested 40 GB of memory but used only 21.8 GB. The second job requested 256 GB but used only 30.1GB. If you see a job where the used memory matches the requested memory, it likely either failed with an out-of-memory error or began swapping data to disk, negatively affecting performance. Using qhist , you can easily check for large under- and overestimates and adjust future jobs accordingly. PBS logs do not include information about GPU memory usage. Instrumenting your application \u00b6 While the scheduler can provide the maximum memory amount used by a job, it cannot provide details on specific commands in your script, time-varying memory logging, or insight into GPU VRAM usage. For these details, you will want to turn to application instrumentation. Using instrumentation will incur some memory usage overhead, so it can be a good idea to request more memory than your job would typically need when profiling. Using peak_memusage to report and log memory usage \u00b6 The peak_memusage tool is a command-line utility that outputs the maximum node memory \u2013 and GPU memory, if relevant) \u2013 used by each process and/or thread of an application. It has several modes of operation which vary in detail reported and complexity of use. peak_memusage for determining maximum memory usage \u00b6 The peak_memusage command can be used to run your application and report its \"high water,\" or peak memory footprint. To use it, you must load the appropriate module and simply prefix your application launch command as follows: $ module load peak-memusage $ peak_memusage <my_application> <--my-apps-arguments> ... casper-login1 used memory in task 0: 51MiB (+2MiB overhead). ExitStatus: 0. Signal: 0 The regular application output has been abbreviated. The final line of output reveals this application required a total of 51MiB of system memory, including 2MiB \"overhead.\" The overhead may vary considerably by application and depends largely on what libraries the application requires. This approach also works inside MPI applications, with specific examples provided here . Run man peak_memusage for additional details and examples. Determining memory use history \u00b6 CISL provides a log_memusage tool you can use to produce a time history of system memory usage as well as GPU memory if allocated to the job. This tools provides a shared library that can be either linked to directly or, more likely, used with an existing application through the LD_PRELOAD mechanism. For example, if you have a previously compiled application and would like to determine its memory usage over time, set the environment variable LD_PRELOAD when executing the application as follows: $ LD_PRELOAD=${NCAR_ROOT_PEAK_MEMUSAGE}/install/lib/liblog_memusage.so <my_application> ... # (memusage) --> gust02 / PID 44899, peak used memory: 18 MiB The log_memusage tool works by creating a monitoring thread that polls the operating system for the application's memory usage at specified intervals. Several environment variables control the tool's behavior. Run man log_memusage for additional details and examples. This approach also works under MPI if the LD_PRELOAD environment variable is propagated properly into the MPI execution, as in this example: $ mpiexec -n 6 -env LD_PRELOAD=${NCAR_ROOT_PEAK_MEMUSAGE}/lib/liblog_memusage.so <my_application> ... # (memusage) --> gu0017 / PID 121027 / MPI Rank 0, peak used memory: 166 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121029 / MPI Rank 2, peak used memory: 93 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121031 / MPI Rank 4, peak used memory: 93 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121032 / MPI Rank 5, peak used memory: 92 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121030 / MPI Rank 3, peak used memory: 93 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121028 / MPI Rank 1, peak used memory: 165 MiB (CPU), 563 MiB (GPU) The details for how a given MPI implementation passes environment variables can vary. Some use -env while others use -x . Run man mpiexec for specifics of your implementation. Using cray-mpich to report and log Derecho memory \u00b6 The preferred Cray MPICH MPI implementation on Derecho provides capabilities, similar to those above, for recording an application's high-water memory usage. They are controlled by these environment variables: MPICH_MEMORY_REPORT MPICH_MEMORY_REPORT_FILE Here's an example: export MPICH_MEMORY_REPORT=1 mpiexec <typical arguments> ./myapp ... # MPICH_MEMORY: Max memory allocated by malloc: 5540920 bytes by rank 0 # MPICH_MEMORY: Min memory allocated by malloc: 5540224 bytes by rank 1 # MPICH_MEMORY: Max memory allocated by mmap: 110720 bytes by rank 0 # MPICH_MEMORY: Min memory allocated by mmap: 110720 bytes by rank 0 # MPICH_MEMORY: Max memory allocated by shmget: 17834896 bytes by rank 0 # MPICH_MEMORY: Min memory allocated by shmget: 0 bytes by rank 1 The mpiexec command also provides a high-level summary of resources consumed, including memory use, when the --verbose flag is employed: mpiexec --verbose <typical arguments> ./myapp ... Application dc6a4084 resources: utime=7s stime=7s maxrss=222296KB inblock=0 oublock=0 minflt=289502 majflt=0 nvcsw=16550 nivcsw=276 The maxrss (maximum resident set size) is the maximum CPU memory required by a single MPI rank in the application. Run man intro_mpi and man mpiexec on Derecho for more information on these Cray-specific features. Other parallel memory profiling tools \u00b6 We recommend the tools described above for understanding your job memory use and crafting your PBS resource specifications. However, sometimes you need even more detailed information on application memory use patterns (e.g., during code development cycles). Many tools exist for detailed memory profiling; on Derecho and Casper we recommend Linaro Forge . Interactive monitoring \u00b6 The tools described above record the memory consumed during a program's execution, but monitoring in real time can be useful, too. It allows you to take action if you need to while the job is running and you have access to the compute nodes. Take care when executing additional commands on your job nodes, as any additional load you put on the execution host will slow your program down, and any additional memory you consume subtracts from what is available. This consideration is especially important on the Casper cluster, where your host is likely shared with other users. The first step is to identify which nodes are in use by running the qstat -f <JOBID> command as shown here: Job Id: 5677216.casper-pbs ... resources_used.mem = 78004288kb resources_used.ncpus = 1 resources_used.vmem = 1262344kb resources_used.walltime = 00:19:27 job_state = R queue = htc server = casper-pbs ... exec_host = crhtc53/17*4+crhtc62/11*4 ... project = _pbs_project_default Submit_Host = chadmin4.hpc.ucar.edu The command will create a lot of output, which is abbreviated above. Note specifically that PBS is reporting the amount of memory used so far . The execution hosts (compute nodes) are listed in the exec_host field. The job in the example above ran on two hosts, crhtc53 and crhtc62 . While the job is in a running state, it is possible to ssh directly to any of the named hosts and query basic diagnostic programs such as top (for node memory) and nvidia-smi (for GPU memory), or use more advanced tools such as strace and gdb . Typically, you will want to begin with the first execution host listed, as many applications perform file I/O on the first host so memory usage tends to be highest there. Be aware that your ssh session will be terminated abruptly when the PBS job ends and you no longer have batch processes assigned to the host(s). Understanding and resolving various memory issues \u00b6 Issue 1. Your Derecho or Casper job shows signs of a memory issue (e.g., the job log ends with \"Killed\" or \"Bus Error\") despite qhist reporting a higher requested than used memory amount. The output from qhist indicates the total memory used and requested across all nodes in the job. This heuristic, while useful, can obfuscate memory usage patterns. Many scientific applications perform read/write operations on the \"head\" (primary) node of the job, and because data must be collected to this node to do I/O, memory usage can exceed what is available on that node while others use far less. Additionally, the PBS scheduler only records data at set intervals, so aliasing issues come into play for rapid increases in memory footprint. More sophisticated instrumentation like the temporal logging mode of peak_memusage can provide insight into these memory usage patterns. Issue 2. Your data analysis job on Casper begins to run slowly after a large array has been allocated, but no memory error is reported. It is likely that you are swapping data from RAM to disk. If possible, perform garbage collection in your language of choice to free memory. Otherwise, you will likely need to start a new job with a higher memory limit. Issue 3. Your batch job crashes shortly after execution and the output provides no explanation. If you suspect memory problems, it makes sense to examine recent changes to your code or runtime environment. Ask yourself questions like these: When did you last run the job successfully? Was the model's resolution increased since then? Did you port your code from a computer having more memory? Did you add new arrays or change array dimensions? Have you modified the batch job script? If the answer to those questions is \u201cno,\u201d your job might be failing because you have exceeded your disk quota rather than because of a memory issue. To check, follow these steps: Run the gladequota command. Check the \" % Full \" column in the command's output. Clean up any GLADE spaces that are at or near 100% full. Look for core files or other large files that recent jobs may have created. Exceeding disk quota at runtime can result in symptoms that are similar to those resulting from memory problems. If running gladequota doesn't indicate a problem, consider using the ARM Forge DDT advanced memory debugger to isolate the problem. If you have tried all of the above and are still troubled that your job is exceeding usable memory, contact the NCAR Research Computing help desk with the relevant job number(s).","title":"Checking memory use"},{"location":"pbs/checking-memory-use/#checking-memory-use","text":"All compute nodes have a limited amount of physical memory \u2013 RAM \u2013 available to your application. If your program exceeds the available memory on a Derecho node, it will either be killed by system monitors or crash and dump core, often with a \"bus error\" message in your logs. On the Casper cluster, nodes can swap data to NVMe storage, which typically prevents failures because of running out of memory in all but the most intensive workflows. Computing on swapped data, however, is much slower than processing data that is resident in memory, so jobs may can still fail from exhausting the requested wall-clock. Applications that offload computation to GPUs must also consider the available memory on each GPU, which is known as the VRAM . Asking for a large amount of memory in every job might seem like a good idea, but it will typically result in slow dispatch times and long queue waits. Tip Always try to match your memory request to your needs. This allows the scheduler to best fit all jobs onto the available resources. A good match will also prevent you from needlessly waiting for more memory than you actually require. As detailed below, you can query the PBS scheduler for bulk memory statistics from any completed job. You can also observe memory usage via either instrumenting your application or live monitoring.","title":"Checking memory use"},{"location":"pbs/checking-memory-use/#available-memory-by-system","text":"System Usable memory per compute node Derecho 240 GB (2,488 CPU nodes) 490 GB (82 GPU nodes) Cheyenne 45 GB (3,168 nodes) 109 GB (864 nodes) Casper 365 GB (22 nodes) 738 GB (4 nodes) 1115 GB (6 nodes) If your job approaches the usable memory per node threshold shown in the table, you may experience unexpected issues or job failures. Leave a margin of 2 or 3 percent.","title":"Available memory by system"},{"location":"pbs/checking-memory-use/#querying-memory-usage-from-pbs","text":"The qhist tool can be used to display information about completed PBS jobs. By default, it will print the node memory the job consumed (the high-water mark usage), as in this example: $ qhist -u $USER -d 3 Job ID User Queue Nodes NCPUs NGPUs Finish Mem(GB) CPU(%) Elap(h) 3632559 benkirk htc 1 1 0 27-1802 21.8 99.0 8.01 3632475 benkirk htc 1 1 0 27-0949 30.1 86.0 0.26 ... The output shows all jobs run in the previous three days, and it lists the amount of memory consumed. Running qhist with the -w argument specifies \"wide\" output and provides additional details, including the memory requested, as in this example: $ qhist -u $USER -d 3 -w Job ID User Queue Nodes NCPUs NGPUs Finish Time Req Mem Used Mem(GB) Avg CPU (%) Elapsed (h) Job Name 3632559 benkirk htc 1 1 0 07-27T18:02 40.0 21.8 99.0 8.01 STDIN 3632475 benkirk htc 1 1 0 07-27T09:49 256.0 30.1 86.0 0.26 regrid_bmr_casper ... The output in the first job shows that it requested 40 GB of memory but used only 21.8 GB. The second job requested 256 GB but used only 30.1GB. If you see a job where the used memory matches the requested memory, it likely either failed with an out-of-memory error or began swapping data to disk, negatively affecting performance. Using qhist , you can easily check for large under- and overestimates and adjust future jobs accordingly. PBS logs do not include information about GPU memory usage.","title":"Querying memory usage from PBS"},{"location":"pbs/checking-memory-use/#instrumenting-your-application","text":"While the scheduler can provide the maximum memory amount used by a job, it cannot provide details on specific commands in your script, time-varying memory logging, or insight into GPU VRAM usage. For these details, you will want to turn to application instrumentation. Using instrumentation will incur some memory usage overhead, so it can be a good idea to request more memory than your job would typically need when profiling.","title":"Instrumenting your application"},{"location":"pbs/checking-memory-use/#using-peak_memusage-to-report-and-log-memory-usage","text":"The peak_memusage tool is a command-line utility that outputs the maximum node memory \u2013 and GPU memory, if relevant) \u2013 used by each process and/or thread of an application. It has several modes of operation which vary in detail reported and complexity of use.","title":"Using peak_memusage to report and log memory usage"},{"location":"pbs/checking-memory-use/#peak_memusage-for-determining-maximum-memory-usage","text":"The peak_memusage command can be used to run your application and report its \"high water,\" or peak memory footprint. To use it, you must load the appropriate module and simply prefix your application launch command as follows: $ module load peak-memusage $ peak_memusage <my_application> <--my-apps-arguments> ... casper-login1 used memory in task 0: 51MiB (+2MiB overhead). ExitStatus: 0. Signal: 0 The regular application output has been abbreviated. The final line of output reveals this application required a total of 51MiB of system memory, including 2MiB \"overhead.\" The overhead may vary considerably by application and depends largely on what libraries the application requires. This approach also works inside MPI applications, with specific examples provided here . Run man peak_memusage for additional details and examples.","title":"peak_memusage for determining maximum memory usage"},{"location":"pbs/checking-memory-use/#determining-memory-use-history","text":"CISL provides a log_memusage tool you can use to produce a time history of system memory usage as well as GPU memory if allocated to the job. This tools provides a shared library that can be either linked to directly or, more likely, used with an existing application through the LD_PRELOAD mechanism. For example, if you have a previously compiled application and would like to determine its memory usage over time, set the environment variable LD_PRELOAD when executing the application as follows: $ LD_PRELOAD=${NCAR_ROOT_PEAK_MEMUSAGE}/install/lib/liblog_memusage.so <my_application> ... # (memusage) --> gust02 / PID 44899, peak used memory: 18 MiB The log_memusage tool works by creating a monitoring thread that polls the operating system for the application's memory usage at specified intervals. Several environment variables control the tool's behavior. Run man log_memusage for additional details and examples. This approach also works under MPI if the LD_PRELOAD environment variable is propagated properly into the MPI execution, as in this example: $ mpiexec -n 6 -env LD_PRELOAD=${NCAR_ROOT_PEAK_MEMUSAGE}/lib/liblog_memusage.so <my_application> ... # (memusage) --> gu0017 / PID 121027 / MPI Rank 0, peak used memory: 166 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121029 / MPI Rank 2, peak used memory: 93 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121031 / MPI Rank 4, peak used memory: 93 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121032 / MPI Rank 5, peak used memory: 92 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121030 / MPI Rank 3, peak used memory: 93 MiB (CPU), 563 MiB (GPU) # (memusage) --> gu0017 / PID 121028 / MPI Rank 1, peak used memory: 165 MiB (CPU), 563 MiB (GPU) The details for how a given MPI implementation passes environment variables can vary. Some use -env while others use -x . Run man mpiexec for specifics of your implementation.","title":"Determining memory use history"},{"location":"pbs/checking-memory-use/#using-cray-mpich-to-report-and-log-derecho-memory","text":"The preferred Cray MPICH MPI implementation on Derecho provides capabilities, similar to those above, for recording an application's high-water memory usage. They are controlled by these environment variables: MPICH_MEMORY_REPORT MPICH_MEMORY_REPORT_FILE Here's an example: export MPICH_MEMORY_REPORT=1 mpiexec <typical arguments> ./myapp ... # MPICH_MEMORY: Max memory allocated by malloc: 5540920 bytes by rank 0 # MPICH_MEMORY: Min memory allocated by malloc: 5540224 bytes by rank 1 # MPICH_MEMORY: Max memory allocated by mmap: 110720 bytes by rank 0 # MPICH_MEMORY: Min memory allocated by mmap: 110720 bytes by rank 0 # MPICH_MEMORY: Max memory allocated by shmget: 17834896 bytes by rank 0 # MPICH_MEMORY: Min memory allocated by shmget: 0 bytes by rank 1 The mpiexec command also provides a high-level summary of resources consumed, including memory use, when the --verbose flag is employed: mpiexec --verbose <typical arguments> ./myapp ... Application dc6a4084 resources: utime=7s stime=7s maxrss=222296KB inblock=0 oublock=0 minflt=289502 majflt=0 nvcsw=16550 nivcsw=276 The maxrss (maximum resident set size) is the maximum CPU memory required by a single MPI rank in the application. Run man intro_mpi and man mpiexec on Derecho for more information on these Cray-specific features.","title":"Using cray-mpich to report and log Derecho memory"},{"location":"pbs/checking-memory-use/#other-parallel-memory-profiling-tools","text":"We recommend the tools described above for understanding your job memory use and crafting your PBS resource specifications. However, sometimes you need even more detailed information on application memory use patterns (e.g., during code development cycles). Many tools exist for detailed memory profiling; on Derecho and Casper we recommend Linaro Forge .","title":"Other parallel memory profiling tools"},{"location":"pbs/checking-memory-use/#interactive-monitoring","text":"The tools described above record the memory consumed during a program's execution, but monitoring in real time can be useful, too. It allows you to take action if you need to while the job is running and you have access to the compute nodes. Take care when executing additional commands on your job nodes, as any additional load you put on the execution host will slow your program down, and any additional memory you consume subtracts from what is available. This consideration is especially important on the Casper cluster, where your host is likely shared with other users. The first step is to identify which nodes are in use by running the qstat -f <JOBID> command as shown here: Job Id: 5677216.casper-pbs ... resources_used.mem = 78004288kb resources_used.ncpus = 1 resources_used.vmem = 1262344kb resources_used.walltime = 00:19:27 job_state = R queue = htc server = casper-pbs ... exec_host = crhtc53/17*4+crhtc62/11*4 ... project = _pbs_project_default Submit_Host = chadmin4.hpc.ucar.edu The command will create a lot of output, which is abbreviated above. Note specifically that PBS is reporting the amount of memory used so far . The execution hosts (compute nodes) are listed in the exec_host field. The job in the example above ran on two hosts, crhtc53 and crhtc62 . While the job is in a running state, it is possible to ssh directly to any of the named hosts and query basic diagnostic programs such as top (for node memory) and nvidia-smi (for GPU memory), or use more advanced tools such as strace and gdb . Typically, you will want to begin with the first execution host listed, as many applications perform file I/O on the first host so memory usage tends to be highest there. Be aware that your ssh session will be terminated abruptly when the PBS job ends and you no longer have batch processes assigned to the host(s).","title":"Interactive monitoring"},{"location":"pbs/checking-memory-use/#understanding-and-resolving-various-memory-issues","text":"Issue 1. Your Derecho or Casper job shows signs of a memory issue (e.g., the job log ends with \"Killed\" or \"Bus Error\") despite qhist reporting a higher requested than used memory amount. The output from qhist indicates the total memory used and requested across all nodes in the job. This heuristic, while useful, can obfuscate memory usage patterns. Many scientific applications perform read/write operations on the \"head\" (primary) node of the job, and because data must be collected to this node to do I/O, memory usage can exceed what is available on that node while others use far less. Additionally, the PBS scheduler only records data at set intervals, so aliasing issues come into play for rapid increases in memory footprint. More sophisticated instrumentation like the temporal logging mode of peak_memusage can provide insight into these memory usage patterns. Issue 2. Your data analysis job on Casper begins to run slowly after a large array has been allocated, but no memory error is reported. It is likely that you are swapping data from RAM to disk. If possible, perform garbage collection in your language of choice to free memory. Otherwise, you will likely need to start a new job with a higher memory limit. Issue 3. Your batch job crashes shortly after execution and the output provides no explanation. If you suspect memory problems, it makes sense to examine recent changes to your code or runtime environment. Ask yourself questions like these: When did you last run the job successfully? Was the model's resolution increased since then? Did you port your code from a computer having more memory? Did you add new arrays or change array dimensions? Have you modified the batch job script? If the answer to those questions is \u201cno,\u201d your job might be failing because you have exceeded your disk quota rather than because of a memory issue. To check, follow these steps: Run the gladequota command. Check the \" % Full \" column in the command's output. Clean up any GLADE spaces that are at or near 100% full. Look for core files or other large files that recent jobs may have created. Exceeding disk quota at runtime can result in symptoms that are similar to those resulting from memory problems. If running gladequota doesn't indicate a problem, consider using the ARM Forge DDT advanced memory debugger to isolate the problem. If you have tried all of the above and are still troubled that your job is exceeding usable memory, contact the NCAR Research Computing help desk with the relevant job number(s).","title":"Understanding and resolving various memory issues"},{"location":"pbs/common-causes-of-job-failures/","text":"Common causes of job failures \u00b6 These are some of the most common causes of job failures on NCAR systems and some tips for how to avoid them. Keep an eye out for other tips, and change notices that you need to be aware of, in the CISL Daily Bulletin . Running close to the node or GPU memory limit. If you experience failures early in your job, especially at times when the program is reading data from disk or allocating data arrays, you may be running out of memory on compute nodes or GPUs. You may simply need to request more memory per node. Other potential solutions are to either spread your job across more nodes/GPUs (with fewer processes per resource) or shrink your problem size. See Checking memory use to determine how much memory your program or script requires to run. Home or other directory filling up. If you are seeing erratic job failures or experiencing failures for jobs that have run successfully many times in the past, you may have used up your disk quota. Run the gladequota command and clean up any storage spaces that are at or near 100% full. Specifying version-specific modules in your dotfiles. The problem with specifying version-specific modules in your dotfiles (.bashrc and .tcshrc, for example) is that someday the version you specify will be removed, and so the reproducibility of your job is reduced. You'll probably look at your batch job, not see anything wrong with it, and be puzzled, not realizing the problem is rooted in your dotfiles. In fact, it is best not to specify any modules in your dotfiles. Instead, include your environment modifications \u2013 any module load commands, for example \u2013 in your job script. Failure to clean directories when remaking executables and binaries. This failure can be puzzling because it looks like everything built correctly. When you run your application, it fails in a way that does not point to the root cause of the problem: that the build is using one or more binaries from previous, incompatible builds. Using old binaries with new module versions. Some software libraries \u2013 for example, many components of the Cray Programming Environment and NVIDIA HPC Toolkit \u2013 do not guarantee forward-compatibility with newer versions as they are released. Attempting to run binaries compiled with older versions of these libraries can result in strange runtime failures at launch. If you switch to a new version of a compiler, MPI, or GPU toolkit, it is best to rebuild your application. Filling up temporary file space. Using shared /tmp , /var/tmp or similar shared directories to hold temporary files can increase the risk of your own programs and other users' programs failing when no more space is available. Set TMPDIR to point to your GLADE scratch space (or the fast NVMe local scratch on Casper) in every script for all batch jobs. See Storing temporary files with TMPDIR .","title":"Common causes of job failures"},{"location":"pbs/common-causes-of-job-failures/#common-causes-of-job-failures","text":"These are some of the most common causes of job failures on NCAR systems and some tips for how to avoid them. Keep an eye out for other tips, and change notices that you need to be aware of, in the CISL Daily Bulletin . Running close to the node or GPU memory limit. If you experience failures early in your job, especially at times when the program is reading data from disk or allocating data arrays, you may be running out of memory on compute nodes or GPUs. You may simply need to request more memory per node. Other potential solutions are to either spread your job across more nodes/GPUs (with fewer processes per resource) or shrink your problem size. See Checking memory use to determine how much memory your program or script requires to run. Home or other directory filling up. If you are seeing erratic job failures or experiencing failures for jobs that have run successfully many times in the past, you may have used up your disk quota. Run the gladequota command and clean up any storage spaces that are at or near 100% full. Specifying version-specific modules in your dotfiles. The problem with specifying version-specific modules in your dotfiles (.bashrc and .tcshrc, for example) is that someday the version you specify will be removed, and so the reproducibility of your job is reduced. You'll probably look at your batch job, not see anything wrong with it, and be puzzled, not realizing the problem is rooted in your dotfiles. In fact, it is best not to specify any modules in your dotfiles. Instead, include your environment modifications \u2013 any module load commands, for example \u2013 in your job script. Failure to clean directories when remaking executables and binaries. This failure can be puzzling because it looks like everything built correctly. When you run your application, it fails in a way that does not point to the root cause of the problem: that the build is using one or more binaries from previous, incompatible builds. Using old binaries with new module versions. Some software libraries \u2013 for example, many components of the Cray Programming Environment and NVIDIA HPC Toolkit \u2013 do not guarantee forward-compatibility with newer versions as they are released. Attempting to run binaries compiled with older versions of these libraries can result in strange runtime failures at launch. If you switch to a new version of a compiler, MPI, or GPU toolkit, it is best to rebuild your application. Filling up temporary file space. Using shared /tmp , /var/tmp or similar shared directories to hold temporary files can increase the risk of your own programs and other users' programs failing when no more space is available. Set TMPDIR to point to your GLADE scratch space (or the fast NVMe local scratch on Casper) in every script for all batch jobs. See Storing temporary files with TMPDIR .","title":"Common causes of job failures"},{"location":"pbs/job-scripts/","text":"Portable Batch System (PBS) Job Scripts \u00b6 Job scripts form the basis of batch jobs . A job script is simply a text file with instructions of the work to execute. Job scripts are usually written in bash or tcsh and thus mimic commands a user would execute interactively through a shell; but instead are executed on specific resources allocated by the scheduler when available. Scripts can also be written in other languages - commonly Python. Anatomy of a Job Script \u00b6 Sample basic PBS scripts are listed below: PBS job scripts bash tcsh Python #!/bin/bash #PBS -N hello_pbs #PBS -A <project_code> #PBS -j oe #PBS -k eod #PBS -q main #PBS -l walltime=00:05:00 #PBS -l select=2:ncpus=128:mpiprocs=128 ### Set temp to scratch export TMPDIR = ${ SCRATCH } / ${ USER } /temp && mkdir -p $TMPDIR ### specify desired module environment module purge module load ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25 module list ### Compile & Run MPI Program mpicxx -o hello_world_derecho /glade/u/home/benkirk/hello_world_mpi.C -fopenmp mpiexec -n 256 -ppn 128 ./hello_world_derecho The first line denotes the interpreter to be used for the script: #!/bin/bash #!/bin/tcsh #PBS -N hello_pbs #PBS -A <project_code> #PBS -j oe #PBS -k eod #PBS -q main #PBS -l walltime=00:05:00 #PBS -l select=2:ncpus=128:mpiprocs=128 source /etc/csh.cshrc ### Set temp to scratch setenv TMPDIR ${ SCRATCH } / ${ USER } /temp && mkdir -p ${ TMPDIR } ### specify desired module environment module purge module load ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25 module list ### Compile & Run MPI Program mpicxx -o hello_world_derecho /glade/u/home/benkirk/hello_world_mpi.C -fopenmp mpiexec -n 256 -ppn 128 ./hello_world_derecho The first line denotes the interpreter to be used for the script: #!/bin/tcsh #!/glade/u/apps/opt/conda/envs/npl/bin/python #PBS -N hello_pbs #PBS -A <project_code> #PBS -j oe #PBS -k eod #PBS -q main #PBS -l walltime=00:05:00 #PBS -l select=1:ncpus=128:mpiprocs=128 import sys print ( \"Hello, world!! \\n\\n \" ) print ( \"Python version:\" ) print ( sys . version ) print ( \"Version info:\" ) print ( sys . version_info ) The first line denotes the interpreter to be used for the script: #!/glade/u/apps/opt/conda/envs/npl/bin/python indicates this is a python script (and, specifically, the NCAR NPL instance). Focusing on the bash example for discussion , the remainder of the script contains two main sections: The lines beginning with #PBS are directives that will be interpreted by PBS when this script is submitted with qsub . Each of these lines contains an instruction that will be used by qsub to control job resources, execution, etc... The remaining script contents are simply bash commands that will be run inside the batch environment on the selected resources and define the work to be done in this job. PBS directives \u00b6 The example above contains several directives which are interpreted by the qsub submission program: -N hello_pbs provides a job name . This name will be displayed by the scheduler for diagnostic and file output. If omitted, and a script is used to submit the job, the job's name is the name of the script. -A <project_code> indicates which NCAR Project Accounting code resource allocation will be applicable to this job. (You will want to replace <project_code> with your project's specific code.) -j oe requests we combine any standard text output ( o ) and error ( e ) into one output file . (By default, PBS will write program output and error to different log files. This behavior is contrary to what many users expect from terminal interaction, where output and error are generally interspersed. This optional flag changes that behavior.) -q main specifies the desired PBS queue for this job. -l walltime=00:05:00 requests 5 minutes as the maximum job execution ( walltime ) time. Specified in HH:MM:SS format. -l select=2:ncpus=128:mpiprocs=128 is a computational resource chunk request, detailing the quantity and configuration of compute nodes required for this job. This example requests a selection of 2 nodes, where each node must have 128 CPU cores, each of which we will use as an MPI rank in our application. Script contents \u00b6 The remaining script contains shell commands that define the job execution workflow. The commands here are arbitrary, however we strongly recommend the general structure presented above. This includes: Explicitly setting the TMPDIR variable . As described here , many programs write temporary data to TMPDIR , which is usually small and shared among st users. Specifying your own directory for temporary files can help you avoid the risk of your own programs and other users' programs failing when no more space is available. Loading and reporting the specific module environment required for this job. While strictly not necessary (in general, the system default modules will be loaded anyway), we recommend this as best practice as it facilitates debugging and reproducing later. (While the system default modules will change over time, manually specifying module versions allows you to recreate the same execution environment in the future.) ( Optional ) Defining any environment variables specific to the chosen module environment. Occasionally users will want to define particular run time environment variables e.g. for a specific MPI or library chosen via the module load commands. Remaining job-specific steps. In the example above, we first compile and then execute hello_world_mpi.C , a simple MPI program. Common #PBS directives \u00b6 Resource requests \u00b6 Resources (compute node configuration, job duration) are requested through a combination of resource selection flags, each preceded with -l . For example: #PBS -l walltime=00:05:00 #PBS -l select=1:ncpus=64:mpiprocs=4:ngpus=4:mem=400GB #PBS -l gpu_type=a100 #PBS -l job_priority=economy specifies job walltime , compute node selection, GPU type, and job priority. See more details below. select statements \u00b6 Resources are specified through a select statement. The general form of a homogeneous selection statement is select=<# NODES>:ncpus=<# CPU Cores/node>:mpiprocs=<# MPI Ranks/node>:ompthreads=<# OpenMP Threads/rank>:mem=<RAM/node>:ngpus=<# GPUs/node> where <# NODES> is the total number of compute nodes requested, followed by a colon-separated list of <# CPU Cores/node> is the total number of CPUs requested on each node , which can be a mix of MPI Ranks and/or OpenMP threads, <# MPI Ranks/node is the number of MPI Ranks on each node , <# OpenMP Threads/node> is the number of OpenMP ranks per MPI Rank on each node . (Optional, defaults to 1), <RAM/node> is how much main memory (RAM) the job will be able to access on each node . (Optional, default is system dependent), and <# GPUs/node> is the number of GPUs per node . (Optional, defaults to 0). Taken together, this specifies a resource chunk . Homogeneous resource chunks are the most common case, however, heterogeneous selection statements can be constructed by multiple chunks separated by a + (examples below). Examples \u00b6 4 128-core nodes, each running 128 MPI ranks (4 x 128 = 512 MPI ranks total). select=4:ncpus=128:mpiprocs=128 4 128-core nodes, each running 32 MPI ranks with 4 OpenMP threads per ranl (4 x 32 = 128 MPI ranks total, each with 4 threads = 512 total CPU cores). select=4:ncpus=128:mpiprocs=32:ompthreads=4 2 64-core nodes, each running 4 MPI ranks, 4 GPUS, and 384 GB memory (8 GPUs total, with 8 MPI ranks). select=2:ncpus=64:mpiprocs=4:ngpus=4:mem=384GB 4 36-core nodes, each running 4 MPI ranks, 4 GPUS configured with NVIDIA's Multi-Process Service (MPS), and 768 GB memory (16 GPUs total, with 16 MPI ranks). select=4:ncpus=36:mpiprocs=4:ngpus=4:mem=768GB:mps=1 MPS is simply enabled via mps=1 , and is disabled by default ( mps=0 ) A heterogeneous selection, 96 128-core nodes each with 128 MPI ranks, and 32 128-core nodes each with 16 MPI ranks and 8 OpenMP threads select=96:ncpus=128:mpiprocs=128+32:ncpus=16:ompthreads=8 The particular values for ncpus , mem , ngpus are node-type dependent, and most NCAR systems have more than one available node type. (See system specific documentation for recommended values.) Request all ncpus when running on exclusive nodes For large multi-node jobs on machines like Derecho nodes are usually assigned exclusively to a single PBS job at a time. For most use cases, users will request the maximum number of CPUS available via ncpus , and consume all through a combination of mpiprocs and ompthreads . Occasionally users may want fewer than the maximum CPUs for computation, \"under-subscribing\" compute nodes. This is usually done for memory intensive applications, where some cores are intentionally left idle in order to increase the memory available for the running cores. In such circumstances users should still request access to all CPUs, but only use a subset. For example select = 4 :ncpus = 128 :mpiprocs = 64 :ompthreads = 1 :mem = 235GB requests access to all 128 CPUs on a dedicated node, but only assigns 64 for MPI use. By requesting access to all ncpus=128 is recommended for this case because it allows optimally locating the actually used mpiprocs=64 across the compute node via process binding . walltime \u00b6 The -l walltime=HH:MM:SS resource directive specifies maximum job duration. Jobs still running when this wall time is exceeded will be terminated automatically by the scheduler. walltime=HH:MM:SS job_priority \u00b6 Users may request a specific job priority with the -l job_priority=... resource directive. Valid options are: job_priority=<regular|premium|economy> Job priority impacts both scheduling and resource accounting, allowing users to run at a higher/lower priority in exchange for additional/reduced allocation consumption. See here for additional information. gpu_type \u00b6 For highly heterogeneous systems such as Casper, a resource chunk statement including GPUS may match more than one particular GPU type. The resource specification -l gpu_type=... requests a particular GPU type, removing such ambiguity. Valid options are: gpu_type=<gp100|v100|a100> Listing of frequently used #PBS directives \u00b6 Directive Impact -A <project_code> NCAR Project Accounting string associated with the job. -a <date at time> Allows users to request a future eligible time for job execution. (By default jobs are considered immediately eligible for execution.) Format: [[[YY]MM]DD]hhmm[.SS] -h Holds the job. <Held jobs can be released with qrls . -I Specifies interactive execution. Interactive jobs place the user a login session on the first compute node. Interactive jobs terminate when the shell exits, or walltime is exceeded. -J <range> Specifies an array job . Use the range argument to specify the indices of the sub jobs of the array. range is specified in the form X-Y[:Z] where X is the first index, Y is the upper bound on the indices, and Z is the stepping factor. Indices must be greater than or equal to zero. Use the optional %max_subjobs argument to set a limit on the number of subjobs that can be running at one time. more details on array jobs -m <mail events> Sends email on specific events (may be combined). n : No mail is sent a : Mail is sent when the job is aborted by the batch system b : Mail is sent when the job begins execution e : Mail is sent when the job terminates Example: -m abe -M <address(es)> List of users to whom mail about the job is sent. The user list argument has the form: <username>[@<hostname>][,<username>[@<hostname>],...] qsub arguments take precedence over #PBS directives Best practice is to fully specify your PBS queue, job name, and resources in your job script as shown above. This allows for better debugging and facilitates reproducing runs in the future. When a job's PBS attributes are fully specified, you can usually submit the script with no additional arguments, for example qsub script.pbs (See Running Jobs for more details on interacting with the scheduler.) On occasion users may want to change some of the PBS parameters without modifying the job script. A common example may be the account code, the job name ( -N ) or even the walltime . Any #PBS directives specified in the job script can be overridden at submission time by equivalent arguments to qsub . For example, qsub -A <OTHER_ACCOUNT> \\ -N testing \\ -l job_priority = premium \\ script.pbs will run script.pbs under the specified <OTHER_ACCOUNT> with the job name testing and requests premium priority, regardless of what other values may be specified in script.pbs Execution environment variables \u00b6 Within the script contents of the job script, it is common for the specifics of the job to depend slightly on the PBS and specific module execution environment. Both running under PBS and loading certain module files create some environment variables that might be useful when writing portable scripts; for example scripts that might be shared among users or executed within several different configurations. Use common environment variables to write portable PBS batch scripts Avoid hard-coding paths into your shell scripts if instead any of the environment variables below might be used. This will facilitate moving scripts between systems, users, and application versions with minimal modifications, as output paths can be defined generically as opposed to hard-coded for each user. PBS execution environment variables \u00b6 PBS creates a number of environment variables that are accessible within a job's execution environment. Some of the more useful ones are: Variable Value PBS_ACCOUNT The NCAR Project Accounting code used for this job. PBS_JOBID The PBS Job ID for this job. Example: 1473351.desched1 PBS_JOBNAME The name of this job. Matches the -N specified. Example: hello_pbs PBS_O_WORKDIR The working directory from where the job was submitted. PBS_SELECT The resource specification -l select= line for this job. This can be useful for setting runtime-specific configuration options that might depend on resource selection. (e.g. processor layout, CPU binding, etc...) Example: 2:ncpus=128:mpiprocs=2:ompthreads=2:mem=200GB:Qlist=cpu:ngpus=0 PBS_NODEFILE A file whose contents lists the nodes assigned to this job. Typically listed as one node name per line, for each MPI rank in the job. Each node will be listed for as many times as it has MPI ranks. Example: /var/spool/pbs/aux/1473351.desched1 NCAR module -specific execution environment variables \u00b6 Variable Value Machine and Software Environment NCAR_HOST Specifies the host class of machine, e.g. derecho or casper NCAR_BUILD_ENV_COMPILER A unique string identifying the host and compiler+version currently loaded. Example: casper-oneapi-2023.2.1 NCAR_BUILD_ENV_MPI A unique string identifying the host, compiler+version, and mpi+version currently loaded. Example: casper-oneapi-2023.2.1-openmpi-4.1.5 NCAR_BUILD_ENV A unique string identifying the current build environment , identical to NCAR_BUILD_ENV_MPI when an MPI module is loaded, or NCAR_BUILD_ENV_COMPILER if only a compiler is loaded. LMOD_FAMILY_COMPILER LMOD_FAMILY_COMPILER_VERSION Specifies the type and version of compiler currently loaded, if any. Example: intel , gcc , nvhpc LMOD_FAMILY_MPI LMOD_FAMILY_MPI_VERSION Specifies the type and version of MPI currently loaded, if any. Example: openmpi , cray-mpich , intel-mpi User and File System Paths ${USER} The username of user executing the script. ${HOME} The GLADE home file space for the user executing the script. Example: /glade/u/home/${USER} ${WORK} The GLADE work file space for the user executing the script. Example: /glade/work/${USER} ${SCRATCH} The GLADE scratch file space for the user executing the script. Example: /glade/derecho/scratch/${USER} PBS Job Arrays \u00b6 Occasionally users may want to execute a large number of similar jobs. Such workflows may arise when post-processing a large number of similar files, for example, often with a serial post-processing tool. One approach is simply to create a unique job script for each. While simple, this approach has some drawbacks, namely scheduler overhead and job management complexity. PBS provides a convenient job array mechanism for such cases. When using job arrays, the queue script contents can be thought of as a template that is applied repeatedly, for different instances of the PBS_ARRAY_INDEX . Consider the following example: PBS job array Suppose your working directory contains a number of files data.year-2010 , data.year-2011 , ..., data.year-2020 . You would like to run ./executable_name on each. job_array.pbs #PBS -N job_array #PBS -A <project_code> ### Each array sub-job will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=00:10:00 #PBS -q casper ### Request 11 sub-jobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program on a specific input file: ./executable_name data.year- ${ PBS_ARRAY_INDEX } The directive -J 2010-2020 instructs the scheduler to launch a number of sub-jobs , each with a distinct value of PBS_ARRAY_INDEX spanning from 2010 to 2020 (inclusive). The -l select=1:ncpus=1:mem=4GB resource chunk and -l walltime=00:10:00 run time limit applies to each sub-job . PBS job arrays provide a convenient mechanism to launch a large number of small, similar jobs. The approach outlined above is particularly suitable for Casper, where nodes are typically shared and individual CPU cores are scheduled. This allows a job array sub-job to be as small as a single core. When entire compute nodes are assigned to jobs (and therefore also array sub-jobs) we need a slightly different approach, as employed in the following use case. Using job arrays to launch a \"command file\" \u00b6 Multiple Program, Multiple Data (MPMD) jobs run multiple independent, typically serial executables simultaneously. Such jobs can easily be dispatched with PBS job arrays, even on machines like Derecho where compute nodes are exclusively and entirely assigned to a users' job. This process is outlined in the example below. Command File / MPMD jobs with PBS Job Arrays For this example, executable commands appear in a command file ( cmdfile ). cmdfile # this is a comment line for demonstration ./cmd1.exe < input1 # comments are allowed for steps too, but not required ./cmd2.exe < input2 ./cmd3.exe < input3 ... ./cmdN.exe < inputN The command file, executables, and input files should all reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full paths in both your command file and job scripts. The sub-jobs will produce output files that reside in the directory in which the job was submitted. The command file can then be executed with the launch_cf command. Examples : # launches the commands listed in ./cmdfile: launch_cf -A PBS_ACCOUNT -l walltime=1:00:00 # launches the OpenMP-threaded commands listed in ./omp_cmdfile: # (requires ppn=128 = (32 steps/node) * (4 threads/step) launch_cf -A PBS_ACCOUNT -l walltime=1:00:00 --nthreads 4 --steps-per-node 32 ./omp_cmdfile The jobs listed in the cmdfile will be launched from a bash shell in the users default environment. The optional file config_env.sh will be \"sourced\" from the run directory in case environment modification is required, for example to load a particular module environment, to set file paths, etc... The command will assume reasonable defaults on Derecho and Casper for the number of job \"steps\" from the cmdfile to run per node, memory per node, PBS queues, etc... Each of these parameters can be controlled via launch_cf command line arguments, see launch_cf --help : launch_cf command line options launch_cf <-h|--help> <--queue PBS_QUEUE> <--ppn|--processors-per-node #CPUS> <--steps-per-node #Steps/node> <--nthreads|--threads-per-step #Threads/step> <--mem|--memory RAM/node> -A PBS_ACCOUNT -l walltime=01:00:00 ... other PBS args ... <command file> ------------------------------------------------------------------ All options in \"<>\" brackets are optional. Any unrecognized arguments are passed through directly to qsub. The PBS options -A and -l walltime are required at minimum. The two PBS required arguments are -A <project_code> and -l walltime=... . Any command line argument not interpreted directly by launch_cf are assumed PBS arguments and are passed along to qsub . Discussion This PBS array implementation is a departure from the command file technique used previously on Cheyenne , where MPI was used to launch the desired commands on each rank. While slightly more complex, the array approach has several advantages. Since the array steps are independent, the job can begin execution as soon as even a single node is available, and can scale to fill the available resources. Additionally, the array approach is well suited for when the run times of the specific commands varies. In the previous MPI approach, all nodes were held until the slowest step completed, with the consequence of idle resources for varied command run times. With the array approach each node completes independently, when the slowest of its unique steps has completed. Thus the utilization of each node is controlled by the run times of its own steps, rather than all steps. The implementation details are unimportant for general users exercising this capability, however may be interesting for advanced users wishing to leverage PBS job arrays in different scenarios. See the hpc-demos GitHub repository for source code. Sample PBS job scripts \u00b6 Casper \u00b6 Batch script to run a high-throughput computing (HTC) job on Casper This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU. bash tcsh #!/bin/bash -l ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat #!/bin/tcsh ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat Batch script to run an MPI GPU job on Casper bash tcsh #!/bin/bash -l #PBS -N mpi_job #PBS -A <project_code> #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB #PBS -l gpu_type=v100 #PBS -l walltime=01:00:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name #!/bin/tcsh ### Job Name #PBS -N mpi_gpu_job ### Charging account #PBS -A <project_code> ### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB ### Specify that the GPUs will be V100s #PBS -l gpu_type=v100 ### Allow job to run up to 1 hour #PBS -l walltime=01:00:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name Batch script to run a pure OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name #!/bin/tcsh #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name Batch script to run a hybrid MPI/OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name #!/bin/tcsh #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name Batch script to run a job array on Casper Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. This example uses environment variable PBS_ARRAY_INDEX as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the #PBS -J array directive. bash tcsh #!/bin/bash -l #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=00:10:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX #!/bin/tcsh #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=01:00:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX If you need to include a job ID in a subsequent qsub command, be sure to use quotation marks to preserve the [] brackets, as in this example: qsub -W \"depend=afterok:317485[]\" postprocess.pbs Using NVIDIA MPS in Casper GPU jobs Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper. Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU. The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk: Launch the MPS control daemon on each job node. Start the MPS server on each node. Run your GPU application. Terminate the MPS server and daemon. To enable MPS on job hosts, add mps=1 to your select statement chunks as follows: #PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1 On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time. Jobs may not request MPS activation on nodes with GP100 GPUs. In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU. bash #!/bin/bash #PBS -A <project_code> #PBS -N gpu_mps_job #PBS -q casper@casper-pbs #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1 #PBS -l gpu_type=v100 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4 # Run application using Open MPI mpirun ./executable_name Derecho \u00b6 Running a hybrid CPU program with MPI and OpenMP on Derecho In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A <project_code> #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name Running an MPI-enabled GPU application on Derecho In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A <project_code> #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018set_gpu_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The set_gpu_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name Binding MPI ranks to CPU cores and GPU devices on Derecho For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: set_gpu_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name Running a containerized application under MPI on GPUs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" See here for a more complete discussion of the nuances of containerized applications on Derecho.","title":"Portable Batch System (PBS) Job Scripts"},{"location":"pbs/job-scripts/#portable-batch-system-pbs-job-scripts","text":"Job scripts form the basis of batch jobs . A job script is simply a text file with instructions of the work to execute. Job scripts are usually written in bash or tcsh and thus mimic commands a user would execute interactively through a shell; but instead are executed on specific resources allocated by the scheduler when available. Scripts can also be written in other languages - commonly Python.","title":"Portable Batch System (PBS) Job Scripts"},{"location":"pbs/job-scripts/#anatomy-of-a-job-script","text":"Sample basic PBS scripts are listed below: PBS job scripts bash tcsh Python #!/bin/bash #PBS -N hello_pbs #PBS -A <project_code> #PBS -j oe #PBS -k eod #PBS -q main #PBS -l walltime=00:05:00 #PBS -l select=2:ncpus=128:mpiprocs=128 ### Set temp to scratch export TMPDIR = ${ SCRATCH } / ${ USER } /temp && mkdir -p $TMPDIR ### specify desired module environment module purge module load ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25 module list ### Compile & Run MPI Program mpicxx -o hello_world_derecho /glade/u/home/benkirk/hello_world_mpi.C -fopenmp mpiexec -n 256 -ppn 128 ./hello_world_derecho The first line denotes the interpreter to be used for the script: #!/bin/bash #!/bin/tcsh #PBS -N hello_pbs #PBS -A <project_code> #PBS -j oe #PBS -k eod #PBS -q main #PBS -l walltime=00:05:00 #PBS -l select=2:ncpus=128:mpiprocs=128 source /etc/csh.cshrc ### Set temp to scratch setenv TMPDIR ${ SCRATCH } / ${ USER } /temp && mkdir -p ${ TMPDIR } ### specify desired module environment module purge module load ncarenv/23.09 gcc/12.2.0 cray-mpich/8.1.25 module list ### Compile & Run MPI Program mpicxx -o hello_world_derecho /glade/u/home/benkirk/hello_world_mpi.C -fopenmp mpiexec -n 256 -ppn 128 ./hello_world_derecho The first line denotes the interpreter to be used for the script: #!/bin/tcsh #!/glade/u/apps/opt/conda/envs/npl/bin/python #PBS -N hello_pbs #PBS -A <project_code> #PBS -j oe #PBS -k eod #PBS -q main #PBS -l walltime=00:05:00 #PBS -l select=1:ncpus=128:mpiprocs=128 import sys print ( \"Hello, world!! \\n\\n \" ) print ( \"Python version:\" ) print ( sys . version ) print ( \"Version info:\" ) print ( sys . version_info ) The first line denotes the interpreter to be used for the script: #!/glade/u/apps/opt/conda/envs/npl/bin/python indicates this is a python script (and, specifically, the NCAR NPL instance). Focusing on the bash example for discussion , the remainder of the script contains two main sections: The lines beginning with #PBS are directives that will be interpreted by PBS when this script is submitted with qsub . Each of these lines contains an instruction that will be used by qsub to control job resources, execution, etc... The remaining script contents are simply bash commands that will be run inside the batch environment on the selected resources and define the work to be done in this job.","title":"Anatomy of a Job Script"},{"location":"pbs/job-scripts/#pbs-directives","text":"The example above contains several directives which are interpreted by the qsub submission program: -N hello_pbs provides a job name . This name will be displayed by the scheduler for diagnostic and file output. If omitted, and a script is used to submit the job, the job's name is the name of the script. -A <project_code> indicates which NCAR Project Accounting code resource allocation will be applicable to this job. (You will want to replace <project_code> with your project's specific code.) -j oe requests we combine any standard text output ( o ) and error ( e ) into one output file . (By default, PBS will write program output and error to different log files. This behavior is contrary to what many users expect from terminal interaction, where output and error are generally interspersed. This optional flag changes that behavior.) -q main specifies the desired PBS queue for this job. -l walltime=00:05:00 requests 5 minutes as the maximum job execution ( walltime ) time. Specified in HH:MM:SS format. -l select=2:ncpus=128:mpiprocs=128 is a computational resource chunk request, detailing the quantity and configuration of compute nodes required for this job. This example requests a selection of 2 nodes, where each node must have 128 CPU cores, each of which we will use as an MPI rank in our application.","title":"PBS directives"},{"location":"pbs/job-scripts/#script-contents","text":"The remaining script contains shell commands that define the job execution workflow. The commands here are arbitrary, however we strongly recommend the general structure presented above. This includes: Explicitly setting the TMPDIR variable . As described here , many programs write temporary data to TMPDIR , which is usually small and shared among st users. Specifying your own directory for temporary files can help you avoid the risk of your own programs and other users' programs failing when no more space is available. Loading and reporting the specific module environment required for this job. While strictly not necessary (in general, the system default modules will be loaded anyway), we recommend this as best practice as it facilitates debugging and reproducing later. (While the system default modules will change over time, manually specifying module versions allows you to recreate the same execution environment in the future.) ( Optional ) Defining any environment variables specific to the chosen module environment. Occasionally users will want to define particular run time environment variables e.g. for a specific MPI or library chosen via the module load commands. Remaining job-specific steps. In the example above, we first compile and then execute hello_world_mpi.C , a simple MPI program.","title":"Script contents"},{"location":"pbs/job-scripts/#common-pbs-directives","text":"","title":"Common #PBS directives"},{"location":"pbs/job-scripts/#resource-requests","text":"Resources (compute node configuration, job duration) are requested through a combination of resource selection flags, each preceded with -l . For example: #PBS -l walltime=00:05:00 #PBS -l select=1:ncpus=64:mpiprocs=4:ngpus=4:mem=400GB #PBS -l gpu_type=a100 #PBS -l job_priority=economy specifies job walltime , compute node selection, GPU type, and job priority. See more details below.","title":"Resource requests"},{"location":"pbs/job-scripts/#select-statements","text":"Resources are specified through a select statement. The general form of a homogeneous selection statement is select=<# NODES>:ncpus=<# CPU Cores/node>:mpiprocs=<# MPI Ranks/node>:ompthreads=<# OpenMP Threads/rank>:mem=<RAM/node>:ngpus=<# GPUs/node> where <# NODES> is the total number of compute nodes requested, followed by a colon-separated list of <# CPU Cores/node> is the total number of CPUs requested on each node , which can be a mix of MPI Ranks and/or OpenMP threads, <# MPI Ranks/node is the number of MPI Ranks on each node , <# OpenMP Threads/node> is the number of OpenMP ranks per MPI Rank on each node . (Optional, defaults to 1), <RAM/node> is how much main memory (RAM) the job will be able to access on each node . (Optional, default is system dependent), and <# GPUs/node> is the number of GPUs per node . (Optional, defaults to 0). Taken together, this specifies a resource chunk . Homogeneous resource chunks are the most common case, however, heterogeneous selection statements can be constructed by multiple chunks separated by a + (examples below).","title":"select statements"},{"location":"pbs/job-scripts/#examples","text":"4 128-core nodes, each running 128 MPI ranks (4 x 128 = 512 MPI ranks total). select=4:ncpus=128:mpiprocs=128 4 128-core nodes, each running 32 MPI ranks with 4 OpenMP threads per ranl (4 x 32 = 128 MPI ranks total, each with 4 threads = 512 total CPU cores). select=4:ncpus=128:mpiprocs=32:ompthreads=4 2 64-core nodes, each running 4 MPI ranks, 4 GPUS, and 384 GB memory (8 GPUs total, with 8 MPI ranks). select=2:ncpus=64:mpiprocs=4:ngpus=4:mem=384GB 4 36-core nodes, each running 4 MPI ranks, 4 GPUS configured with NVIDIA's Multi-Process Service (MPS), and 768 GB memory (16 GPUs total, with 16 MPI ranks). select=4:ncpus=36:mpiprocs=4:ngpus=4:mem=768GB:mps=1 MPS is simply enabled via mps=1 , and is disabled by default ( mps=0 ) A heterogeneous selection, 96 128-core nodes each with 128 MPI ranks, and 32 128-core nodes each with 16 MPI ranks and 8 OpenMP threads select=96:ncpus=128:mpiprocs=128+32:ncpus=16:ompthreads=8 The particular values for ncpus , mem , ngpus are node-type dependent, and most NCAR systems have more than one available node type. (See system specific documentation for recommended values.) Request all ncpus when running on exclusive nodes For large multi-node jobs on machines like Derecho nodes are usually assigned exclusively to a single PBS job at a time. For most use cases, users will request the maximum number of CPUS available via ncpus , and consume all through a combination of mpiprocs and ompthreads . Occasionally users may want fewer than the maximum CPUs for computation, \"under-subscribing\" compute nodes. This is usually done for memory intensive applications, where some cores are intentionally left idle in order to increase the memory available for the running cores. In such circumstances users should still request access to all CPUs, but only use a subset. For example select = 4 :ncpus = 128 :mpiprocs = 64 :ompthreads = 1 :mem = 235GB requests access to all 128 CPUs on a dedicated node, but only assigns 64 for MPI use. By requesting access to all ncpus=128 is recommended for this case because it allows optimally locating the actually used mpiprocs=64 across the compute node via process binding .","title":"Examples"},{"location":"pbs/job-scripts/#walltime","text":"The -l walltime=HH:MM:SS resource directive specifies maximum job duration. Jobs still running when this wall time is exceeded will be terminated automatically by the scheduler. walltime=HH:MM:SS","title":"walltime"},{"location":"pbs/job-scripts/#job_priority","text":"Users may request a specific job priority with the -l job_priority=... resource directive. Valid options are: job_priority=<regular|premium|economy> Job priority impacts both scheduling and resource accounting, allowing users to run at a higher/lower priority in exchange for additional/reduced allocation consumption. See here for additional information.","title":"job_priority"},{"location":"pbs/job-scripts/#gpu_type","text":"For highly heterogeneous systems such as Casper, a resource chunk statement including GPUS may match more than one particular GPU type. The resource specification -l gpu_type=... requests a particular GPU type, removing such ambiguity. Valid options are: gpu_type=<gp100|v100|a100>","title":"gpu_type"},{"location":"pbs/job-scripts/#listing-of-frequently-used-pbs-directives","text":"Directive Impact -A <project_code> NCAR Project Accounting string associated with the job. -a <date at time> Allows users to request a future eligible time for job execution. (By default jobs are considered immediately eligible for execution.) Format: [[[YY]MM]DD]hhmm[.SS] -h Holds the job. <Held jobs can be released with qrls . -I Specifies interactive execution. Interactive jobs place the user a login session on the first compute node. Interactive jobs terminate when the shell exits, or walltime is exceeded. -J <range> Specifies an array job . Use the range argument to specify the indices of the sub jobs of the array. range is specified in the form X-Y[:Z] where X is the first index, Y is the upper bound on the indices, and Z is the stepping factor. Indices must be greater than or equal to zero. Use the optional %max_subjobs argument to set a limit on the number of subjobs that can be running at one time. more details on array jobs -m <mail events> Sends email on specific events (may be combined). n : No mail is sent a : Mail is sent when the job is aborted by the batch system b : Mail is sent when the job begins execution e : Mail is sent when the job terminates Example: -m abe -M <address(es)> List of users to whom mail about the job is sent. The user list argument has the form: <username>[@<hostname>][,<username>[@<hostname>],...] qsub arguments take precedence over #PBS directives Best practice is to fully specify your PBS queue, job name, and resources in your job script as shown above. This allows for better debugging and facilitates reproducing runs in the future. When a job's PBS attributes are fully specified, you can usually submit the script with no additional arguments, for example qsub script.pbs (See Running Jobs for more details on interacting with the scheduler.) On occasion users may want to change some of the PBS parameters without modifying the job script. A common example may be the account code, the job name ( -N ) or even the walltime . Any #PBS directives specified in the job script can be overridden at submission time by equivalent arguments to qsub . For example, qsub -A <OTHER_ACCOUNT> \\ -N testing \\ -l job_priority = premium \\ script.pbs will run script.pbs under the specified <OTHER_ACCOUNT> with the job name testing and requests premium priority, regardless of what other values may be specified in script.pbs","title":"Listing of frequently used #PBS directives"},{"location":"pbs/job-scripts/#execution-environment-variables","text":"Within the script contents of the job script, it is common for the specifics of the job to depend slightly on the PBS and specific module execution environment. Both running under PBS and loading certain module files create some environment variables that might be useful when writing portable scripts; for example scripts that might be shared among users or executed within several different configurations. Use common environment variables to write portable PBS batch scripts Avoid hard-coding paths into your shell scripts if instead any of the environment variables below might be used. This will facilitate moving scripts between systems, users, and application versions with minimal modifications, as output paths can be defined generically as opposed to hard-coded for each user.","title":"Execution environment variables"},{"location":"pbs/job-scripts/#pbs-execution-environment-variables","text":"PBS creates a number of environment variables that are accessible within a job's execution environment. Some of the more useful ones are: Variable Value PBS_ACCOUNT The NCAR Project Accounting code used for this job. PBS_JOBID The PBS Job ID for this job. Example: 1473351.desched1 PBS_JOBNAME The name of this job. Matches the -N specified. Example: hello_pbs PBS_O_WORKDIR The working directory from where the job was submitted. PBS_SELECT The resource specification -l select= line for this job. This can be useful for setting runtime-specific configuration options that might depend on resource selection. (e.g. processor layout, CPU binding, etc...) Example: 2:ncpus=128:mpiprocs=2:ompthreads=2:mem=200GB:Qlist=cpu:ngpus=0 PBS_NODEFILE A file whose contents lists the nodes assigned to this job. Typically listed as one node name per line, for each MPI rank in the job. Each node will be listed for as many times as it has MPI ranks. Example: /var/spool/pbs/aux/1473351.desched1","title":"PBS execution environment variables"},{"location":"pbs/job-scripts/#ncar-module-specific-execution-environment-variables","text":"Variable Value Machine and Software Environment NCAR_HOST Specifies the host class of machine, e.g. derecho or casper NCAR_BUILD_ENV_COMPILER A unique string identifying the host and compiler+version currently loaded. Example: casper-oneapi-2023.2.1 NCAR_BUILD_ENV_MPI A unique string identifying the host, compiler+version, and mpi+version currently loaded. Example: casper-oneapi-2023.2.1-openmpi-4.1.5 NCAR_BUILD_ENV A unique string identifying the current build environment , identical to NCAR_BUILD_ENV_MPI when an MPI module is loaded, or NCAR_BUILD_ENV_COMPILER if only a compiler is loaded. LMOD_FAMILY_COMPILER LMOD_FAMILY_COMPILER_VERSION Specifies the type and version of compiler currently loaded, if any. Example: intel , gcc , nvhpc LMOD_FAMILY_MPI LMOD_FAMILY_MPI_VERSION Specifies the type and version of MPI currently loaded, if any. Example: openmpi , cray-mpich , intel-mpi User and File System Paths ${USER} The username of user executing the script. ${HOME} The GLADE home file space for the user executing the script. Example: /glade/u/home/${USER} ${WORK} The GLADE work file space for the user executing the script. Example: /glade/work/${USER} ${SCRATCH} The GLADE scratch file space for the user executing the script. Example: /glade/derecho/scratch/${USER}","title":"NCAR module-specific execution environment variables"},{"location":"pbs/job-scripts/#pbs-job-arrays","text":"Occasionally users may want to execute a large number of similar jobs. Such workflows may arise when post-processing a large number of similar files, for example, often with a serial post-processing tool. One approach is simply to create a unique job script for each. While simple, this approach has some drawbacks, namely scheduler overhead and job management complexity. PBS provides a convenient job array mechanism for such cases. When using job arrays, the queue script contents can be thought of as a template that is applied repeatedly, for different instances of the PBS_ARRAY_INDEX . Consider the following example: PBS job array Suppose your working directory contains a number of files data.year-2010 , data.year-2011 , ..., data.year-2020 . You would like to run ./executable_name on each. job_array.pbs #PBS -N job_array #PBS -A <project_code> ### Each array sub-job will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=00:10:00 #PBS -q casper ### Request 11 sub-jobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program on a specific input file: ./executable_name data.year- ${ PBS_ARRAY_INDEX } The directive -J 2010-2020 instructs the scheduler to launch a number of sub-jobs , each with a distinct value of PBS_ARRAY_INDEX spanning from 2010 to 2020 (inclusive). The -l select=1:ncpus=1:mem=4GB resource chunk and -l walltime=00:10:00 run time limit applies to each sub-job . PBS job arrays provide a convenient mechanism to launch a large number of small, similar jobs. The approach outlined above is particularly suitable for Casper, where nodes are typically shared and individual CPU cores are scheduled. This allows a job array sub-job to be as small as a single core. When entire compute nodes are assigned to jobs (and therefore also array sub-jobs) we need a slightly different approach, as employed in the following use case.","title":"PBS Job Arrays"},{"location":"pbs/job-scripts/#using-job-arrays-to-launch-a-command-file","text":"Multiple Program, Multiple Data (MPMD) jobs run multiple independent, typically serial executables simultaneously. Such jobs can easily be dispatched with PBS job arrays, even on machines like Derecho where compute nodes are exclusively and entirely assigned to a users' job. This process is outlined in the example below. Command File / MPMD jobs with PBS Job Arrays For this example, executable commands appear in a command file ( cmdfile ). cmdfile # this is a comment line for demonstration ./cmd1.exe < input1 # comments are allowed for steps too, but not required ./cmd2.exe < input2 ./cmd3.exe < input3 ... ./cmdN.exe < inputN The command file, executables, and input files should all reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full paths in both your command file and job scripts. The sub-jobs will produce output files that reside in the directory in which the job was submitted. The command file can then be executed with the launch_cf command. Examples : # launches the commands listed in ./cmdfile: launch_cf -A PBS_ACCOUNT -l walltime=1:00:00 # launches the OpenMP-threaded commands listed in ./omp_cmdfile: # (requires ppn=128 = (32 steps/node) * (4 threads/step) launch_cf -A PBS_ACCOUNT -l walltime=1:00:00 --nthreads 4 --steps-per-node 32 ./omp_cmdfile The jobs listed in the cmdfile will be launched from a bash shell in the users default environment. The optional file config_env.sh will be \"sourced\" from the run directory in case environment modification is required, for example to load a particular module environment, to set file paths, etc... The command will assume reasonable defaults on Derecho and Casper for the number of job \"steps\" from the cmdfile to run per node, memory per node, PBS queues, etc... Each of these parameters can be controlled via launch_cf command line arguments, see launch_cf --help : launch_cf command line options launch_cf <-h|--help> <--queue PBS_QUEUE> <--ppn|--processors-per-node #CPUS> <--steps-per-node #Steps/node> <--nthreads|--threads-per-step #Threads/step> <--mem|--memory RAM/node> -A PBS_ACCOUNT -l walltime=01:00:00 ... other PBS args ... <command file> ------------------------------------------------------------------ All options in \"<>\" brackets are optional. Any unrecognized arguments are passed through directly to qsub. The PBS options -A and -l walltime are required at minimum. The two PBS required arguments are -A <project_code> and -l walltime=... . Any command line argument not interpreted directly by launch_cf are assumed PBS arguments and are passed along to qsub . Discussion This PBS array implementation is a departure from the command file technique used previously on Cheyenne , where MPI was used to launch the desired commands on each rank. While slightly more complex, the array approach has several advantages. Since the array steps are independent, the job can begin execution as soon as even a single node is available, and can scale to fill the available resources. Additionally, the array approach is well suited for when the run times of the specific commands varies. In the previous MPI approach, all nodes were held until the slowest step completed, with the consequence of idle resources for varied command run times. With the array approach each node completes independently, when the slowest of its unique steps has completed. Thus the utilization of each node is controlled by the run times of its own steps, rather than all steps. The implementation details are unimportant for general users exercising this capability, however may be interesting for advanced users wishing to leverage PBS job arrays in different scenarios. See the hpc-demos GitHub repository for source code.","title":"Using job arrays to launch a \"command file\""},{"location":"pbs/job-scripts/#sample-pbs-job-scripts","text":"","title":"Sample PBS job scripts"},{"location":"pbs/job-scripts/#casper","text":"Batch script to run a high-throughput computing (HTC) job on Casper This example shows how to create a script for running a high-throughput computing (HTC) job. Such jobs typically use only a few CPU cores and likely do not require the use of an MPI library or GPU. bash tcsh #!/bin/bash -l ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat #!/bin/tcsh ### Job Name #PBS -N htc_job ### Charging account #PBS -A <project_code> ### Request one chunk of resources with 1 CPU and 10 GB of memory #PBS -l select=1:ncpus=1:mem=10GB ### Allow job to run up to 30 minutes #PBS -l walltime=30:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Load Conda/Python module and activate NPL environment module load conda conda activate npl ### Run analysis script python myscript.py datafile.dat Batch script to run an MPI GPU job on Casper bash tcsh #!/bin/bash -l #PBS -N mpi_job #PBS -A <project_code> #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB #PBS -l gpu_type=v100 #PBS -l walltime=01:00:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name #!/bin/tcsh ### Job Name #PBS -N mpi_gpu_job ### Charging account #PBS -A <project_code> ### Request two resource chunks, each with 4 CPUs, GPUs, MPI ranks, and 40 GB of memory #PBS -l select=2:ncpus=4:mpiprocs=4:ngpus=4:mem=40GB ### Specify that the GPUs will be V100s #PBS -l gpu_type=v100 ### Allow job to run up to 1 hour #PBS -l walltime=01:00:00 ### Route the job to the casper queue #PBS -q casper ### Join output and error streams into single file #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Provide CUDA runtime libraries module load cuda ### Run program mpirun ./executable_name Batch script to run a pure OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name #!/bin/tcsh #PBS -N OpenMP_job #PBS -A <project_code> #PBS -l select=1:ncpus=8:ompthreads=8 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name Batch script to run a hybrid MPI/OpenMP job on Casper bash tcsh #!/bin/bash -l #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name #!/bin/tcsh #PBS -N hybrid_job #PBS -A <project_code> #PBS -l select=2:ncpus=8:mpiprocs=2:ompthreads=4 #PBS -l walltime=00:10:00 #PBS -q casper #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program mpirun ./executable_name Batch script to run a job array on Casper Job arrays are useful for submitting and managing collections of similar jobs \u2013 for example, running the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. This example uses environment variable PBS_ARRAY_INDEX as an argument in running the jobs. This variable is set by the scheduler in each of your array subjobs, and spans the range of values set in the #PBS -J array directive. bash tcsh #!/bin/bash -l #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=00:10:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX #!/bin/tcsh #PBS -N job_array #PBS -A <project_code> ### Each array subjob will be assigned a single CPU with 4 GB of memory #PBS -l select=1:ncpus=1:mem=4GB #PBS -l walltime=01:00:00 #PBS -q casper ### Request 10 subjobs with array indices spanning 2010-2020 (input year) #PBS -J 2010-2020 #PBS -j oe setenv TMPDIR ${ SCRATCH } /temp mkdir -p ${ TMPDIR } ### Run program ./executable_name data.year- $PBS_ARRAY_INDEX If you need to include a job ID in a subsequent qsub command, be sure to use quotation marks to preserve the [] brackets, as in this example: qsub -W \"depend=afterok:317485[]\" postprocess.pbs Using NVIDIA MPS in Casper GPU jobs Some workflows benefit from processing more than one CUDA kernel on a GPU concurrently, as a single kernel is not sufficient to keep the GPU fully utilized. NVIDIA\u2019s Multi-Process Service (MPS) enables this capability on modern NVIDIA GPUs like the V100s on Casper. Consider using MPS when you are requesting more MPI tasks than physical GPUs. Particularly for jobs with large problem sizes, using multiple MPI tasks with MPS active can sometimes offer a performance boost over using a single task per GPU. The PBS job scheduler provides MPS support via a chunk-level resource. When you request MPS, PBS will perform the following steps on each specified chunk: Launch the MPS control daemon on each job node. Start the MPS server on each node. Run your GPU application. Terminate the MPS server and daemon. To enable MPS on job hosts, add mps=1 to your select statement chunks as follows: #PBS -l select=1:ncpus=8:mpiprocs=8:mem=60GB:ngpus=1:mps=1 On each V100 GPU, you may use MPI to launch up to 48 CUDA contexts (GPU kernels launched by MPI tasks) when using MPS. MPS can be used with OpenACC and OpenMP offload codes as well, as the compiler generates CUDA code from your directives at compile time. Jobs may not request MPS activation on nodes with GP100 GPUs. In this example, we run a CUDA Fortran program that also uses MPI. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and Open MPI. We request all GPUs on each node and use NVIDIA MPS to use multiple MPI tasks on CPU nodes for each GPU. bash #!/bin/bash #PBS -A <project_code> #PBS -N gpu_mps_job #PBS -q casper@casper-pbs #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=36:mpiprocs=36:ngpus=4:mem=300GB:mps=1 #PBS -l gpu_type=v100 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv nvhpc/22.5 cuda/11.4 openmpi/4.1.4 # Run application using Open MPI mpirun ./executable_name","title":"Casper"},{"location":"pbs/job-scripts/#derecho","text":"Running a hybrid CPU program with MPI and OpenMP on Derecho In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A <project_code> #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name Running an MPI-enabled GPU application on Derecho In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A <project_code> #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = ${ SCRATCH } /temp mkdir -p ${ TMPDIR } # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018set_gpu_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The set_gpu_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 set_gpu_rank ./executable_name Binding MPI ranks to CPU cores and GPU devices on Derecho For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: set_gpu_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./set_gpu_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./set_gpu_rank ./executable_name Running a containerized application under MPI on GPUs #!/bin/bash #PBS -q main #PBS -j oe #PBS -o fasteddy_job.log #PBS -l walltime=02:00:00 #PBS -l select=6:ncpus=64:mpiprocs=4:ngpus=4 module load ncarenv/23.09 module load apptainer gcc cuda || exit 1 module list nnodes = $( cat ${ PBS_NODEFILE } | sort | uniq | wc -l ) nranks = $( cat ${ PBS_NODEFILE } | sort | wc -l ) nranks_per_node = $(( ${ nranks } / ${ nnodes } )) container_image = \"./rocky8-openhpc-fasteddy.sif\" singularity \\ --quiet \\ exec \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ ${ container_image } \\ ldd /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy echo \"# --> BEGIN execution\" ; tstart = $( date +%s ) mpiexec \\ --np ${ nranks } --ppn ${ nranks_per_node } --no-transfer \\ set_gpu_rank \\ singularity \\ --quiet \\ exec \\ --bind ${ SCRATCH } \\ --bind ${ WORK } \\ --pwd $( pwd ) \\ --bind /run \\ --bind /opt/cray \\ --bind /usr/lib64:/host/lib64 \\ --env LD_LIBRARY_PATH = ${ CRAY_MPICH_DIR } /lib-abi-mpich:/opt/cray/pe/lib64: ${ LD_LIBRARY_PATH } :/host/lib64 \\ --env LD_PRELOAD = /opt/cray/pe/mpich/ ${ CRAY_MPICH_VERSION } /gtl/lib/libmpi_gtl_cuda.so.0 \\ --env MPICH_GPU_SUPPORT_ENABLED = 1 \\ --env MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 \\ --env MPICH_SMP_SINGLE_COPY_MODE = NONE \\ ${ container_image } \\ /opt/local/FastEddy-model/SRC/FEMAIN/FastEddy \\ ./Example02_CBL.in echo \"# --> END execution\" echo $(($( date +%s ) - ${ tstart } )) \" elapsed seconds; $( date ) \" See here for a more complete discussion of the nuances of containerized applications on Derecho.","title":"Derecho"},{"location":"storage-systems/","text":"Storage Resources, Data Transfer, and Data Access Resources \u00b6 CISL provides several broad classes of storage resources and data access mechanisms. Follow the links below for additional information. Storage Resources \u00b6 GLADE for typical POSIX file system use Quasar is a cold, tape-based archive for storing curated data collections that have an indefinite lifetime. Stratus is an object storage disk system for long-term data storage. Data Access and Data Transfer \u00b6 Dedicated data-access for facilitating data transfers. Tools such as Globus , and scp , sftp , bbcp for performing large data set transfers.","title":"Storage Resources, Data Transfer, and Data Access Resources"},{"location":"storage-systems/#storage-resources-data-transfer-and-data-access-resources","text":"CISL provides several broad classes of storage resources and data access mechanisms. Follow the links below for additional information.","title":"Storage Resources, Data Transfer, and Data Access Resources"},{"location":"storage-systems/#storage-resources","text":"GLADE for typical POSIX file system use Quasar is a cold, tape-based archive for storing curated data collections that have an indefinite lifetime. Stratus is an object storage disk system for long-term data storage.","title":"Storage Resources"},{"location":"storage-systems/#data-access-and-data-transfer","text":"Dedicated data-access for facilitating data transfers. Tools such as Globus , and scp , sftp , bbcp for performing large data set transfers.","title":"Data Access and Data Transfer"},{"location":"storage-systems/cmip-analysis-platform/","text":"CMIP Analysis Platform \u00b6 The CMIP Analysis Platform gives researchers convenient access to climate data from the Coupled Model Inter-comparison Project (CMIP) on CISL\u2019s GLADE disk storage resource. Note The tables below show which data sets are already available on GLADE. They are updated each Friday. These data are be located under /glade/collections/cmip/ If what you need is not already on GLADE, please review the additional documentation in the Data Services section of the CISL web site and submit a request. CMIP6 data sets on GLADE \u00b6 CMIP5 data sets on GLADE \u00b6","title":"CMIP Analysis Platform"},{"location":"storage-systems/cmip-analysis-platform/#cmip-analysis-platform","text":"The CMIP Analysis Platform gives researchers convenient access to climate data from the Coupled Model Inter-comparison Project (CMIP) on CISL\u2019s GLADE disk storage resource. Note The tables below show which data sets are already available on GLADE. They are updated each Friday. These data are be located under /glade/collections/cmip/ If what you need is not already on GLADE, please review the additional documentation in the Data Services section of the CISL web site and submit a request.","title":"CMIP Analysis Platform"},{"location":"storage-systems/cmip-analysis-platform/#cmip6-data-sets-on-glade","text":"","title":"CMIP6 data sets on GLADE"},{"location":"storage-systems/cmip-analysis-platform/#cmip5-data-sets-on-glade","text":"","title":"CMIP5 data sets on GLADE"},{"location":"storage-systems/data-access-nodes/","text":"Using data-access nodes \u00b6 The data-access nodes are provided to enable authorized users to perform a number of file-management operations. These include: managing file and directory permissions for NCAR Campaign Storage data holdings. copying data from the Research Data Archive . The data-access nodes are not compute nodes and any tasks users run on them are run \"at risk.\" They also are not intended for use as long-term storage. Short, non-memory-intensive processes are allowed, including such tasks as text editing or running small scripts for transferring files. If any task consumes excessive resources (determined at the discretion of the CISL consultants or system administrators), a system administrator will kill the process or processes and you will be notified. No usage charges are assessed for reading data. If you do not have an allocation but need to use the nodes to get data from CISL storage resources, submit a Data Analysis Allocation request. Logging in \u00b6 To use these resources, log in to data-access.ucar.edu and authenticate as described in our Getting Started documentation. Use ssh as follows: ssh username@data-access.ucar.edu Using GLADE scratch \u00b6 When you log in, you are in /glade/u/home/username by default. Use your larger scratch or work file spaces to hold data that you read out of Campaign Storage or other storage resources. You can also read files from other GLADE file spaces to which you have been granted read permissions. See GLADE for more information about file spaces. Transferring files \u00b6 To transfer data between data-access.ucar.edu and another system, use any of these tools: Globus \u2013 the endpoint for data-access projects is NCAR GLADE SCP/SFTP and similar SSH-based tools See those links for documentation.","title":"Data Access Nodes"},{"location":"storage-systems/data-access-nodes/#using-data-access-nodes","text":"The data-access nodes are provided to enable authorized users to perform a number of file-management operations. These include: managing file and directory permissions for NCAR Campaign Storage data holdings. copying data from the Research Data Archive . The data-access nodes are not compute nodes and any tasks users run on them are run \"at risk.\" They also are not intended for use as long-term storage. Short, non-memory-intensive processes are allowed, including such tasks as text editing or running small scripts for transferring files. If any task consumes excessive resources (determined at the discretion of the CISL consultants or system administrators), a system administrator will kill the process or processes and you will be notified. No usage charges are assessed for reading data. If you do not have an allocation but need to use the nodes to get data from CISL storage resources, submit a Data Analysis Allocation request.","title":"Using data-access nodes"},{"location":"storage-systems/data-access-nodes/#logging-in","text":"To use these resources, log in to data-access.ucar.edu and authenticate as described in our Getting Started documentation. Use ssh as follows: ssh username@data-access.ucar.edu","title":"Logging in"},{"location":"storage-systems/data-access-nodes/#using-glade-scratch","text":"When you log in, you are in /glade/u/home/username by default. Use your larger scratch or work file spaces to hold data that you read out of Campaign Storage or other storage resources. You can also read files from other GLADE file spaces to which you have been granted read permissions. See GLADE for more information about file spaces.","title":"Using GLADE scratch"},{"location":"storage-systems/data-access-nodes/#transferring-files","text":"To transfer data between data-access.ucar.edu and another system, use any of these tools: Globus \u2013 the endpoint for data-access projects is NCAR GLADE SCP/SFTP and similar SSH-based tools See those links for documentation.","title":"Transferring files"},{"location":"storage-systems/data-transfer/","text":"Data transfers and sharing \u00b6 The best way to transfer files between systems or to share data with colleagues depends on the context. For example, you might want to move or copy data that you own from the GLADE system to a university storage resource or from GLADE to NCAR Campaign Storage. At another time, you might want to share data that you have on GLADE with someone who does not have access to NCAR systems. There are several ways of accomplishing these transfer and sharing tasks. Review the descriptions of each below and follow the links provided for details. Transferring files between systems \u00b6 Globus \u00b6 CISL recommends using Globus to transfer files between systems \u2013 for example, between non-NCAR facilities and the resources that CISL manages. The Globus platform also enables users to transfer files between their GLADE file spaces and the NCAR Campaign Storage file system. Globus has both web and command line interfaces, and its Globus Connect feature enables users to move files to and from laptop or desktop computers and other systems. Globus has a typical transfer rate that ranges from 100 to 200 MBps. More information. SCP and SFTP \u00b6 The installed Secure Copy Protocol (SCP) and Secure FTP (SFTP) utilities are best suited to transferring small numbers of small files to or from a local computer and the resources that CISL manages. More information . These other tools also can be used to make secure transfers between your local computer and CISL systems: PSCP / PSFTP \u2013 PuTTY utilities including a Windows version WinSCP \u2013 SCP and SFTP transfers with a graphical user interface for Windows users BBCP \u00b6 BBCP is a multi-streaming utility for transferring large files. It splits files into multiple streams that are transferred simultaneously, so it is faster than the single-streaming SCP and SFTP utilities. More information . Sharing data with colleagues \u00b6 Globus \u00b6 In addition to the features described above, Globus enables users to share files with colleagues and others who do not have NCAR user accounts. More information . Sharing via Google Drive and other services \u00b6 Individuals who have GLADE access can copy files from GLADE and then share them with others via Google Drive or services such as DropBox and Amazon Web Services. More information .","title":"Data transfers and sharing"},{"location":"storage-systems/data-transfer/#data-transfers-and-sharing","text":"The best way to transfer files between systems or to share data with colleagues depends on the context. For example, you might want to move or copy data that you own from the GLADE system to a university storage resource or from GLADE to NCAR Campaign Storage. At another time, you might want to share data that you have on GLADE with someone who does not have access to NCAR systems. There are several ways of accomplishing these transfer and sharing tasks. Review the descriptions of each below and follow the links provided for details.","title":"Data transfers and sharing"},{"location":"storage-systems/data-transfer/#transferring-files-between-systems","text":"","title":"Transferring files between systems"},{"location":"storage-systems/data-transfer/#globus","text":"CISL recommends using Globus to transfer files between systems \u2013 for example, between non-NCAR facilities and the resources that CISL manages. The Globus platform also enables users to transfer files between their GLADE file spaces and the NCAR Campaign Storage file system. Globus has both web and command line interfaces, and its Globus Connect feature enables users to move files to and from laptop or desktop computers and other systems. Globus has a typical transfer rate that ranges from 100 to 200 MBps. More information.","title":"Globus"},{"location":"storage-systems/data-transfer/#scp-and-sftp","text":"The installed Secure Copy Protocol (SCP) and Secure FTP (SFTP) utilities are best suited to transferring small numbers of small files to or from a local computer and the resources that CISL manages. More information . These other tools also can be used to make secure transfers between your local computer and CISL systems: PSCP / PSFTP \u2013 PuTTY utilities including a Windows version WinSCP \u2013 SCP and SFTP transfers with a graphical user interface for Windows users","title":"SCP and SFTP"},{"location":"storage-systems/data-transfer/#bbcp","text":"BBCP is a multi-streaming utility for transferring large files. It splits files into multiple streams that are transferred simultaneously, so it is faster than the single-streaming SCP and SFTP utilities. More information .","title":"BBCP"},{"location":"storage-systems/data-transfer/#sharing-data-with-colleagues","text":"","title":"Sharing data with colleagues"},{"location":"storage-systems/data-transfer/#globus_1","text":"In addition to the features described above, Globus enables users to share files with colleagues and others who do not have NCAR user accounts. More information .","title":"Globus"},{"location":"storage-systems/data-transfer/#sharing-via-google-drive-and-other-services","text":"Individuals who have GLADE access can copy files from GLADE and then share them with others via Google Drive or services such as DropBox and Amazon Web Services. More information .","title":"Sharing via Google Drive and other services"},{"location":"storage-systems/data-transfer/Transferring%2Bfiles%2Bto%2BGoogle%2BDrive%2Bor%2BDropBox/","text":"","title":"Transferring+files+to+Google+Drive+or+DropBox"},{"location":"storage-systems/data-transfer/scp-and-sftp/","text":"SSH-based command line tools for file transfer \u00b6 SCP and SFTP \u00b6 Secure Copy Protocol (SCP) and Secure FTP (SFTP) are two utilities for transferring files between remote systems and the NCAR systems that CISL manages. They are best suited for transferring small numbers of small files (for example, fewer than 1,000 files totaling less than 200 MB). For larger-scale transfers, we recommend using Globus You can make SCP and SFTP transfers between the GLADE storage system and a remote machine if the remote machine accepts incoming SSH sessions. If it doesn't, the transfer will hang or you will receive a message such as \"connection refused,\" depending on the system's firewall settings. From an NCAR system \u00b6 To make SCP and SFTP transfers from your GLADE file space to a remote system, log in to the data access nodes at data-access.ucar.edu and execute the commands shown below. Use SCP if you need to transfer a single file or if you want to transfer multiple files with a single command by using a wildcard or recursive option. SCP transfer \u00b6 To transfer multiple files with similar names or extensions, follow this example, in which supersystem.univ.edu is a fictitious remote system. scp /glade/u/home/pparker/mydata/*.dat pparker@supersystem.univ.edu:/home/pparker SFTP transfer \u00b6 If you need to transfer many files from multiple directories to a remote machine, doing so in an SFTP session is likely to be more efficient for you than SCP. Log in to data-access.ucar.edu , then start your transfer session with the sftp command followed by your login information for the remote system. sftp pparker@supersystem.univ.edu You will be asked to authenticate at this point. Then, within the session, you can change between directories as needed and execute put commands to copy files to the remote machine. Use lcd to change local directories, and use cd to change directories on the remote system, as shown in this example. sftp> lcd /glade/u/home/pparker/mydata sftp> put filename1 sftp> lcd /glade/scratch/pparker sftp> cd /home/mydata sftp> put filename2 sftp> quit You can also transfer files from batch jobs running on an NCAR machine. To an NCAR system \u00b6 To transfer files from a remote system to your GLADE file space, log in to the remote system and reverse the procedures shown above. For example: scp /remotedir/*.dat pparker@data-access.ucar.edu:/glade/u/home/pparker/mydata You will be asked to authenticate for each individual SCP command that you execute to transfer files to the NCAR system. PSCP and PSFTP \u00b6 Note PSCP and PSFTP are PuTTY's implementation of SCP and SFTP, and may be useful for users on a Windows computer. PuTTY Secure Copy (PSCP) and PuTTY SFTP (PSFTP) enable you to transfer files to another system after opening a command window on a Windows computer. Both applications are available as free downloads. Usage is very similar to SCP and SFTP as described above. Expand the example box below for a full description of PSCP and PSFTP. Using PSCP and PSFTP Go to the download site and find the latest release version of the pscp.exe and psftp.exe files. Click on each and save them to your hard drive\u2014for example, in your C:\\Users\\username\\Downloads folder or in C:\\Program Files . To run either program, first open a command window: Enter cmd.exe in the search field of your Start menu. Press Enter . Then follow the applicable instructions below. PSCP transfer To copy a file or files using PSCP, open a command window and change to the directory in which you saved pscp.exe . C:\\Users\\jbsmith>cd C:\\Program Files Then type pscp , followed by the path that identifies the files to copy and the target directory, as in this example. pscp C:\\Users\\jbsmith\\directory\\*.txt jbsmith@cheyenne.ucar.edu:/glade/u/home/username Press Enter , then follow your usual authentication procedures to execute the transfer. Token_Response: file1.txt | 0 kB | 0.5 kB/s | ETA: 00:00:00 | 100% file1.txt | 0 kB | 0.5 kB/s | ETA: 00:00:00 | 100% file1.txt | 0 kB | 0.5 kB/s | ETA: 00:00:00 | 100% C:\\Users\\jbsmith\\Downloads> When the transfer is complete, type exit , then press Enter to close the command window. PSFTP transfer Open your command window, then change to the directory in which you saved psftp.exe . C:\\Users\\jbsmith>cd C:\\Program Files To start a session, type psftp followed by your login for the target computer. psftp jbsmith@cheyenne.ucar.edu Press Enter , then follow your usual authentication procedures to log in to the remote machine. Token_Response: Remote working directory is /glade/u/home/jbsmith psftp> Within the session that you just started, you can copy a file or files from your computer to the remote system by changing between directories as needed and executing multiple put commands. Use lcd to change local directories, and cd to change directories on the remote system, as in this example: psftp> lcd ..\\documents psftp> lcd documents New local directory is C:\\Users\\jbsmith\\documents psftp> put file1.txt local:file1.txt => remote:/glade/u/home/jbsmith/file1.txt psftp> cd /glade/scratch/jbsmith Remote directory is now /glade/scratch/jbsmith psftp> mput file*.txt local:file1.txt => remote:/glade/scratch/jbsmith/file1.txt local:file2.txt => remote:/glade/scratch/jbsmith/file2.txt local:file3.txt => remote:/glade/scratch/jbsmith/file3.txt psftp> To end the psftp session, type exit , then press Enter . To close the command window, type exit again, then press Enter . To copy multiple files, you can use a wildcard and an mput or mget command rather than put or get . WinSCP \u00b6 For Windows users, WinSCP offers a choice of GUI interfaces for managing files. It is easy to download and install from winscp.net . Starting the application will bring you to a login screen like the one shown here. (Highlights added.) To start a session, input the host name and your username for that system. Leave the password field blank, and click Login . The first time you log in to a system, you may get a dialog box like this: Click Yes to continue. Next, you will be asked for your Token_Response . Follow your regular authentication procedures . If you\u2019re using the \u201cCommander\u201d interface (shown below), WinSCP will display the contents of your local system on the left side of your screen and the contents of your remote system home directory on the right. You can manage files using typical Windows commands and tools. The alternative \u201cExplorer\u201d interface displays only the remote folder, and you can transfer files by dragging and dropping from Windows File Explorer. On the WinSCP menu, go to Options/Preferences/Environment/Interface to use it. BBCP \u00b6 The BBCP utility for transferring large files is an alternative for users who are unable to use Globus to transfer data. BBCP splits the files into multiple streams that are transferred simultaneously, so it is faster than the single-streaming SCP and SFTP utilities. To make transfers with BBCP, it must be installed on all the systems where you want to use it. It is already installed on the NCAR systems that CISL manages, including the data-access nodes . Transfer examples \u00b6 To transfer a file from GLADE to a remote system that uses bbcp , log in to data-access.ucar.edu and follow this example. Replace \"target\" with the intended pathname of the file you are transferring. bbcp -w 4m -s 16 filename username@supersystem.univ.edu:target To transfer a file from a remote system to GLADE , log in to the remote system and follow this example. Replace \"target\" with the intended pathname of the file you are transferring \u2013 for example, /glade/u/home/\\$USER/filename . bbcp -w 4m -s 16 -V -D filename username@data-access.ucar.edu:target Detailed documentation \u00b6 For complete details, see the official BBCP man page .","title":"SCP, SFTP and other tools"},{"location":"storage-systems/data-transfer/scp-and-sftp/#ssh-based-command-line-tools-for-file-transfer","text":"","title":"SSH-based command line tools for file transfer"},{"location":"storage-systems/data-transfer/scp-and-sftp/#scp-and-sftp","text":"Secure Copy Protocol (SCP) and Secure FTP (SFTP) are two utilities for transferring files between remote systems and the NCAR systems that CISL manages. They are best suited for transferring small numbers of small files (for example, fewer than 1,000 files totaling less than 200 MB). For larger-scale transfers, we recommend using Globus You can make SCP and SFTP transfers between the GLADE storage system and a remote machine if the remote machine accepts incoming SSH sessions. If it doesn't, the transfer will hang or you will receive a message such as \"connection refused,\" depending on the system's firewall settings.","title":"SCP and SFTP"},{"location":"storage-systems/data-transfer/scp-and-sftp/#from-an-ncar-system","text":"To make SCP and SFTP transfers from your GLADE file space to a remote system, log in to the data access nodes at data-access.ucar.edu and execute the commands shown below. Use SCP if you need to transfer a single file or if you want to transfer multiple files with a single command by using a wildcard or recursive option.","title":"From an NCAR system"},{"location":"storage-systems/data-transfer/scp-and-sftp/#scp-transfer","text":"To transfer multiple files with similar names or extensions, follow this example, in which supersystem.univ.edu is a fictitious remote system. scp /glade/u/home/pparker/mydata/*.dat pparker@supersystem.univ.edu:/home/pparker","title":"SCP transfer"},{"location":"storage-systems/data-transfer/scp-and-sftp/#sftp-transfer","text":"If you need to transfer many files from multiple directories to a remote machine, doing so in an SFTP session is likely to be more efficient for you than SCP. Log in to data-access.ucar.edu , then start your transfer session with the sftp command followed by your login information for the remote system. sftp pparker@supersystem.univ.edu You will be asked to authenticate at this point. Then, within the session, you can change between directories as needed and execute put commands to copy files to the remote machine. Use lcd to change local directories, and use cd to change directories on the remote system, as shown in this example. sftp> lcd /glade/u/home/pparker/mydata sftp> put filename1 sftp> lcd /glade/scratch/pparker sftp> cd /home/mydata sftp> put filename2 sftp> quit You can also transfer files from batch jobs running on an NCAR machine.","title":"SFTP transfer"},{"location":"storage-systems/data-transfer/scp-and-sftp/#to-an-ncar-system","text":"To transfer files from a remote system to your GLADE file space, log in to the remote system and reverse the procedures shown above. For example: scp /remotedir/*.dat pparker@data-access.ucar.edu:/glade/u/home/pparker/mydata You will be asked to authenticate for each individual SCP command that you execute to transfer files to the NCAR system.","title":"To an NCAR system"},{"location":"storage-systems/data-transfer/scp-and-sftp/#pscp-and-psftp","text":"Note PSCP and PSFTP are PuTTY's implementation of SCP and SFTP, and may be useful for users on a Windows computer. PuTTY Secure Copy (PSCP) and PuTTY SFTP (PSFTP) enable you to transfer files to another system after opening a command window on a Windows computer. Both applications are available as free downloads. Usage is very similar to SCP and SFTP as described above. Expand the example box below for a full description of PSCP and PSFTP. Using PSCP and PSFTP Go to the download site and find the latest release version of the pscp.exe and psftp.exe files. Click on each and save them to your hard drive\u2014for example, in your C:\\Users\\username\\Downloads folder or in C:\\Program Files . To run either program, first open a command window: Enter cmd.exe in the search field of your Start menu. Press Enter . Then follow the applicable instructions below. PSCP transfer To copy a file or files using PSCP, open a command window and change to the directory in which you saved pscp.exe . C:\\Users\\jbsmith>cd C:\\Program Files Then type pscp , followed by the path that identifies the files to copy and the target directory, as in this example. pscp C:\\Users\\jbsmith\\directory\\*.txt jbsmith@cheyenne.ucar.edu:/glade/u/home/username Press Enter , then follow your usual authentication procedures to execute the transfer. Token_Response: file1.txt | 0 kB | 0.5 kB/s | ETA: 00:00:00 | 100% file1.txt | 0 kB | 0.5 kB/s | ETA: 00:00:00 | 100% file1.txt | 0 kB | 0.5 kB/s | ETA: 00:00:00 | 100% C:\\Users\\jbsmith\\Downloads> When the transfer is complete, type exit , then press Enter to close the command window. PSFTP transfer Open your command window, then change to the directory in which you saved psftp.exe . C:\\Users\\jbsmith>cd C:\\Program Files To start a session, type psftp followed by your login for the target computer. psftp jbsmith@cheyenne.ucar.edu Press Enter , then follow your usual authentication procedures to log in to the remote machine. Token_Response: Remote working directory is /glade/u/home/jbsmith psftp> Within the session that you just started, you can copy a file or files from your computer to the remote system by changing between directories as needed and executing multiple put commands. Use lcd to change local directories, and cd to change directories on the remote system, as in this example: psftp> lcd ..\\documents psftp> lcd documents New local directory is C:\\Users\\jbsmith\\documents psftp> put file1.txt local:file1.txt => remote:/glade/u/home/jbsmith/file1.txt psftp> cd /glade/scratch/jbsmith Remote directory is now /glade/scratch/jbsmith psftp> mput file*.txt local:file1.txt => remote:/glade/scratch/jbsmith/file1.txt local:file2.txt => remote:/glade/scratch/jbsmith/file2.txt local:file3.txt => remote:/glade/scratch/jbsmith/file3.txt psftp> To end the psftp session, type exit , then press Enter . To close the command window, type exit again, then press Enter . To copy multiple files, you can use a wildcard and an mput or mget command rather than put or get .","title":"PSCP and PSFTP"},{"location":"storage-systems/data-transfer/scp-and-sftp/#winscp","text":"For Windows users, WinSCP offers a choice of GUI interfaces for managing files. It is easy to download and install from winscp.net . Starting the application will bring you to a login screen like the one shown here. (Highlights added.) To start a session, input the host name and your username for that system. Leave the password field blank, and click Login . The first time you log in to a system, you may get a dialog box like this: Click Yes to continue. Next, you will be asked for your Token_Response . Follow your regular authentication procedures . If you\u2019re using the \u201cCommander\u201d interface (shown below), WinSCP will display the contents of your local system on the left side of your screen and the contents of your remote system home directory on the right. You can manage files using typical Windows commands and tools. The alternative \u201cExplorer\u201d interface displays only the remote folder, and you can transfer files by dragging and dropping from Windows File Explorer. On the WinSCP menu, go to Options/Preferences/Environment/Interface to use it.","title":"WinSCP"},{"location":"storage-systems/data-transfer/scp-and-sftp/#bbcp","text":"The BBCP utility for transferring large files is an alternative for users who are unable to use Globus to transfer data. BBCP splits the files into multiple streams that are transferred simultaneously, so it is faster than the single-streaming SCP and SFTP utilities. To make transfers with BBCP, it must be installed on all the systems where you want to use it. It is already installed on the NCAR systems that CISL manages, including the data-access nodes .","title":"BBCP"},{"location":"storage-systems/data-transfer/scp-and-sftp/#transfer-examples","text":"To transfer a file from GLADE to a remote system that uses bbcp , log in to data-access.ucar.edu and follow this example. Replace \"target\" with the intended pathname of the file you are transferring. bbcp -w 4m -s 16 filename username@supersystem.univ.edu:target To transfer a file from a remote system to GLADE , log in to the remote system and follow this example. Replace \"target\" with the intended pathname of the file you are transferring \u2013 for example, /glade/u/home/\\$USER/filename . bbcp -w 4m -s 16 -V -D filename username@data-access.ucar.edu:target","title":"Transfer examples"},{"location":"storage-systems/data-transfer/scp-and-sftp/#detailed-documentation","text":"For complete details, see the official BBCP man page .","title":"Detailed documentation"},{"location":"storage-systems/data-transfer/globus/","text":"Globus file transfers \u00b6 Updated 4/25/2023: CISL has enabled the *Globus for Google Drive connector service to facilitate file transfers to NCAR's Google Drive by NCAR and UCAR staff. University users and others who are interested in using the connector service are advised to consult with their own institutional IT experts. Globus is the most efficient way to transfer files \u2013 large files, in particular \u2013 between NCAR file systems such as Campaign Storage, GLADE, and non-NCAR resources and storage systems. NCAR and UCAR researchers can also use the Globus for Google Drive connector service to transfer files to the NCAR Google Drive collection on Globus for sharing and storage purposes as described below. Globus has both a web interface and a command line interface (CLI), which are described below. To use either interface, the first step is to create a free personal account and log in using a Globus ID or a Google account. (UCAR and NCAR staff: Do not use the NCAR RDA organizational login .) Several mapped collections , which provide access to different file system locations on a public endpoint, are set up on the Globus system for transferring files to and from NCAR storage systems. These include: NCAR GLADE NCAR Campaign Storage NCAR Data Sharing Service Users can also create their own guest collections as described here in order to facilitate data sharing with colleagues and to accommodate unattended workflows. Globus also offers a feature called Globus Connect Personal for moving files to and from a laptop or desktop computer and other endpoints. Cautions \u00b6 Globus Usage Cautions The Globus interface for transferring data does not handle symbolic links and does not create symbolic links on a destination endpoint. It is possible to corrupt data when performing a transfer if you accidentally specify the same files as both source and destination. To avoid inadvertently deleting data when using the web interface, activate the sync option in the Transfer & Sync Options menu. Sync can be set to allow a file transfer only if the file does not exist on the destination, if there\u2019s a difference in checksum or file size, or if the source copy is newer than the destination copy. Transferred files assume the user's umask permissions on the destination system regardless of permissions on the source system. Transferring files with the web interface \u00b6 When transferring files between systems, keep in mind that your username might not be the same on each system. Follow these steps to transfer files. See the image below for reference. Go to the main Globus page ( globus.org ) and log in using your personal Globus ID . Go to File Manager . Use the Panels button to display two endpoint panels side by side. Enter the name of your source endpoint in the Collection field on one panel. Specify the path where your source files are located. Enter your username and authenticate as you do when logging in to NCAR systems. You will not need to authenticate to access the collection for the next 30 days. Identify your target endpoint in the other panel. Specify a destination path. Select the files you want to copy. Click the Start button to initiate the transfer. You can check the status of your transfers any time through the web interface and will be notified when they are complete. Using mapped collections for CLI transfers \u00b6 The Globus CLI application, an installable Python package, can be used to make both manual and unattended file transfers . The application is: installed on the NCAR/CISL data-access nodes . available within the NCAR Python Library conda environment. can be added to a personal conda environment using conda install globus-cli . To begin, log in as shown in this example for using the data-access nodes. (If your UCAR username and your username on your local computer are different, follow the alternative example.) ssh data-access.ucar.edu # (alternative: ssh username@data-access.ucar.edu) Run globus login and follow the on-screen instructions. globus login Output example (if you are not already logged in): Please authenticate with Globus here: ------------------------------------ URL to copy and paste into your browser. ------------------------------------ Enter the resulting Authorization Code here: Copy the lengthy URL and paste it into your browser. It will ask you to choose an identity \u2013 as above, use either a Globus ID or Google account \u2013 and then it will take you to a consent form that looks like this: Click Allow to give the CLI app the necessary access and you will receive an authorization code. After entering the code at the terminal prompt, you will be logged in to the Globus CLI and in your /glade/u/home directory. You can use this authentication credential for 30 days before you will need to reactivate a mapped collection. Workflows that need longer collection access \u2013 to facilitate unattended file transfers, for example \u2013 can instead use guest collections . Using the CLI, it is possible to query your current mapped collection activation and also force Globus to reactivate it (thus extending your activation lifetime): gc_glade=$(globus endpoint search \"ncar@globusid.org NCAR GLADE\" --jq \"DATA[0].id\" --format UNIX) globus endpoint activate $gc_glade Endpoint is already activated. Activation expires at 2022-11-22 20:47:46+00:00 globus endpoint activate --force $gc_glade Autoactivation succeeded with message: Endpoint activated successfully using Globus Online credentials. Your default shell on the data-access nodes is tcsh . To change your current shell, just enter bash or another preferred shell. Executing CLI transfers \u00b6 For details regarding how to make batch transfers and single-item transfers, manage endpoints, and more, see these resources: CLI Examples CLI QuickStart Guide Globus CLI Reference Globus for Google Drive \u00b6 NCAR and UCAR researchers can use the Globus for Google Drive connector service to transfer files to the NCAR Google Drive collection on Globus for sharing and storage purposes by following the instructions below. The Globus Google Drive connector is not designed to transfer Google apps products \u2013 such as Google Docs, Sheets, or Slides \u2013 between Google accounts or beyond Google Drive. While such files might appear to be visible in your Google Drive as .gsheet or .gdoc files, those are just pointers and downloading them will not download the data stored in the files. It also is not intended for backing up your scratch space or other GLADE files. It is intended for sharing smaller files such as plots. If you need to transfer many small files, compress or archive them into a smaller number of files. To make a transfer, follow these steps after logging in to your personal Globus account: Go to File Manager and search for the collection named NCAR Google Drive . Select the drive and authenticate as required. You will be directed to the NCAR/UCAR authentication page to give Globus access to the Google Drive by using Duo and your CIT credentials. (You are only required to do this once.) Select your target collection (endpoint) in the other File Manager panel. Execute the transfer. Your Google Drive endpoint will originate in your /My Drive/ path by default. To access other Google Drive folders such as Shared with me or Starred , simply navigate up one level. Warning Avoid giving the same name to multiple files when using Globus integration. While Google Drive can support multiple files with the same name, they cannot be mapped into a POSIX file system. Storage limits and limitations \u00b6 Keep these storage and file size limitations in mind when using Globus to transfer data to the NCAR Google Drive: - Google imposes a maximum file size limit of 5 TB . - The daily upload limit is 750 GB per user. However, Google allows uploading of a single file larger than 750 GB. - Each user is limited to having a maximum of 5 million files and folders within their Google Drive, which includes all Google products such as Google Docs. Example use case \u00b6 The Globus Google Drive connector can streamline data sharing and collaboration with team members who don't have access to Globus . For example, a meteorologist who performs daily operational simulations on an NCAR system as part of a multi-organizational field campaign and generates plots for flight planning can share them with the operational team and collaborators via a shared drive. Instead of manually transferring the plots to a personal device and then uploading them to the shared drive, the meteorologist can leverage the connector to automate the workflow and move the plots directly from GLADE to Google Drive. Globus Connect Personal \u00b6 To set up your laptop or desktop computer to use Globus Connect Personal: Go to Globus Connect Personal and follow the instructions to download and install it on your local system. Add your local system as an endpoint by following the instructions on the Globus Connect website. Start Globus Connect, and then sign in to globus.org . Your local system should now appear as an endpoint that can be used for transferring files. Using Globus to sharing data and making unattended transfers \u00b6 Some users need unauthenticated access of data from NCAR storage systems to share that data or to accommodate their workflows. Guest collections enable unauthenticated data access by pointing to a specified subset of data stored in a mapped collection like NCAR GLADE . They can be created by anyone with authenticated access to NCAR storage systems. Usage of Globus Guest Collection for data sharing is described here .","title":"Globus file transfers"},{"location":"storage-systems/data-transfer/globus/#globus-file-transfers","text":"Updated 4/25/2023: CISL has enabled the *Globus for Google Drive connector service to facilitate file transfers to NCAR's Google Drive by NCAR and UCAR staff. University users and others who are interested in using the connector service are advised to consult with their own institutional IT experts. Globus is the most efficient way to transfer files \u2013 large files, in particular \u2013 between NCAR file systems such as Campaign Storage, GLADE, and non-NCAR resources and storage systems. NCAR and UCAR researchers can also use the Globus for Google Drive connector service to transfer files to the NCAR Google Drive collection on Globus for sharing and storage purposes as described below. Globus has both a web interface and a command line interface (CLI), which are described below. To use either interface, the first step is to create a free personal account and log in using a Globus ID or a Google account. (UCAR and NCAR staff: Do not use the NCAR RDA organizational login .) Several mapped collections , which provide access to different file system locations on a public endpoint, are set up on the Globus system for transferring files to and from NCAR storage systems. These include: NCAR GLADE NCAR Campaign Storage NCAR Data Sharing Service Users can also create their own guest collections as described here in order to facilitate data sharing with colleagues and to accommodate unattended workflows. Globus also offers a feature called Globus Connect Personal for moving files to and from a laptop or desktop computer and other endpoints.","title":"Globus file transfers"},{"location":"storage-systems/data-transfer/globus/#cautions","text":"Globus Usage Cautions The Globus interface for transferring data does not handle symbolic links and does not create symbolic links on a destination endpoint. It is possible to corrupt data when performing a transfer if you accidentally specify the same files as both source and destination. To avoid inadvertently deleting data when using the web interface, activate the sync option in the Transfer & Sync Options menu. Sync can be set to allow a file transfer only if the file does not exist on the destination, if there\u2019s a difference in checksum or file size, or if the source copy is newer than the destination copy. Transferred files assume the user's umask permissions on the destination system regardless of permissions on the source system.","title":"Cautions"},{"location":"storage-systems/data-transfer/globus/#transferring-files-with-the-web-interface","text":"When transferring files between systems, keep in mind that your username might not be the same on each system. Follow these steps to transfer files. See the image below for reference. Go to the main Globus page ( globus.org ) and log in using your personal Globus ID . Go to File Manager . Use the Panels button to display two endpoint panels side by side. Enter the name of your source endpoint in the Collection field on one panel. Specify the path where your source files are located. Enter your username and authenticate as you do when logging in to NCAR systems. You will not need to authenticate to access the collection for the next 30 days. Identify your target endpoint in the other panel. Specify a destination path. Select the files you want to copy. Click the Start button to initiate the transfer. You can check the status of your transfers any time through the web interface and will be notified when they are complete.","title":"Transferring files with the web interface"},{"location":"storage-systems/data-transfer/globus/#using-mapped-collections-for-cli-transfers","text":"The Globus CLI application, an installable Python package, can be used to make both manual and unattended file transfers . The application is: installed on the NCAR/CISL data-access nodes . available within the NCAR Python Library conda environment. can be added to a personal conda environment using conda install globus-cli . To begin, log in as shown in this example for using the data-access nodes. (If your UCAR username and your username on your local computer are different, follow the alternative example.) ssh data-access.ucar.edu # (alternative: ssh username@data-access.ucar.edu) Run globus login and follow the on-screen instructions. globus login Output example (if you are not already logged in): Please authenticate with Globus here: ------------------------------------ URL to copy and paste into your browser. ------------------------------------ Enter the resulting Authorization Code here: Copy the lengthy URL and paste it into your browser. It will ask you to choose an identity \u2013 as above, use either a Globus ID or Google account \u2013 and then it will take you to a consent form that looks like this: Click Allow to give the CLI app the necessary access and you will receive an authorization code. After entering the code at the terminal prompt, you will be logged in to the Globus CLI and in your /glade/u/home directory. You can use this authentication credential for 30 days before you will need to reactivate a mapped collection. Workflows that need longer collection access \u2013 to facilitate unattended file transfers, for example \u2013 can instead use guest collections . Using the CLI, it is possible to query your current mapped collection activation and also force Globus to reactivate it (thus extending your activation lifetime): gc_glade=$(globus endpoint search \"ncar@globusid.org NCAR GLADE\" --jq \"DATA[0].id\" --format UNIX) globus endpoint activate $gc_glade Endpoint is already activated. Activation expires at 2022-11-22 20:47:46+00:00 globus endpoint activate --force $gc_glade Autoactivation succeeded with message: Endpoint activated successfully using Globus Online credentials. Your default shell on the data-access nodes is tcsh . To change your current shell, just enter bash or another preferred shell.","title":"Using mapped collections for CLI transfers"},{"location":"storage-systems/data-transfer/globus/#executing-cli-transfers","text":"For details regarding how to make batch transfers and single-item transfers, manage endpoints, and more, see these resources: CLI Examples CLI QuickStart Guide Globus CLI Reference","title":"Executing CLI transfers"},{"location":"storage-systems/data-transfer/globus/#globus-for-google-drive","text":"NCAR and UCAR researchers can use the Globus for Google Drive connector service to transfer files to the NCAR Google Drive collection on Globus for sharing and storage purposes by following the instructions below. The Globus Google Drive connector is not designed to transfer Google apps products \u2013 such as Google Docs, Sheets, or Slides \u2013 between Google accounts or beyond Google Drive. While such files might appear to be visible in your Google Drive as .gsheet or .gdoc files, those are just pointers and downloading them will not download the data stored in the files. It also is not intended for backing up your scratch space or other GLADE files. It is intended for sharing smaller files such as plots. If you need to transfer many small files, compress or archive them into a smaller number of files. To make a transfer, follow these steps after logging in to your personal Globus account: Go to File Manager and search for the collection named NCAR Google Drive . Select the drive and authenticate as required. You will be directed to the NCAR/UCAR authentication page to give Globus access to the Google Drive by using Duo and your CIT credentials. (You are only required to do this once.) Select your target collection (endpoint) in the other File Manager panel. Execute the transfer. Your Google Drive endpoint will originate in your /My Drive/ path by default. To access other Google Drive folders such as Shared with me or Starred , simply navigate up one level. Warning Avoid giving the same name to multiple files when using Globus integration. While Google Drive can support multiple files with the same name, they cannot be mapped into a POSIX file system.","title":"Globus for Google Drive"},{"location":"storage-systems/data-transfer/globus/#storage-limits-and-limitations","text":"Keep these storage and file size limitations in mind when using Globus to transfer data to the NCAR Google Drive: - Google imposes a maximum file size limit of 5 TB . - The daily upload limit is 750 GB per user. However, Google allows uploading of a single file larger than 750 GB. - Each user is limited to having a maximum of 5 million files and folders within their Google Drive, which includes all Google products such as Google Docs.","title":"Storage limits and limitations"},{"location":"storage-systems/data-transfer/globus/#example-use-case","text":"The Globus Google Drive connector can streamline data sharing and collaboration with team members who don't have access to Globus . For example, a meteorologist who performs daily operational simulations on an NCAR system as part of a multi-organizational field campaign and generates plots for flight planning can share them with the operational team and collaborators via a shared drive. Instead of manually transferring the plots to a personal device and then uploading them to the shared drive, the meteorologist can leverage the connector to automate the workflow and move the plots directly from GLADE to Google Drive.","title":"Example use case"},{"location":"storage-systems/data-transfer/globus/#globus-connect-personal","text":"To set up your laptop or desktop computer to use Globus Connect Personal: Go to Globus Connect Personal and follow the instructions to download and install it on your local system. Add your local system as an endpoint by following the instructions on the Globus Connect website. Start Globus Connect, and then sign in to globus.org . Your local system should now appear as an endpoint that can be used for transferring files.","title":"Globus Connect Personal"},{"location":"storage-systems/data-transfer/globus/#using-globus-to-sharing-data-and-making-unattended-transfers","text":"Some users need unauthenticated access of data from NCAR storage systems to share that data or to accommodate their workflows. Guest collections enable unauthenticated data access by pointing to a specified subset of data stored in a mapped collection like NCAR GLADE . They can be created by anyone with authenticated access to NCAR storage systems. Usage of Globus Guest Collection for data sharing is described here .","title":"Using Globus to sharing data and making unattended transfers"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/","text":"Sharing data and making unattended transfers with Globus \u00b6 Some users need unauthenticated access of data from NCAR storage systems to share that data or to accommodate their workflows. Common use cases include: Easily share specified subsets of their data with external colleagues. Access their own data without authentication in order to make unattended transfers. Guest collections enable unauthenticated data access by pointing to a specified subset of data stored in a mapped collection like NCAR GLADE . They can be created by anyone with authenticated access to NCAR storage systems. Once a guest collection is created, it can be used in place of a mapped collection as a transfer endpoint in the web interface, Globus CLI, or Globus Python API. Users can also bypass the Globus transfer interface entirely by creating a URL to an individual file contained within a guest collection. The URL will give specified individuals download access to the data from a browser or terminal utility like wget or curl . New Collections must be created through Globus web interface While the Globus CLI supports the use of guest collections as transfer endpoints ( see this note for more on endpoint vs collection nomenclature), it does not provide commands to create new guest collections or file URLs. Those actions must be done in the web interface. Do not allow file deletion! Be sure to review and follow the recommendations below to avoid inadvertently sharing your data with unknown users or allowing unauthorized deletions of your data. Guest collections make data sharing easy, but they also bypass the traditional data safeguards (two-factor authentication) that users and administrators rely on to protect data. Creating a guest collection \u00b6 To create a guest collection using the Globus web interface, log in and navigate to a mapped collections like NCAR GLADE . Then: Use the File Manager to navigate to the directory you want to designate as your guest collection. Select the directory and click the Share button in the toolbar. From the next screen, click Add a Guest Collection . This option will take you to the following page (sometimes after a one-time consent page), which allows you to name and customize your collection's metadata. Once your collection is created, it will be assigned an ID you can use for both web and CLI transfers. You can modify collection properties by selecting Collections on the left toolbar, then following these steps: Select Administered by you to see your collections, including your newly created guest collection. Select your guest collection, then the Permissions tab. Finally, click Add Permissions - Share With . At this point, you can add read permissions for other users, groups, all Globus users, or any individual to access data contained within your collection. You can also now add read-write permission for yourself to enable unattended workflows via this guest collection (using the Globus CLI or Python API). Providing URL access to files \u00b6 Globus allows you to create a sharable URL (web link) to any file in your guest collection or a mapped collection. This means that you can share files from supported file systems without the other user needing to interact at all with Globus itself. The individual can simply use a browser and your URL to download the files. The web links will be subject to the permissions model of the collection being used. If you create a link using a mapped collection like NCAR GLADE , the recipients will need to be able to authenticate to the collection as well. If you obtain a link using a guest collection that permits the recipient read-access, they will be able to open the link regardless of their ability to access the rest of the original mapped collection. To create a URL: Navigate to the desired file in the web interface's File Manager. Click on the file, then select Get Link from the toolbar. Recommendations for creating guest collections \u00b6 The features described above make data sharing easy, but they also bypass the traditional data safeguards (two-factor authentication) that users and administrators rely on to protect data. Using guest collections, it is quite possible to grant access to data unintentionally or even allow unknown users to delete your data. To avoid these situations, consider the following recommendations: Properly scope your collections. It may be tempting to set a collection to a top-level directory (e.g., /glade/derecho/scratch/${USER} ), but this will grant access to all of your data on scratch. Instead, create collections with a narrow scope and move or copy files around to selectively grant access to data. Avoid write permissions. Generally speaking, Globus is most useful as a tool to access/read data. Write permissions within Globus are only useful for data collection curation, so avoid granting it on your data. It may be useful to grant write permissions on a directory to allow collaborators to \"push\" data to you, but remember that the space will still be subject to storage quotas and your collaborators will not have visibility into those limits! It is also wise to avoid changes to guest collection metadata \u2013 especially the collection name \u2013 once it has been shared with others. The text name is useful in both the web and command-line interfaces for collection discoverability, so changing it and other metadata may break workflows or result in a loss of reproducibility (e.g., when providing data for publication requirements). How to use your guest collections \u00b6 Users of your guest collections will access data in one of the following two ways, depending on how you have configured your collections. Accessible to Globus users or groups \u00b6 If you have set up your collection to provide access to individual users or groups, or all Globus users, they will be able to find and open your collection using the Globus website, CLI, or Python API. If they do not already have a Globus account, they will need to create one to log into the Globus service itself. NCAR/UCAR staff should use their Google login credentials. Universities may have their own guidance about which type of account to use. Once a user is logged in, no further authentication will be required to see data that you have made accessible in your collection. Alternatively, you can create web URLs to specific files that will allow permitted users to access the files either in the Globus web interface or via direct download, depending on how you have configured the URL. Accessible to anyone \u00b6 If you have configured your collection to be accessible to public (anonymous) users, you can create direct-download URLs as described above, but those users will not need to have a Globus account to open them. This method is the easiest for end users but has some limitations. For example, instead of having a file browser view with metadata, users will simply download the file as if it were hosted on a web server.","title":"Sharing data and making unattended transfers"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#sharing-data-and-making-unattended-transfers-with-globus","text":"Some users need unauthenticated access of data from NCAR storage systems to share that data or to accommodate their workflows. Common use cases include: Easily share specified subsets of their data with external colleagues. Access their own data without authentication in order to make unattended transfers. Guest collections enable unauthenticated data access by pointing to a specified subset of data stored in a mapped collection like NCAR GLADE . They can be created by anyone with authenticated access to NCAR storage systems. Once a guest collection is created, it can be used in place of a mapped collection as a transfer endpoint in the web interface, Globus CLI, or Globus Python API. Users can also bypass the Globus transfer interface entirely by creating a URL to an individual file contained within a guest collection. The URL will give specified individuals download access to the data from a browser or terminal utility like wget or curl . New Collections must be created through Globus web interface While the Globus CLI supports the use of guest collections as transfer endpoints ( see this note for more on endpoint vs collection nomenclature), it does not provide commands to create new guest collections or file URLs. Those actions must be done in the web interface. Do not allow file deletion! Be sure to review and follow the recommendations below to avoid inadvertently sharing your data with unknown users or allowing unauthorized deletions of your data. Guest collections make data sharing easy, but they also bypass the traditional data safeguards (two-factor authentication) that users and administrators rely on to protect data.","title":"Sharing data and making unattended transfers with Globus"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#creating-a-guest-collection","text":"To create a guest collection using the Globus web interface, log in and navigate to a mapped collections like NCAR GLADE . Then: Use the File Manager to navigate to the directory you want to designate as your guest collection. Select the directory and click the Share button in the toolbar. From the next screen, click Add a Guest Collection . This option will take you to the following page (sometimes after a one-time consent page), which allows you to name and customize your collection's metadata. Once your collection is created, it will be assigned an ID you can use for both web and CLI transfers. You can modify collection properties by selecting Collections on the left toolbar, then following these steps: Select Administered by you to see your collections, including your newly created guest collection. Select your guest collection, then the Permissions tab. Finally, click Add Permissions - Share With . At this point, you can add read permissions for other users, groups, all Globus users, or any individual to access data contained within your collection. You can also now add read-write permission for yourself to enable unattended workflows via this guest collection (using the Globus CLI or Python API).","title":"Creating a guest collection"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#providing-url-access-to-files","text":"Globus allows you to create a sharable URL (web link) to any file in your guest collection or a mapped collection. This means that you can share files from supported file systems without the other user needing to interact at all with Globus itself. The individual can simply use a browser and your URL to download the files. The web links will be subject to the permissions model of the collection being used. If you create a link using a mapped collection like NCAR GLADE , the recipients will need to be able to authenticate to the collection as well. If you obtain a link using a guest collection that permits the recipient read-access, they will be able to open the link regardless of their ability to access the rest of the original mapped collection. To create a URL: Navigate to the desired file in the web interface's File Manager. Click on the file, then select Get Link from the toolbar.","title":"Providing URL access to files"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#recommendations-for-creating-guest-collections","text":"The features described above make data sharing easy, but they also bypass the traditional data safeguards (two-factor authentication) that users and administrators rely on to protect data. Using guest collections, it is quite possible to grant access to data unintentionally or even allow unknown users to delete your data. To avoid these situations, consider the following recommendations: Properly scope your collections. It may be tempting to set a collection to a top-level directory (e.g., /glade/derecho/scratch/${USER} ), but this will grant access to all of your data on scratch. Instead, create collections with a narrow scope and move or copy files around to selectively grant access to data. Avoid write permissions. Generally speaking, Globus is most useful as a tool to access/read data. Write permissions within Globus are only useful for data collection curation, so avoid granting it on your data. It may be useful to grant write permissions on a directory to allow collaborators to \"push\" data to you, but remember that the space will still be subject to storage quotas and your collaborators will not have visibility into those limits! It is also wise to avoid changes to guest collection metadata \u2013 especially the collection name \u2013 once it has been shared with others. The text name is useful in both the web and command-line interfaces for collection discoverability, so changing it and other metadata may break workflows or result in a loss of reproducibility (e.g., when providing data for publication requirements).","title":"Recommendations for creating guest collections"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#how-to-use-your-guest-collections","text":"Users of your guest collections will access data in one of the following two ways, depending on how you have configured your collections.","title":"How to use your guest collections"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#accessible-to-globus-users-or-groups","text":"If you have set up your collection to provide access to individual users or groups, or all Globus users, they will be able to find and open your collection using the Globus website, CLI, or Python API. If they do not already have a Globus account, they will need to create one to log into the Globus service itself. NCAR/UCAR staff should use their Google login credentials. Universities may have their own guidance about which type of account to use. Once a user is logged in, no further authentication will be required to see data that you have made accessible in your collection. Alternatively, you can create web URLs to specific files that will allow permitted users to access the files either in the Globus web interface or via direct download, depending on how you have configured the URL.","title":"Accessible to Globus users or groups"},{"location":"storage-systems/data-transfer/globus/Sharing%2Bdata%2Band%2Bmaking%2Bunattended%2Btransfers/#accessible-to-anyone","text":"If you have configured your collection to be accessible to public (anonymous) users, you can create direct-download URLs as described above, but those users will not need to have a Globus account to open them. This method is the easiest for end users but has some limitations. For example, instead of having a file browser view with metadata, users will simply download the file as if it were hosted on a web server.","title":"Accessible to anyone"},{"location":"storage-systems/glade/","text":"GLADE file spaces \u00b6 Overview \u00b6 The Globally Accessible Data Environment \u2013 a centralized file service known as GLADE \u2013 uses high-performance Spectrum Scale and Lustre shared file system technology to give users a common view of their data across the HPC, analysis, and visualization resources that CISL manages. File space User quota (Size/Count) Backup Purge policy Descriptions/notices Home: /glade/u/home/<username> 50 GB Yes Not purged User home directory Access: POSIX Scratch: (Cheyenne) /glade/scratch/<username> 10 TB No 120 days Temporary computational space Access: POSIX Scratch: (Derecho) /glade/derecho/scratch/<username> 30TB / 10M No 180 days Derecho's scratch file system also includes a limit on a users' total number of files Work: /glade/work/<username> 2 TB No Not purged User work space Access: POSIX Project: /glade/p/ entity/project_code N/A No Not purged Deprecated; will be migrated to Campaign Storage in 2023 Collections: /glade/collections N/A No Not purged Curated collections (CMIP, RDA, others) Access: POSIX Campaign Storage: /glade/campaign N/A No Not purged Project space allocations (via allocation request) GLADE system status report CISL backs up the GLADE home file space several times a week and also creates snapshots to enable users to recover deleted files quickly and easily. Data can remain in each of these spaces in accordance with the policies detailed below. The policies are subject to change; any changes necessary will be announced in advance. CISL does not provide backups of other spaces. You are responsible for the safe storage of any data that must be preserved. Best practice: Check your space usage regularly with gladequota Check your space usage regularly with gladequota as described below, and remove data that you no longer need. You can conserve GLADE space by storing large files, such as tar files, rather than numerous small, individual files. This is because the system allocates a minimum amount of space for each file (currently configured to 16 KB), no matter how small the file is. Thus 16KB is the smallest amount of space the system can allocate to a file, including empty files, directories and symlinks. Status \u00b6 Home space \u00b6 Each user has a /glade/u/home/<username> space with a quota of 50 GB* for managing scripts, source code, and small data sets. It is backed up weekly, with backups retained for several months. CISL also creates snapshots of the space to enable users to recover deleted files quickly and easily. Backup policy \u00b6 Your /glade/u/home directory is backed up several times a week while your account is open. Each backup is kept for several weeks. The frequency and scheduling of backups and the length of time they are kept may change with no prior notice. If you are unable to find files that you would like to restore in the snapshots of your home directory, contact the NCAR Research Computing help desk to request restoration of the files if they are in a backup. Core dump files are not backed up. Core dump file names typically follow this format: core.xxxxx (where the extension can include from one to five digits). CISL does not purge or scrub your home directory, and deletes files only as stated in the following data retention policy . Data retention policy \u00b6 When your account is closed, files will remain in your home directory for 30 days, after which CISL backs up the final contents and removes them from the file system. This backup is retained for six months after account termination. However, note that your project and group memberships are also terminated as described below, which can limit your ability to access files based on shared group permissions. If one or more of your project allocations expires but your account is not closed, files are retained in your home directory. Core dump files are not archived. Work space \u00b6 Your /glade/work/<username> space is best suited for actively working with data sets over time periods greater than what is permitted in the scratch space. The default quota for these spaces is 1 TB. Purge policy \u00b6 This space is not purged or scrubbed. CISL deletes files only as stated in the following data retention policy . Data retention policy \u00b6 When your user account is closed, files are retained for 30 days before being deleted . Files are not recoverable from backups, as there are none. If one or more of your project allocations expires but your account is not closed, your work directory files are retained. Scratch file space \u00b6 Each user has a /glade/derecho/scratch/<username> space by default, with an individual quota of 30 TB. The scratch file space is intended to support output from large-scale capability runs as well as computing and analysis workflows across CISL-managed resources. It is a temporary space for data to be analyzed and removed within a short amount of time. If you will need to occupy more than your quota of scratch space at some point, contact the NCAR Research Computing help desk to request a temporary increase. Include a paragraph justifying your need for additional space when making your request. Purge policy \u00b6 Individual files are removed from the scratch space automatically if they have not been accessed (for example: modified, read, or copied) in more than 120 days. A file's access time ( atime ) is updated at most once per day for purposes of I/O efficiency. To check a file's atime , run ls -ul filename . Users may not run touch commands or similar commands for the purpose of altering their files' timestamps to circumvent this purge policy. CISL staff will reduce the scratch quotas of users who violate this policy; running jobs may be killed as a result. In addition: CISL routinely monitors scratch space usage to ensure that it remains below the 90% mark and to determine if a reduction in the retention period is necessary. We will announce in advance any changes to the retention period. Best practice: To help us avoid the need to shorten the retention period, please use this space conscientiously. Delete files that you no longer need as soon as you're done with them rather than leave large amounts of data sitting untouched for the full 120 days. If you need to retain data on disk for more than 120 days, consider using your /glade/work space or Campaign Storage space. Data retention policy \u00b6 When your account is closed, files are purged automatically as they become 120 days old. If one or more of your project allocations expires but your account is not closed, your scratch files are removed as stated in the purge policy. Files are not recoverable from backups, as there are none. Campaign Storage / project space \u00b6 Dedicated project spaces on the /glade/campaign file system are available through our allocations process to support longer-term disk needs that are not easily accommodated by the scratch or work spaces. Allocations for project spaces are made to collaborative groups of users through the University/CHAP or NCAR allocations processes . The allocations are based on project needs and resource availability. Requests are reviewed according to the various allocation schedules. If you have a user account and project space but lack the directory permissions you need for that space, contact the NCAR Research Computing help desk to request changes. Identify the directories and the permissions you are requesting. Access reports \u00b6 CISL generates weekly usage reports throughout /glade to help users manage their data. The reports provide a summary of when files were last accessed, how much space is used, and details for the top 25 users. The files are named access_report.txt and can be found at the top-level of shared file spaces, for example: /glade/campaign/group_name/ (or similar, depending on the project) /glade/p/lab_name/group_name/ (or similar, depending on the project) /glade/p/univ/project_code/ /glade/p/uwyo/project_code/ Data retention policy \u00b6 Users' files are not deleted from project space after their accounts become inactive. Files are not recoverable from backups, as there are none. CISL reserves the right to reclaim space from expired projects. As disk space is a limited resource shared by the entire community, permanent storage of infrequently accessed data should be avoided. These spaces should be used as efficiently as possible so that other projects can take advantage of the resource. The Quasar tape storage system is more appropriate for long-term storage of infrequently accessed data. Checking space usage \u00b6 Knowing your quotas and usage is important \u00b6 All files that you own in your home, work, or scratch spaces are counted against your GLADE quota, regardless of the directory in which they are stored. If you write files to another user's home or scratch space, for example, they still count against your own individual user quota for that space. If you reach your disk quotas for the GLADE file spaces (see gladequota below), you may encounter problems until you remove files to make more space available. For example, you may not be able to log in, the system may appear hung, you may not be able to access some of your directories or files, your batch jobs may fail, and commands may not work as expected. If you cannot log in or execute commands, contact the NCAR Research Computing help desk . You can check your space usage as shown below. gladequota command \u00b6 This command will generate a report showing your quota and usage information: gladequota Current GLADE space usage: csgteam Space Used Quota % Full # Files ----------------------------------------- ------------ ------------ --------- ----------- /glade/derecho/scratch/csgteam 111.03 TiB 400.00 TiB 27.76 % 10226778 /glade/cheyenne/scratch/csgteam 152.70 GiB 100.00 TiB 0.15 % 439076 /glade/work/csgteam 1.40 TiB 4.00 TiB 35.02 % 9153458 /glade/u/home/csgteam 68.27 GiB 100.00 GiB 68.27 % 237251 ----------------------------------------- ------------ ------------ --------- ----------- /glade/collections/cmip 1.26 PiB 2.93 PiB 42.95 % 3326866 /glade/p/cisl/CSG 1.69 TiB 5.00 TiB 33.79 % 8168998 /glade/u/apps 2.02 TiB 10.00 TiB 20.19 % 20038473 ----------------------------------------- ------------ ------------ --------- ----------- Campaign: csgteam (user total) 1.41 TiB n/a n/a 9891104 /glade/campaign/cisl/csg 7.94 TiB 23.00 TiB 34.54 % 2859 /glade/campaign/cisl/sssg0001 217.15 TiB 250.00 TiB 86.86 % 50606617 Note Output from the gladequota command will show the home space quota as 100 GB instead of 50 GB. This is because the system stores dual copies of users' data for increased data integrity and safety. In some circumstances, queries of storage utilization from du and ls will also report a duplicated data footprint in your home directory for the same reason. General data retention policies and procedures \u00b6 Project data \u00b6 When a sponsored project approaches expiry, there are several steps in the process that affect the accessibility of associated data: 30 days before expiration, the project PIs will receive an email reminding them of the pending expiration. The project team should assess remaining files and disposition appropriately in preparation for group deactivation. 90 days after project expiration, the UNIX group associated with the project is removed. At this point users with accounts remaining on the system will likely no longer have access permissions to the projects' data, as the primary group no longer exists. It is therefore imperative that any remaining project data be relocated and ownership permissions assessed prior to this group deactivation. Finally, files are removed as scheduled on the timeline described above for the relevant file system. Restoring access to project data \u00b6 CISL has limited ability to modify access to project data after the 90-day post-expiry window. Such modifications require the approval of the original project owner. CISL has no ability to restore data after the purge or removal policies stated above have taken effect. User accounts \u00b6 User accounts are deactivated when they are no longer associated with an active project. When a user account is deactivated, several steps in the process affect the accessibility of the users' data: 30 days after a user account is deactivated, a final home directory backup is performed and the home directory is removed. The user\u2019s work directory is removed. No backup is performed. Finally, additional scratch files are removed as scheduled on the timeline described above for the relevant file system. Restoring access to collaborators' data \u00b6 A typical request for data access comes not from the departing user, but from remaining collaborators. Colleagues occasionally request access to a departed users' files, sometimes many months after the account is terminated, often when they realize the original owner set permissions that limit their access. While CISL has a limited ability to help in these situations, there are also legal limits to what we can do. For example, CISL cannot share files beyond the clear intent of the original owner as inferred from the UNIX file permissions. If a collaborator would like access to a file that was previously group- or world-readable, we may be able to help. If the original file was restricted to user-only read, however, we cannot override those intentions. The only exceptions to this policy are in compliance with broader UCAR IT records or investigation policies as described in UCAR's 1-7 Information Security Policy.","title":"GLADE file spaces"},{"location":"storage-systems/glade/#glade-file-spaces","text":"","title":"GLADE file spaces"},{"location":"storage-systems/glade/#overview","text":"The Globally Accessible Data Environment \u2013 a centralized file service known as GLADE \u2013 uses high-performance Spectrum Scale and Lustre shared file system technology to give users a common view of their data across the HPC, analysis, and visualization resources that CISL manages. File space User quota (Size/Count) Backup Purge policy Descriptions/notices Home: /glade/u/home/<username> 50 GB Yes Not purged User home directory Access: POSIX Scratch: (Cheyenne) /glade/scratch/<username> 10 TB No 120 days Temporary computational space Access: POSIX Scratch: (Derecho) /glade/derecho/scratch/<username> 30TB / 10M No 180 days Derecho's scratch file system also includes a limit on a users' total number of files Work: /glade/work/<username> 2 TB No Not purged User work space Access: POSIX Project: /glade/p/ entity/project_code N/A No Not purged Deprecated; will be migrated to Campaign Storage in 2023 Collections: /glade/collections N/A No Not purged Curated collections (CMIP, RDA, others) Access: POSIX Campaign Storage: /glade/campaign N/A No Not purged Project space allocations (via allocation request) GLADE system status report CISL backs up the GLADE home file space several times a week and also creates snapshots to enable users to recover deleted files quickly and easily. Data can remain in each of these spaces in accordance with the policies detailed below. The policies are subject to change; any changes necessary will be announced in advance. CISL does not provide backups of other spaces. You are responsible for the safe storage of any data that must be preserved. Best practice: Check your space usage regularly with gladequota Check your space usage regularly with gladequota as described below, and remove data that you no longer need. You can conserve GLADE space by storing large files, such as tar files, rather than numerous small, individual files. This is because the system allocates a minimum amount of space for each file (currently configured to 16 KB), no matter how small the file is. Thus 16KB is the smallest amount of space the system can allocate to a file, including empty files, directories and symlinks.","title":"Overview"},{"location":"storage-systems/glade/#status","text":"","title":"Status"},{"location":"storage-systems/glade/#home-space","text":"Each user has a /glade/u/home/<username> space with a quota of 50 GB* for managing scripts, source code, and small data sets. It is backed up weekly, with backups retained for several months. CISL also creates snapshots of the space to enable users to recover deleted files quickly and easily.","title":"Home space"},{"location":"storage-systems/glade/#backup-policy","text":"Your /glade/u/home directory is backed up several times a week while your account is open. Each backup is kept for several weeks. The frequency and scheduling of backups and the length of time they are kept may change with no prior notice. If you are unable to find files that you would like to restore in the snapshots of your home directory, contact the NCAR Research Computing help desk to request restoration of the files if they are in a backup. Core dump files are not backed up. Core dump file names typically follow this format: core.xxxxx (where the extension can include from one to five digits). CISL does not purge or scrub your home directory, and deletes files only as stated in the following data retention policy .","title":"Backup policy"},{"location":"storage-systems/glade/#data-retention-policy","text":"When your account is closed, files will remain in your home directory for 30 days, after which CISL backs up the final contents and removes them from the file system. This backup is retained for six months after account termination. However, note that your project and group memberships are also terminated as described below, which can limit your ability to access files based on shared group permissions. If one or more of your project allocations expires but your account is not closed, files are retained in your home directory. Core dump files are not archived.","title":"Data retention policy"},{"location":"storage-systems/glade/#work-space","text":"Your /glade/work/<username> space is best suited for actively working with data sets over time periods greater than what is permitted in the scratch space. The default quota for these spaces is 1 TB.","title":"Work space"},{"location":"storage-systems/glade/#purge-policy","text":"This space is not purged or scrubbed. CISL deletes files only as stated in the following data retention policy .","title":"Purge policy"},{"location":"storage-systems/glade/#data-retention-policy_1","text":"When your user account is closed, files are retained for 30 days before being deleted . Files are not recoverable from backups, as there are none. If one or more of your project allocations expires but your account is not closed, your work directory files are retained.","title":"Data retention policy"},{"location":"storage-systems/glade/#scratch-file-space","text":"Each user has a /glade/derecho/scratch/<username> space by default, with an individual quota of 30 TB. The scratch file space is intended to support output from large-scale capability runs as well as computing and analysis workflows across CISL-managed resources. It is a temporary space for data to be analyzed and removed within a short amount of time. If you will need to occupy more than your quota of scratch space at some point, contact the NCAR Research Computing help desk to request a temporary increase. Include a paragraph justifying your need for additional space when making your request.","title":"Scratch file space"},{"location":"storage-systems/glade/#purge-policy_1","text":"Individual files are removed from the scratch space automatically if they have not been accessed (for example: modified, read, or copied) in more than 120 days. A file's access time ( atime ) is updated at most once per day for purposes of I/O efficiency. To check a file's atime , run ls -ul filename . Users may not run touch commands or similar commands for the purpose of altering their files' timestamps to circumvent this purge policy. CISL staff will reduce the scratch quotas of users who violate this policy; running jobs may be killed as a result. In addition: CISL routinely monitors scratch space usage to ensure that it remains below the 90% mark and to determine if a reduction in the retention period is necessary. We will announce in advance any changes to the retention period. Best practice: To help us avoid the need to shorten the retention period, please use this space conscientiously. Delete files that you no longer need as soon as you're done with them rather than leave large amounts of data sitting untouched for the full 120 days. If you need to retain data on disk for more than 120 days, consider using your /glade/work space or Campaign Storage space.","title":"Purge policy"},{"location":"storage-systems/glade/#data-retention-policy_2","text":"When your account is closed, files are purged automatically as they become 120 days old. If one or more of your project allocations expires but your account is not closed, your scratch files are removed as stated in the purge policy. Files are not recoverable from backups, as there are none.","title":"Data retention policy"},{"location":"storage-systems/glade/#campaign-storage-project-space","text":"Dedicated project spaces on the /glade/campaign file system are available through our allocations process to support longer-term disk needs that are not easily accommodated by the scratch or work spaces. Allocations for project spaces are made to collaborative groups of users through the University/CHAP or NCAR allocations processes . The allocations are based on project needs and resource availability. Requests are reviewed according to the various allocation schedules. If you have a user account and project space but lack the directory permissions you need for that space, contact the NCAR Research Computing help desk to request changes. Identify the directories and the permissions you are requesting.","title":"Campaign Storage / project space"},{"location":"storage-systems/glade/#access-reports","text":"CISL generates weekly usage reports throughout /glade to help users manage their data. The reports provide a summary of when files were last accessed, how much space is used, and details for the top 25 users. The files are named access_report.txt and can be found at the top-level of shared file spaces, for example: /glade/campaign/group_name/ (or similar, depending on the project) /glade/p/lab_name/group_name/ (or similar, depending on the project) /glade/p/univ/project_code/ /glade/p/uwyo/project_code/","title":"Access reports"},{"location":"storage-systems/glade/#data-retention-policy_3","text":"Users' files are not deleted from project space after their accounts become inactive. Files are not recoverable from backups, as there are none. CISL reserves the right to reclaim space from expired projects. As disk space is a limited resource shared by the entire community, permanent storage of infrequently accessed data should be avoided. These spaces should be used as efficiently as possible so that other projects can take advantage of the resource. The Quasar tape storage system is more appropriate for long-term storage of infrequently accessed data.","title":"Data retention policy"},{"location":"storage-systems/glade/#checking-space-usage","text":"","title":"Checking space usage"},{"location":"storage-systems/glade/#knowing-your-quotas-and-usage-is-important","text":"All files that you own in your home, work, or scratch spaces are counted against your GLADE quota, regardless of the directory in which they are stored. If you write files to another user's home or scratch space, for example, they still count against your own individual user quota for that space. If you reach your disk quotas for the GLADE file spaces (see gladequota below), you may encounter problems until you remove files to make more space available. For example, you may not be able to log in, the system may appear hung, you may not be able to access some of your directories or files, your batch jobs may fail, and commands may not work as expected. If you cannot log in or execute commands, contact the NCAR Research Computing help desk . You can check your space usage as shown below.","title":"Knowing your quotas and usage is important"},{"location":"storage-systems/glade/#gladequota-command","text":"This command will generate a report showing your quota and usage information: gladequota Current GLADE space usage: csgteam Space Used Quota % Full # Files ----------------------------------------- ------------ ------------ --------- ----------- /glade/derecho/scratch/csgteam 111.03 TiB 400.00 TiB 27.76 % 10226778 /glade/cheyenne/scratch/csgteam 152.70 GiB 100.00 TiB 0.15 % 439076 /glade/work/csgteam 1.40 TiB 4.00 TiB 35.02 % 9153458 /glade/u/home/csgteam 68.27 GiB 100.00 GiB 68.27 % 237251 ----------------------------------------- ------------ ------------ --------- ----------- /glade/collections/cmip 1.26 PiB 2.93 PiB 42.95 % 3326866 /glade/p/cisl/CSG 1.69 TiB 5.00 TiB 33.79 % 8168998 /glade/u/apps 2.02 TiB 10.00 TiB 20.19 % 20038473 ----------------------------------------- ------------ ------------ --------- ----------- Campaign: csgteam (user total) 1.41 TiB n/a n/a 9891104 /glade/campaign/cisl/csg 7.94 TiB 23.00 TiB 34.54 % 2859 /glade/campaign/cisl/sssg0001 217.15 TiB 250.00 TiB 86.86 % 50606617 Note Output from the gladequota command will show the home space quota as 100 GB instead of 50 GB. This is because the system stores dual copies of users' data for increased data integrity and safety. In some circumstances, queries of storage utilization from du and ls will also report a duplicated data footprint in your home directory for the same reason.","title":"gladequota command"},{"location":"storage-systems/glade/#general-data-retention-policies-and-procedures","text":"","title":"General data retention policies and procedures"},{"location":"storage-systems/glade/#project-data","text":"When a sponsored project approaches expiry, there are several steps in the process that affect the accessibility of associated data: 30 days before expiration, the project PIs will receive an email reminding them of the pending expiration. The project team should assess remaining files and disposition appropriately in preparation for group deactivation. 90 days after project expiration, the UNIX group associated with the project is removed. At this point users with accounts remaining on the system will likely no longer have access permissions to the projects' data, as the primary group no longer exists. It is therefore imperative that any remaining project data be relocated and ownership permissions assessed prior to this group deactivation. Finally, files are removed as scheduled on the timeline described above for the relevant file system.","title":"Project data"},{"location":"storage-systems/glade/#restoring-access-to-project-data","text":"CISL has limited ability to modify access to project data after the 90-day post-expiry window. Such modifications require the approval of the original project owner. CISL has no ability to restore data after the purge or removal policies stated above have taken effect.","title":"Restoring access to project data"},{"location":"storage-systems/glade/#user-accounts","text":"User accounts are deactivated when they are no longer associated with an active project. When a user account is deactivated, several steps in the process affect the accessibility of the users' data: 30 days after a user account is deactivated, a final home directory backup is performed and the home directory is removed. The user\u2019s work directory is removed. No backup is performed. Finally, additional scratch files are removed as scheduled on the timeline described above for the relevant file system.","title":"User accounts"},{"location":"storage-systems/glade/#restoring-access-to-collaborators-data","text":"A typical request for data access comes not from the departing user, but from remaining collaborators. Colleagues occasionally request access to a departed users' files, sometimes many months after the account is terminated, often when they realize the original owner set permissions that limit their access. While CISL has a limited ability to help in these situations, there are also legal limits to what we can do. For example, CISL cannot share files beyond the clear intent of the original owner as inferred from the UNIX file permissions. If a collaborator would like access to a file that was previously group- or world-readable, we may be able to help. If the original file was restricted to user-only read, however, we cannot override those intentions. The only exceptions to this policy are in compliance with broader UCAR IT records or investigation policies as described in UCAR's 1-7 Information Security Policy.","title":"Restoring access to collaborators' data"},{"location":"storage-systems/glade/campaign/","text":"Campaign Storage file system \u00b6 NCAR Campaign Storage is a resource for medium-term storage of project data, typically for three to five years, by NCAR labs and universities that have project allocations. Campaign Storage is accessible a number of ways that are described below: through the Globus web and command-line interfaces from the data-access nodes, for Globus transfers and managing data holdings from the Derecho and Casper clusters to facilitate data analysis and visualization workflows Files stored on this system are not backed up. Users are responsible for replicating any data they feel should be stored at an additional location as backup. Files that are deleted or overwritten cannot be recovered. Globus transfers \u00b6 The Globus mapped collection established for the file system is NCAR Campaign Storage . How to make transfers to and from that collection is documented here: Globus file transfers . How to make transfers using the command line interface also is covered in detail in this tutorial: Using Globus v5 at NCAR (tutorial) . CAVEAT: The Globus interface for transferring data does not handle symbolic links and does not create symbolic links on a destination endpoint. Data-access nodes \u00b6 The Campaign Storage file system is mounted on the data-access nodes as /glade/campaign to: enable users to manage file and directory permissions using POSIX commands. facilitate transfers of small files to and from GLADE spaces such as /glade/derecho/scratch and /glade/work . HPC system use \u00b6 The Campaign Storage file system can be accessed from the Casper and Derecho clusters as /glade/campaign so users are able to: read and write data directly from their data analysis and visualization workflows. submit batch scripts to migrate data to the Campaign Storage resource. Data retention policy \u00b6 Campaign Storage is designed to provide medium-term storage for project data, typically for three to five years. While data will not be purged automatically after five years, retaining data longer will reduce the capacity for storing additional, new data. Users are expected to monitor their holdings, remove files that are no longer needed, and move necessary data to other storage options for longer-term preservation. NCAR researchers are expected to collaborate with CISL\u2019s Digital Asset Services Hub (log in to Sundog ) to develop data migration plans for storage needs that exceed five years. University researchers are expected to transfer their project data to their home institutions or other alternative storage repositories within one year of their NSF grant expiring. CISL will not award storage space for researchers to carry data forward from one grant to another. Allocations \u00b6 NCAR labs \u00b6 Each NCAR lab has an allocation of Campaign Storage space and the labs manage how those allocations are used. Users who have questions related to lab allocations should contact the lab's own allocation representative. Universities \u00b6 University users can request Campaign Storage space through the NCAR Resource Allocation System as supplements to their project allocations. Requests must include detailed justification for the amount of space requested. Because NCAR is not currently funded to provide long-term data storage services to the university community, university users' requests for these allocations are prioritized based on the following factors. Higher priority is given to requests if: \u00b6 You have an active project, supported by an active NSF award, for using Cheyenne. Your request is for a period of no more than three (3) months and to support migrating of your data to your home institution. Lower priority is given to requests if: \u00b6 Your need relates to satisfying external requirements or promises \u2013 to a publisher or agency, for example \u2013 to retain data for extended periods. Any data requiring longer storage should be migrated to your home institution or to another appropriate repository. Reports \u00b6 The Systems Accounting Manager ( SAM ) provides overall summary information about the use of Campaign Storage allocations and other allocations. CISL is developing additional tools for use in allocation management. Automated data compression \u00b6 Campaign Storage has an automated data compression feature for long-duration data sets. Our compression policy targets files that are 180 days old or older and 100MB in size or larger for \"z\" compression using IBM Spectrum Scale file system mechanisms (details below). The action is transparent to the user \u2013 that is, no changes to metadata timestamps or reported size occur, and subsequent reads of the data proceed as usual. During a read, the compressed data are sent to the file system client and then transparently uncompressed for application use. Tool and accounting behavior \u00b6 The number of blocks reported consumed by the file will change. Note the following tool-specific behavior: Tool Output ls -l shows original (uncompressed) file size stat shows compressed number of blocks, original file size du shows compressed file sizes. du \u2013apparent-size shows original (uncompressed) size gladequota shows project space usage after compression, as do SAM reports Individual data sets can be excluded from the compression algorithm, if necessary. To discuss this option, please submit a request through the NCAR Research Computing help desk . Compression details \u00b6 When a file is considered for compression, the algorithm tests compression of chunks of the file. If the realized compression efficiency of a given chunk is at least 10%, it is then stored compressed on disk. The compression status of a file can be queried via the mmlsattr command. Follow this example: /usr/lpp/mmfs/bin/mmlsattr -L filename A file has been compressed if the mmlsattr output: includes \"Misc attributes: COMPRESSION\" \u2013 which indicates that the file was targeted for compression. does not include \"flags: illCompressed\" \u2013 which indicates the file was targeted or deferred but not yet compressed. Several output examples are provided below. User-driven manual compression is also possible before the automated policy is triggered if desired via the mmchattr command: /usr/lpp/mmfs/bin/mmchattr [-I defer] \u254ccompression z filename z: best compression (Campaign Storage default) If deferred, the file will be compressed during the next Campaign Storage policy execution rather than instantly. Examples: commands and output \u00b6 Run du, ls, stat for an original uncompressed file \u00b6 $ du -h 1GB.dat && du -h --apparent-size 1GB.dat && ls -lh 1GB.dat && stat 1GB.dat 1000M 1GB.dat 1000M 1GB.dat -rw-r-----+ 1 benkirk csg 1000M Mar 9 10:08 1GB.dat File: \u20181GB.dat\u2019 Size: 1048576000 Blocks: 2048000 IO Block: 8388608 regular file Device: 2dh/45d Inode: 1006073884 Links: 1 Access: (0640/-rw-r-----) Uid: (38057/ benkirk) Gid: ( 1564/ csg) Access: 2022-03-09 10:08:00.479563000 -0700 Modify: 2022-03-09 10:08:01.486585235 -0700 Change: 2022-03-09 10:08:01.486585235 -0700 Birth: - Request compression manually \u00b6 $ /usr/lpp/mmfs/bin/mmchattr --compression z 1GB.dat Run du, ls, stat for a compressed file (note that metadata dates are not changed) \u00b6 $ du -h 1GB.dat && du -h --apparent-size 1GB.dat && ls -lh 1GB.dat && stat 1GB.dat 104M 1GB.dat 1000M 1GB.dat -rw-r-----+ 1 benkirk csg 1000M Mar 9 10:08 1GB.dat File: \u20181GB.dat\u2019 Size: 1048576000 Blocks: 212992 IO Block: 8388608 regular file Device: 2dh/45d Inode: 1006073884 Links: 1 Access: (0640/-rw-r-----) Uid: (38057/ benkirk) Gid: ( 1564/ csg) Access: 2022-03-09 10:08:00.479563000 -0700 Modify: 2022-03-09 10:08:01.486585235 -0700 Change: 2022-03-09 10:08:01.486585235 -0700 Birth: - List file attributes to verify a compressed file \u00b6 $ /usr/lpp/mmfs/bin/mmlsattr -L 1GB.dat file name: 1GB.dat metadata replication: 2 max 2 data replication: 1 max 2 immutable: no appendOnly: no flags: storage pool name: DATA fileset name: csg snapshot name: creation time: Wed Mar 9 10:08:00 2022 Misc attributes: ARCHIVE COMPRESSION (library z) Encrypted: no Request deferred compression of a file \u00b6 $ /usr/lpp/mmfs/bin/mmchattr -I defer --compression z 1GB_deferred.dat Note that deferred compression is recommended when manually requesting compression for a large number of files. In this case, the mmchattr command will return immediately, and the file compression will occur at the next regularly scheduled system interval. List file attributes (note that \"illcompressed\" indicates the compression has not yet been applied) \u00b6 $ /usr/lpp/mmfs/bin/mmlsattr -L 1GB_deferred.dat file name: 1GB_deferred.dat metadata replication: 2 max 2 data replication: 1 max 2 immutable: no appendOnly: no flags: illcompressed storage pool name: DATA fileset name: csg snapshot name: creation time: Wed Mar 9 10:07:17 2022 Misc attributes: ARCHIVE COMPRESSION (library z) Encrypted: no","title":"Campaign storage file system"},{"location":"storage-systems/glade/campaign/#campaign-storage-file-system","text":"NCAR Campaign Storage is a resource for medium-term storage of project data, typically for three to five years, by NCAR labs and universities that have project allocations. Campaign Storage is accessible a number of ways that are described below: through the Globus web and command-line interfaces from the data-access nodes, for Globus transfers and managing data holdings from the Derecho and Casper clusters to facilitate data analysis and visualization workflows Files stored on this system are not backed up. Users are responsible for replicating any data they feel should be stored at an additional location as backup. Files that are deleted or overwritten cannot be recovered.","title":"Campaign Storage file system"},{"location":"storage-systems/glade/campaign/#globus-transfers","text":"The Globus mapped collection established for the file system is NCAR Campaign Storage . How to make transfers to and from that collection is documented here: Globus file transfers . How to make transfers using the command line interface also is covered in detail in this tutorial: Using Globus v5 at NCAR (tutorial) . CAVEAT: The Globus interface for transferring data does not handle symbolic links and does not create symbolic links on a destination endpoint.","title":"Globus transfers"},{"location":"storage-systems/glade/campaign/#data-access-nodes","text":"The Campaign Storage file system is mounted on the data-access nodes as /glade/campaign to: enable users to manage file and directory permissions using POSIX commands. facilitate transfers of small files to and from GLADE spaces such as /glade/derecho/scratch and /glade/work .","title":"Data-access nodes"},{"location":"storage-systems/glade/campaign/#hpc-system-use","text":"The Campaign Storage file system can be accessed from the Casper and Derecho clusters as /glade/campaign so users are able to: read and write data directly from their data analysis and visualization workflows. submit batch scripts to migrate data to the Campaign Storage resource.","title":"HPC system use"},{"location":"storage-systems/glade/campaign/#data-retention-policy","text":"Campaign Storage is designed to provide medium-term storage for project data, typically for three to five years. While data will not be purged automatically after five years, retaining data longer will reduce the capacity for storing additional, new data. Users are expected to monitor their holdings, remove files that are no longer needed, and move necessary data to other storage options for longer-term preservation. NCAR researchers are expected to collaborate with CISL\u2019s Digital Asset Services Hub (log in to Sundog ) to develop data migration plans for storage needs that exceed five years. University researchers are expected to transfer their project data to their home institutions or other alternative storage repositories within one year of their NSF grant expiring. CISL will not award storage space for researchers to carry data forward from one grant to another.","title":"Data retention policy"},{"location":"storage-systems/glade/campaign/#allocations","text":"","title":"Allocations"},{"location":"storage-systems/glade/campaign/#ncar-labs","text":"Each NCAR lab has an allocation of Campaign Storage space and the labs manage how those allocations are used. Users who have questions related to lab allocations should contact the lab's own allocation representative.","title":"NCAR labs"},{"location":"storage-systems/glade/campaign/#universities","text":"University users can request Campaign Storage space through the NCAR Resource Allocation System as supplements to their project allocations. Requests must include detailed justification for the amount of space requested. Because NCAR is not currently funded to provide long-term data storage services to the university community, university users' requests for these allocations are prioritized based on the following factors.","title":"Universities"},{"location":"storage-systems/glade/campaign/#higher-priority-is-given-to-requests-if","text":"You have an active project, supported by an active NSF award, for using Cheyenne. Your request is for a period of no more than three (3) months and to support migrating of your data to your home institution.","title":"Higher priority is given to requests if:"},{"location":"storage-systems/glade/campaign/#lower-priority-is-given-to-requests-if","text":"Your need relates to satisfying external requirements or promises \u2013 to a publisher or agency, for example \u2013 to retain data for extended periods. Any data requiring longer storage should be migrated to your home institution or to another appropriate repository.","title":"Lower priority is given to requests if:"},{"location":"storage-systems/glade/campaign/#reports","text":"The Systems Accounting Manager ( SAM ) provides overall summary information about the use of Campaign Storage allocations and other allocations. CISL is developing additional tools for use in allocation management.","title":"Reports"},{"location":"storage-systems/glade/campaign/#automated-data-compression","text":"Campaign Storage has an automated data compression feature for long-duration data sets. Our compression policy targets files that are 180 days old or older and 100MB in size or larger for \"z\" compression using IBM Spectrum Scale file system mechanisms (details below). The action is transparent to the user \u2013 that is, no changes to metadata timestamps or reported size occur, and subsequent reads of the data proceed as usual. During a read, the compressed data are sent to the file system client and then transparently uncompressed for application use.","title":"Automated data compression"},{"location":"storage-systems/glade/campaign/#tool-and-accounting-behavior","text":"The number of blocks reported consumed by the file will change. Note the following tool-specific behavior: Tool Output ls -l shows original (uncompressed) file size stat shows compressed number of blocks, original file size du shows compressed file sizes. du \u2013apparent-size shows original (uncompressed) size gladequota shows project space usage after compression, as do SAM reports Individual data sets can be excluded from the compression algorithm, if necessary. To discuss this option, please submit a request through the NCAR Research Computing help desk .","title":"Tool and accounting behavior"},{"location":"storage-systems/glade/campaign/#compression-details","text":"When a file is considered for compression, the algorithm tests compression of chunks of the file. If the realized compression efficiency of a given chunk is at least 10%, it is then stored compressed on disk. The compression status of a file can be queried via the mmlsattr command. Follow this example: /usr/lpp/mmfs/bin/mmlsattr -L filename A file has been compressed if the mmlsattr output: includes \"Misc attributes: COMPRESSION\" \u2013 which indicates that the file was targeted for compression. does not include \"flags: illCompressed\" \u2013 which indicates the file was targeted or deferred but not yet compressed. Several output examples are provided below. User-driven manual compression is also possible before the automated policy is triggered if desired via the mmchattr command: /usr/lpp/mmfs/bin/mmchattr [-I defer] \u254ccompression z filename z: best compression (Campaign Storage default) If deferred, the file will be compressed during the next Campaign Storage policy execution rather than instantly.","title":"Compression details"},{"location":"storage-systems/glade/campaign/#examples-commands-and-output","text":"","title":"Examples: commands and output"},{"location":"storage-systems/glade/campaign/#run-du-ls-stat-for-an-original-uncompressed-file","text":"$ du -h 1GB.dat && du -h --apparent-size 1GB.dat && ls -lh 1GB.dat && stat 1GB.dat 1000M 1GB.dat 1000M 1GB.dat -rw-r-----+ 1 benkirk csg 1000M Mar 9 10:08 1GB.dat File: \u20181GB.dat\u2019 Size: 1048576000 Blocks: 2048000 IO Block: 8388608 regular file Device: 2dh/45d Inode: 1006073884 Links: 1 Access: (0640/-rw-r-----) Uid: (38057/ benkirk) Gid: ( 1564/ csg) Access: 2022-03-09 10:08:00.479563000 -0700 Modify: 2022-03-09 10:08:01.486585235 -0700 Change: 2022-03-09 10:08:01.486585235 -0700 Birth: -","title":"Run du, ls, stat for an original uncompressed file"},{"location":"storage-systems/glade/campaign/#request-compression-manually","text":"$ /usr/lpp/mmfs/bin/mmchattr --compression z 1GB.dat","title":"Request compression manually"},{"location":"storage-systems/glade/campaign/#run-du-ls-stat-for-a-compressed-file-note-that-metadata-dates-are-not-changed","text":"$ du -h 1GB.dat && du -h --apparent-size 1GB.dat && ls -lh 1GB.dat && stat 1GB.dat 104M 1GB.dat 1000M 1GB.dat -rw-r-----+ 1 benkirk csg 1000M Mar 9 10:08 1GB.dat File: \u20181GB.dat\u2019 Size: 1048576000 Blocks: 212992 IO Block: 8388608 regular file Device: 2dh/45d Inode: 1006073884 Links: 1 Access: (0640/-rw-r-----) Uid: (38057/ benkirk) Gid: ( 1564/ csg) Access: 2022-03-09 10:08:00.479563000 -0700 Modify: 2022-03-09 10:08:01.486585235 -0700 Change: 2022-03-09 10:08:01.486585235 -0700 Birth: -","title":"Run du, ls, stat for a compressed file (note that metadata dates are not changed)"},{"location":"storage-systems/glade/campaign/#list-file-attributes-to-verify-a-compressed-file","text":"$ /usr/lpp/mmfs/bin/mmlsattr -L 1GB.dat file name: 1GB.dat metadata replication: 2 max 2 data replication: 1 max 2 immutable: no appendOnly: no flags: storage pool name: DATA fileset name: csg snapshot name: creation time: Wed Mar 9 10:08:00 2022 Misc attributes: ARCHIVE COMPRESSION (library z) Encrypted: no","title":"List file attributes to verify a compressed file"},{"location":"storage-systems/glade/campaign/#request-deferred-compression-of-a-file","text":"$ /usr/lpp/mmfs/bin/mmchattr -I defer --compression z 1GB_deferred.dat Note that deferred compression is recommended when manually requesting compression for a large number of files. In this case, the mmchattr command will return immediately, and the file compression will occur at the next regularly scheduled system interval.","title":"Request deferred compression of a file"},{"location":"storage-systems/glade/campaign/#list-file-attributes-note-that-illcompressed-indicates-the-compression-has-not-yet-been-applied","text":"$ /usr/lpp/mmfs/bin/mmlsattr -L 1GB_deferred.dat file name: 1GB_deferred.dat metadata replication: 2 max 2 data replication: 1 max 2 immutable: no appendOnly: no flags: illcompressed storage pool name: DATA fileset name: csg snapshot name: creation time: Wed Mar 9 10:07:17 2022 Misc attributes: ARCHIVE COMPRESSION (library z) Encrypted: no","title":"List file attributes (note that \"illcompressed\" indicates the compression has not yet been applied)"},{"location":"storage-systems/glade/lustre/","text":"Lustre scratch file system \u00b6 The De recho Stor age subsystsem ( Destor ) scratch file system is a Lustre-based Cray ClusterStor E1000 product configured as shown in the table below. An open-source, parallel system, Lustre will be familiar to users of similar POSIX-compliant file systems. This documentation provides a high-level overview of important Lustre concepts and terminology to help users achieve optimal performance. Capacity and components \u00b6 Total capacity of the system is 40 TB of metadata and 60 PB of data. Component Quantity Details Metadata servers (MDS) 4 Each MDS has two 200 GbE CX6 Cassini network interfaces configured in an active/passive failover pair. Metadata targets (MDT) 4 Each metadata server has a single 12 TB MDT composed of 11 drives in a RAID-10 configuration formatted with ldiskfs (~40 TB usable metadata across the entire file system). Object storage servers (OSS) 24 Each OSS has a single 200 GbE CX6 Cassini network interface. Object storage targets (OST) 96 Each OSS has four 582 TB OSTs. Each OST is composed of 53 drives in a GridRAID configuration formatted with ldiskfs (~60PB usable data across the entire file system). (Additional hardware details are provided below ). Terminology \u00b6 Metadata and data \u00b6 The notion of file metadata and data as related but separable entities is important to understanding Lustre because it is fundamental to the system's parallelization strategy. In a POSIX file system, the metadata describes information about a file (name, permissions, access controls, timestamps, and so on), and the data contains the contents of the file itself. Lustre employs one or more metadata servers (MDS) to store the metadata and data layout of each file, and several object storage servers (OSS) to hold the file contents. Each MDS has one or more metadata targets (MDT), which are storage devices attached to the MDS. Similarly, each OSS has one or more object storage target (OST) storage devices. Typically, the MDTs and OSTs are accessible from two different servers, providing fault tolerance and failover capabilities. A typical Lustre file system is shown in Figure 1 below. Figure 1: Sample Lustre file system: 4 metadata servers (MDS), 4 object storage servers (OSS). Credit: Introduction to Lustre Wiki . A file system may employ several metadata servers for scalability and load balancing, and several object storage servers for capacity and performance scalability. When a user creates a file on a Lustre file system, it communicates with an MDS that is responsible for managing the metadata of the file. The MDS also holds the file's striping layout , which is a template used to map file contents (conceptually, data blocks) onto one or more OSTs. This is important to understand because when users are interacting with a Lustre file, they are really interfacing with several storage servers. Different file operations require different server communication requirements. For example, querying a file's modification time is a metadata-only operation and thus requires communication only with the MDS, whereas querying a file's size involves each OSS over which the file is striped. File striping \u00b6 File striping is a key feature of Lustre file systems. A file is said to be striped when its sequence of bytes is separated into small chunks, or stripes , so that read and write operations can involve multiple OSTs concurrently. This process is illustrated in Figures 2 and 3. In Figure 2, the sample file is split into five stripes: the first four are the same size while the fifth is smaller and contains the \"remainder\" of the file. This introduces an important striping concept: the stripe size . Figure 2: Logical view of a file, broken into five \"stripe\" segments. The first four are the same size while the fifth is smaller and contains the \"remainder\" of the file. Credit: Lustre User Guide . Figure 3 shows how the stripes can be mapped onto several OSTs as defined by the stripe count . In this example, the stripe count is four and the stripe segments are assigned in a round-robin fashion. Figure 3: Physical view of a file broken into five stripes across four OST devices. Credit: Lustre User Guide . Striping has important benefits as well as some drawbacks. Striping over more OSTs allows for more bandwidth. In general, as more OSTs are used, more servers are involved, so more network and disk subsystem bandwidth is available. Striping also allows for files larger than any single OST. The primary drawback of striping is overhead: as more OSTs are employed to store a file, more network overhead is required to orchestrate the components. The preceding discussion is focused on striping the blocks of a given file. When multiple MDTs are present in the file system, as is the case with Derecho, metadata striping is also typically employed and the contents of directories are spread across the available MDSs in the system. Progressive file layouts \u00b6 The configurable stripe size and stripe count parameters were at one time the only modifiable parameters available to govern striping behavior, which made it difficult to implement a one-size-fits-all default configuration on large systems with varied use cases. The introduction of progressive file layouts (PFLs) in modern Lustre versions, however, extended the striping concept to multiple, progressive segments of files, as shown in Figure 4. Figure 4: Sample progressive file layout with three components of different stripe patterns. Credit: PFL Prototype High Level Design . In Figure 4, a single file is mapped to three separate components, each with a different striping layout. The first component has a stripe size of 1 MB with a stripe count of 1, and is 2 MB in total extent. This means the first 2 MB of the file will be striped over only one OST, in two 1-MB chunks. The second component begins beyond this 2 MB threshold up to 256 MB size. It employs a stripe size of 1 MB but increases the stripe count to four. Finally, the third and final component begins when the file size exceeds 256 MB. The stripe size increases to 4 MB and the stripe count to 32 OSTs. PFLs are useful because they define a template that is much more general than a single stripe size/count pair. They allow small files to be striped over a small number of OSTs and only incur the overhead of additional OST stripes when the file is sufficiently large to benefit from increased bandwidth. inodes and data blocks \u00b6 When a file system is constructed, the underlying storage device blocks are segregated into two components: data blocks and inodes . Data blocks are the most familiar; storing a 1 GB file simply requires a sufficient number of data blocks to hold its contents. Inodes, by contrast, are index nodes that hold the metadata associated with a file or directory: ownership, time stamps, striping information, and so on. The number of inodes available in a file system is generally fixed and provides a strict limit on the number of files and directories the file system can hold. This is especially important in a Lustre file system. Its capacity is limited by the size and quantity of OSTs, and its file count capacity is also limited by the number of inodes available on the MDTs. In an extreme example, it is possible to exhaust the available inodes in a file system before its storage capacity by creating many tiny files, so it is important to manage both the overall file system data volume and the file count. Derecho's Destor Lustre scratch file system \u00b6 With the background provided above , we can now discuss Derecho's Destor Lustre file system specific storage configuration. Configuration \u00b6 Derecho's Destor Lustre file system hardware configuration is shown schematically below. The system has 4 MDS servers (each with a single MDT), and 24 OSS servers (each with 4 OSTs). Default Striping \u00b6 Destor default striping configuration Destor uses progressive file layouts (PFLs) to accommodate a wide variety of file sizes and access patterns. File Segment Stripe Count Stripe Size 0-16MB 1 1MB 16MB-16GB 4 16MB 16GB-64GB 12 16MB 64GB+ 24 16MB This default PFL was created at the base of the file system and is inherited by default for all sub-directories. Users may apply alternate striping patterns for any directory or file they own. (File striping must be set prior to creation and cannot be changed on existing files.) Note that per-directory file striping is inherited by new files and sub-directories created within. The default Destor stripe configuration can be applied if needed with the lfs setstripe command: lfs setstripe \\ -E 16M -c 1 -S 1M \\ -E 16G -c 4 -S 16M \\ -E 64G -c 12 -S 16M \\ -E -1 -c 24 -S 16M See man lfs-setstripe for additional details. Performance expectations \u00b6 Lustre in general, and on Derecho specifically, is designed for high-speed, parallel access to large files . Key points: Derecho has >2400 compute nodes (each a Lustre client), All scratch metadata traffic, for all users, is served by only 4 MDS servers Tiny files do not effectively use the primary performance potential. Large files can be effectively spread across the storage cluster. 5,088 hard drives spread across 96 OSTs served up by 24 OSSes . Destor can deliver over 300 GB/sec of large file access bandwdth. Jobstats \u00b6 Lustre provides a mechanism to collate I/O statistics for reporting purposes using the job stats mechanism. On Destor we collect job statistics according to PBS_JOBID . These statistics are then presented in a graphana dashboard , where users can optionally query the statistics for a particular job. Best practices \u00b6 Manage your file count and file volume \u00b6 Users have quotas for both data volume and file count on the Derecho Lustre file system. The gladequota utility is preferred for reporting comprehensive storage usage across all GLADE file spaces, including Lustre scratch spaces. Additionally, the lfs quota command can be used to query Lustre-specific quota information. Many simulation codes produce large quantities of small, diagnostic output files that are useful for diagnosing problems but not often referenced for successful production runs. Consider removing or tarring such files incrementally in your workflow to manage overall file count. Avoid unnecessary metadata requests \u00b6 From the background provided above, it is clear not all metadata access requests are equal. Querying a file's timestamps is a cheap operation requiring communication with the appropriate MDT, whereas querying a file's size requires communication with each and every OST holding data stripes. Therefore, it is a best practice to be aware of these performance implications and request only the metadata needed for a given operation. For example, especially when in a large directory with hundreds of files, avoid typing ls -l if a simple ls will do. The former will communicate with every MDS and OSS in the file system in order to determine the current file size, while the latter is simply an MDS communication. Unnecessary communication can make the file system feel slower to you and other users. When file size is required, limit the request to the file(s) of interest when practical. Finally, Lustre provides the notion of a \"lazy\" file size that can be useful in circumstances where approximation is appropriate, for example finding the largest or smallest files in a directory tree. See examples below. Similarly, avoid excessive file status calls when possible. When repeatedly checking a file's status in a script \u2013 inside a loop for example \u2013 consider adding a sleep command as a preventive measure. This will prevent flooding the MDS with status requests when your loop executes very quickly. Prefer Lustre-specific lfs find command \u00b6 Lustre's lfs find is an optimized implementation of the familiar find command. It will request only the data required to perform the specified action, and so should be preferred whenever possible. See the examples and use cases below. Examples, tools, tips, tricks \u00b6 Using df and lfs df to query file system status \u00b6 Use the familiar df utility to query overall file system capacity. For example, df -h shows the data size of a file system in a human-readable format: df -h /glade/derecho/scratch Filesystem Size Used Avail Use% Mounted on 10.14.64.3@tcp:10.14.64.4@tcp:/desc1 55P 5.9P 49P 11% /glade/derecho/scratch Use df -ih to get the corresponding metadata information: df -ih /glade/derecho/scratch Filesystem Inodes IUsed IFree IUse% Mounted on 10.14.64.3@tcp:10.14.64.4@tcp:/desc1 16G 105M 16G 1% /glade/derecho/scratch In the example, the file system overall capacity is 1.2 PB, of which 35 TB is used. The file system has 1.2 billion inodes, 19 million of which are used, providing an additional limit on the total number of files and directories that can be stored. Running lfs df provides similar information but at the Lustre-aware component level. For example, lfs df -h shows the data size broken down by MDS and OST components: lfs df -h /glade/derecho/scratch UUID bytes Used Available Use% Mounted on desc1-MDT0000_UUID 11.8T 26.9G 11.6T 1% /glade/derecho/scratch[MDT:0] desc1-MDT0001_UUID 11.8T 29.7G 11.6T 1% /glade/derecho/scratch[MDT:1] desc1-MDT0002_UUID 11.8T 32.7G 11.6T 1% /glade/derecho/scratch[MDT:2] desc1-MDT0003_UUID 11.8T 22.9G 11.6T 1% /glade/derecho/scratch[MDT:3] desc1-OST0000_UUID 581.4T 62.3T 513.2T 11% /glade/derecho/scratch[OST:0] desc1-OST0001_UUID 581.4T 61.7T 513.9T 11% /glade/derecho/scratch[OST:1] desc1-OST0002_UUID 581.4T 62.2T 513.3T 11% /glade/derecho/scratch[OST:2] desc1-OST0003_UUID 581.4T 62.0T 513.5T 11% /glade/derecho/scratch[OST:3] ... desc1-OST005c_UUID 581.4T 61.7T 513.8T 11% /glade/derecho/scratch[OST:92] desc1-OST005d_UUID 581.4T 62.2T 513.3T 11% /glade/derecho/scratch[OST:93] desc1-OST005e_UUID 581.4T 61.8T 513.7T 11% /glade/derecho/scratch[OST:94] desc1-OST005f_UUID 581.4T 62.4T 513.1T 11% /glade/derecho/scratch[OST:95] filesystem_summary: 54.5P 5.8P 48.1P 11% /glade/derecho/scratch This sample file system is composed of four MDTs and 96 OSTs, and lfs df shows the data size of each component. Administrators typically monitor this information to ensure overall file system health, but it can provide useful user diagnostics as well. If one or more of the OSTs is temporarily unavailable due to a storage server issue, for example, lfs df will hang at the affected component, indicating the file system is not healthy. Using lfs dh -ih works similarly, showing the per-component inode usage. Because Lustre file systems typically have a smaller number of MDTs than OSTs, the per-MDT inode usage is an important bound on the overall file system file/directory count capacity. Using lfs find to change directory tree ownership or permissions \u00b6 Tools such as chown , chgrp , and chmod provide a recursive option to allow easy application to all the contents of a given directory. Best practice is to avoid such features and invoke the desired action through lfs find instead. For example, if you want to change the group ownership of an entire directory tree, you might run a command similar to chgrp -R <newgroup> <dirname> . However, you can do it more efficiently \u2013 albeit more verbosely \u2013 with lfs find as follows: lfs find <dirname> -print0 | xargs -0 chgrp <newgroup> To be UNIX-specific about the preceding command, it first asks lfs find to list all the contents of a directory and print them separated with a NULL character ( \\0 ). This list is then sent to the command xargs , which is told to expect a NULL -separated list with the -0 flag. Then xargs will run the command chgrp <newgroup> on batches of files and split what could be a long list of files into small enough chunks to comply with UNIX's maximum command line argument restrictions. See man lfs-find and man xargs for additional details and examples. Using lfs find to tar a directory tree \u00b6 This example shows how to create a tar archive file from a specified directory tree efficiently: lfs find <dirname> -print0 | \\ tar --create --acls --no-recursion \\ --verbose --index-file=my_archive.idx --tape-length=1G \\ --file=my_archive-{0000..9999}.tar --null -T - First, lfs find will list all contents of the directory, NULL separated. Then tar will operate on the list of files and subdirectories. Its behavior is modified by the following flags: --create \u2013 Create a tar archive. --acls \u2013 Include any file/subdirectory access control lists (ACLs) encountered in the output tar files. This option is necessary to preserve ACL information when unpacking the archives later. --no-recursion \u2013 List everything in the directory: files, links, subdirectories, etc. By default tar will recurse into any directory name it encounters, so --no-recursion tells it not to do so, since the contents will be listed anyway. Combined with --acls this allows you to properly set ACLs on directories. --verbose \u2013 Print each file/subdirectory as it is processed. --index-file=my_archive.idx \u2013 Redirect the list created by --verbose into a file named my_archive.idx . --tape-length=1G --file=my_archive-{0000..9999}.tar \u2013 This instructs tar to create a series of files \u2013 my_archive-0000.tar , my_archive-0001.tar , and so on \u2013 in which each file is no larger than 1 GB. --null -T - \u2013 This tells tar that the input file list is NULL -separated and coming in on standard input. The process creates several tar files but does not modify the original source tree directory. One consequence is that storage volume increases during this process until the user removes the directory. An alternative to consider carefully is to also use the --remove-files option. It will remove each source file after it is successfully added to the tar archive, so the overall storage requirements should remain flat. This is just one example of many possibilities with this approach. See man lfs-find and man tar for more ideas. Using lfs find --lazy to efficiently locate old, large files \u00b6 Determining the precise size of a file on a Lustre system is generally an expensive operation in that it requires communication with every object storage server that stores segments of the file. In some cases, knowing the approximate file size may be sufficient, and it can be obtained solely from the metadata server(s). For example, to locate all files in a directory modified seven or more days ago that are approximately 10 MB or larger, run: lfs find <dirname> --lazy --size +10M --mtime +7 -type f -print The --lazy flag requests the approximate file size instead of requiring the precise size and the associated communication overhead. That approach can be useful for quickly locating files to clean up and recover quota space. It could also be combined with xargs and rm to remove the files, similar to the chgrp example. More resources \u00b6 Introduction to Lustre lustre.org Oak Ridge Leadership Computing Facility Lustre 101 resources","title":"Lustre file systems"},{"location":"storage-systems/glade/lustre/#lustre-scratch-file-system","text":"The De recho Stor age subsystsem ( Destor ) scratch file system is a Lustre-based Cray ClusterStor E1000 product configured as shown in the table below. An open-source, parallel system, Lustre will be familiar to users of similar POSIX-compliant file systems. This documentation provides a high-level overview of important Lustre concepts and terminology to help users achieve optimal performance.","title":"Lustre scratch file system"},{"location":"storage-systems/glade/lustre/#capacity-and-components","text":"Total capacity of the system is 40 TB of metadata and 60 PB of data. Component Quantity Details Metadata servers (MDS) 4 Each MDS has two 200 GbE CX6 Cassini network interfaces configured in an active/passive failover pair. Metadata targets (MDT) 4 Each metadata server has a single 12 TB MDT composed of 11 drives in a RAID-10 configuration formatted with ldiskfs (~40 TB usable metadata across the entire file system). Object storage servers (OSS) 24 Each OSS has a single 200 GbE CX6 Cassini network interface. Object storage targets (OST) 96 Each OSS has four 582 TB OSTs. Each OST is composed of 53 drives in a GridRAID configuration formatted with ldiskfs (~60PB usable data across the entire file system). (Additional hardware details are provided below ).","title":"Capacity and components"},{"location":"storage-systems/glade/lustre/#terminology","text":"","title":"Terminology"},{"location":"storage-systems/glade/lustre/#metadata-and-data","text":"The notion of file metadata and data as related but separable entities is important to understanding Lustre because it is fundamental to the system's parallelization strategy. In a POSIX file system, the metadata describes information about a file (name, permissions, access controls, timestamps, and so on), and the data contains the contents of the file itself. Lustre employs one or more metadata servers (MDS) to store the metadata and data layout of each file, and several object storage servers (OSS) to hold the file contents. Each MDS has one or more metadata targets (MDT), which are storage devices attached to the MDS. Similarly, each OSS has one or more object storage target (OST) storage devices. Typically, the MDTs and OSTs are accessible from two different servers, providing fault tolerance and failover capabilities. A typical Lustre file system is shown in Figure 1 below. Figure 1: Sample Lustre file system: 4 metadata servers (MDS), 4 object storage servers (OSS). Credit: Introduction to Lustre Wiki . A file system may employ several metadata servers for scalability and load balancing, and several object storage servers for capacity and performance scalability. When a user creates a file on a Lustre file system, it communicates with an MDS that is responsible for managing the metadata of the file. The MDS also holds the file's striping layout , which is a template used to map file contents (conceptually, data blocks) onto one or more OSTs. This is important to understand because when users are interacting with a Lustre file, they are really interfacing with several storage servers. Different file operations require different server communication requirements. For example, querying a file's modification time is a metadata-only operation and thus requires communication only with the MDS, whereas querying a file's size involves each OSS over which the file is striped.","title":"Metadata and data"},{"location":"storage-systems/glade/lustre/#file-striping","text":"File striping is a key feature of Lustre file systems. A file is said to be striped when its sequence of bytes is separated into small chunks, or stripes , so that read and write operations can involve multiple OSTs concurrently. This process is illustrated in Figures 2 and 3. In Figure 2, the sample file is split into five stripes: the first four are the same size while the fifth is smaller and contains the \"remainder\" of the file. This introduces an important striping concept: the stripe size . Figure 2: Logical view of a file, broken into five \"stripe\" segments. The first four are the same size while the fifth is smaller and contains the \"remainder\" of the file. Credit: Lustre User Guide . Figure 3 shows how the stripes can be mapped onto several OSTs as defined by the stripe count . In this example, the stripe count is four and the stripe segments are assigned in a round-robin fashion. Figure 3: Physical view of a file broken into five stripes across four OST devices. Credit: Lustre User Guide . Striping has important benefits as well as some drawbacks. Striping over more OSTs allows for more bandwidth. In general, as more OSTs are used, more servers are involved, so more network and disk subsystem bandwidth is available. Striping also allows for files larger than any single OST. The primary drawback of striping is overhead: as more OSTs are employed to store a file, more network overhead is required to orchestrate the components. The preceding discussion is focused on striping the blocks of a given file. When multiple MDTs are present in the file system, as is the case with Derecho, metadata striping is also typically employed and the contents of directories are spread across the available MDSs in the system.","title":"File striping"},{"location":"storage-systems/glade/lustre/#progressive-file-layouts","text":"The configurable stripe size and stripe count parameters were at one time the only modifiable parameters available to govern striping behavior, which made it difficult to implement a one-size-fits-all default configuration on large systems with varied use cases. The introduction of progressive file layouts (PFLs) in modern Lustre versions, however, extended the striping concept to multiple, progressive segments of files, as shown in Figure 4. Figure 4: Sample progressive file layout with three components of different stripe patterns. Credit: PFL Prototype High Level Design . In Figure 4, a single file is mapped to three separate components, each with a different striping layout. The first component has a stripe size of 1 MB with a stripe count of 1, and is 2 MB in total extent. This means the first 2 MB of the file will be striped over only one OST, in two 1-MB chunks. The second component begins beyond this 2 MB threshold up to 256 MB size. It employs a stripe size of 1 MB but increases the stripe count to four. Finally, the third and final component begins when the file size exceeds 256 MB. The stripe size increases to 4 MB and the stripe count to 32 OSTs. PFLs are useful because they define a template that is much more general than a single stripe size/count pair. They allow small files to be striped over a small number of OSTs and only incur the overhead of additional OST stripes when the file is sufficiently large to benefit from increased bandwidth.","title":"Progressive file layouts"},{"location":"storage-systems/glade/lustre/#inodes-and-data-blocks","text":"When a file system is constructed, the underlying storage device blocks are segregated into two components: data blocks and inodes . Data blocks are the most familiar; storing a 1 GB file simply requires a sufficient number of data blocks to hold its contents. Inodes, by contrast, are index nodes that hold the metadata associated with a file or directory: ownership, time stamps, striping information, and so on. The number of inodes available in a file system is generally fixed and provides a strict limit on the number of files and directories the file system can hold. This is especially important in a Lustre file system. Its capacity is limited by the size and quantity of OSTs, and its file count capacity is also limited by the number of inodes available on the MDTs. In an extreme example, it is possible to exhaust the available inodes in a file system before its storage capacity by creating many tiny files, so it is important to manage both the overall file system data volume and the file count.","title":"inodes and data blocks"},{"location":"storage-systems/glade/lustre/#derechos-destor-lustre-scratch-file-system","text":"With the background provided above , we can now discuss Derecho's Destor Lustre file system specific storage configuration.","title":"Derecho's Destor Lustre scratch file system"},{"location":"storage-systems/glade/lustre/#configuration","text":"Derecho's Destor Lustre file system hardware configuration is shown schematically below. The system has 4 MDS servers (each with a single MDT), and 24 OSS servers (each with 4 OSTs).","title":"Configuration"},{"location":"storage-systems/glade/lustre/#default-striping","text":"Destor default striping configuration Destor uses progressive file layouts (PFLs) to accommodate a wide variety of file sizes and access patterns. File Segment Stripe Count Stripe Size 0-16MB 1 1MB 16MB-16GB 4 16MB 16GB-64GB 12 16MB 64GB+ 24 16MB This default PFL was created at the base of the file system and is inherited by default for all sub-directories. Users may apply alternate striping patterns for any directory or file they own. (File striping must be set prior to creation and cannot be changed on existing files.) Note that per-directory file striping is inherited by new files and sub-directories created within. The default Destor stripe configuration can be applied if needed with the lfs setstripe command: lfs setstripe \\ -E 16M -c 1 -S 1M \\ -E 16G -c 4 -S 16M \\ -E 64G -c 12 -S 16M \\ -E -1 -c 24 -S 16M See man lfs-setstripe for additional details.","title":"Default Striping"},{"location":"storage-systems/glade/lustre/#performance-expectations","text":"Lustre in general, and on Derecho specifically, is designed for high-speed, parallel access to large files . Key points: Derecho has >2400 compute nodes (each a Lustre client), All scratch metadata traffic, for all users, is served by only 4 MDS servers Tiny files do not effectively use the primary performance potential. Large files can be effectively spread across the storage cluster. 5,088 hard drives spread across 96 OSTs served up by 24 OSSes . Destor can deliver over 300 GB/sec of large file access bandwdth.","title":"Performance expectations"},{"location":"storage-systems/glade/lustre/#jobstats","text":"Lustre provides a mechanism to collate I/O statistics for reporting purposes using the job stats mechanism. On Destor we collect job statistics according to PBS_JOBID . These statistics are then presented in a graphana dashboard , where users can optionally query the statistics for a particular job.","title":"Jobstats"},{"location":"storage-systems/glade/lustre/#best-practices","text":"","title":"Best practices"},{"location":"storage-systems/glade/lustre/#manage-your-file-count-and-file-volume","text":"Users have quotas for both data volume and file count on the Derecho Lustre file system. The gladequota utility is preferred for reporting comprehensive storage usage across all GLADE file spaces, including Lustre scratch spaces. Additionally, the lfs quota command can be used to query Lustre-specific quota information. Many simulation codes produce large quantities of small, diagnostic output files that are useful for diagnosing problems but not often referenced for successful production runs. Consider removing or tarring such files incrementally in your workflow to manage overall file count.","title":"Manage your file count and file volume"},{"location":"storage-systems/glade/lustre/#avoid-unnecessary-metadata-requests","text":"From the background provided above, it is clear not all metadata access requests are equal. Querying a file's timestamps is a cheap operation requiring communication with the appropriate MDT, whereas querying a file's size requires communication with each and every OST holding data stripes. Therefore, it is a best practice to be aware of these performance implications and request only the metadata needed for a given operation. For example, especially when in a large directory with hundreds of files, avoid typing ls -l if a simple ls will do. The former will communicate with every MDS and OSS in the file system in order to determine the current file size, while the latter is simply an MDS communication. Unnecessary communication can make the file system feel slower to you and other users. When file size is required, limit the request to the file(s) of interest when practical. Finally, Lustre provides the notion of a \"lazy\" file size that can be useful in circumstances where approximation is appropriate, for example finding the largest or smallest files in a directory tree. See examples below. Similarly, avoid excessive file status calls when possible. When repeatedly checking a file's status in a script \u2013 inside a loop for example \u2013 consider adding a sleep command as a preventive measure. This will prevent flooding the MDS with status requests when your loop executes very quickly.","title":"Avoid unnecessary metadata requests"},{"location":"storage-systems/glade/lustre/#prefer-lustre-specific-lfs-find-command","text":"Lustre's lfs find is an optimized implementation of the familiar find command. It will request only the data required to perform the specified action, and so should be preferred whenever possible. See the examples and use cases below.","title":"Prefer Lustre-specific lfs find command"},{"location":"storage-systems/glade/lustre/#examples-tools-tips-tricks","text":"","title":"Examples, tools, tips, tricks"},{"location":"storage-systems/glade/lustre/#using-df-and-lfs-df-to-query-file-system-status","text":"Use the familiar df utility to query overall file system capacity. For example, df -h shows the data size of a file system in a human-readable format: df -h /glade/derecho/scratch Filesystem Size Used Avail Use% Mounted on 10.14.64.3@tcp:10.14.64.4@tcp:/desc1 55P 5.9P 49P 11% /glade/derecho/scratch Use df -ih to get the corresponding metadata information: df -ih /glade/derecho/scratch Filesystem Inodes IUsed IFree IUse% Mounted on 10.14.64.3@tcp:10.14.64.4@tcp:/desc1 16G 105M 16G 1% /glade/derecho/scratch In the example, the file system overall capacity is 1.2 PB, of which 35 TB is used. The file system has 1.2 billion inodes, 19 million of which are used, providing an additional limit on the total number of files and directories that can be stored. Running lfs df provides similar information but at the Lustre-aware component level. For example, lfs df -h shows the data size broken down by MDS and OST components: lfs df -h /glade/derecho/scratch UUID bytes Used Available Use% Mounted on desc1-MDT0000_UUID 11.8T 26.9G 11.6T 1% /glade/derecho/scratch[MDT:0] desc1-MDT0001_UUID 11.8T 29.7G 11.6T 1% /glade/derecho/scratch[MDT:1] desc1-MDT0002_UUID 11.8T 32.7G 11.6T 1% /glade/derecho/scratch[MDT:2] desc1-MDT0003_UUID 11.8T 22.9G 11.6T 1% /glade/derecho/scratch[MDT:3] desc1-OST0000_UUID 581.4T 62.3T 513.2T 11% /glade/derecho/scratch[OST:0] desc1-OST0001_UUID 581.4T 61.7T 513.9T 11% /glade/derecho/scratch[OST:1] desc1-OST0002_UUID 581.4T 62.2T 513.3T 11% /glade/derecho/scratch[OST:2] desc1-OST0003_UUID 581.4T 62.0T 513.5T 11% /glade/derecho/scratch[OST:3] ... desc1-OST005c_UUID 581.4T 61.7T 513.8T 11% /glade/derecho/scratch[OST:92] desc1-OST005d_UUID 581.4T 62.2T 513.3T 11% /glade/derecho/scratch[OST:93] desc1-OST005e_UUID 581.4T 61.8T 513.7T 11% /glade/derecho/scratch[OST:94] desc1-OST005f_UUID 581.4T 62.4T 513.1T 11% /glade/derecho/scratch[OST:95] filesystem_summary: 54.5P 5.8P 48.1P 11% /glade/derecho/scratch This sample file system is composed of four MDTs and 96 OSTs, and lfs df shows the data size of each component. Administrators typically monitor this information to ensure overall file system health, but it can provide useful user diagnostics as well. If one or more of the OSTs is temporarily unavailable due to a storage server issue, for example, lfs df will hang at the affected component, indicating the file system is not healthy. Using lfs dh -ih works similarly, showing the per-component inode usage. Because Lustre file systems typically have a smaller number of MDTs than OSTs, the per-MDT inode usage is an important bound on the overall file system file/directory count capacity.","title":"Using df and lfs df to query file system status"},{"location":"storage-systems/glade/lustre/#using-lfs-find-to-change-directory-tree-ownership-or-permissions","text":"Tools such as chown , chgrp , and chmod provide a recursive option to allow easy application to all the contents of a given directory. Best practice is to avoid such features and invoke the desired action through lfs find instead. For example, if you want to change the group ownership of an entire directory tree, you might run a command similar to chgrp -R <newgroup> <dirname> . However, you can do it more efficiently \u2013 albeit more verbosely \u2013 with lfs find as follows: lfs find <dirname> -print0 | xargs -0 chgrp <newgroup> To be UNIX-specific about the preceding command, it first asks lfs find to list all the contents of a directory and print them separated with a NULL character ( \\0 ). This list is then sent to the command xargs , which is told to expect a NULL -separated list with the -0 flag. Then xargs will run the command chgrp <newgroup> on batches of files and split what could be a long list of files into small enough chunks to comply with UNIX's maximum command line argument restrictions. See man lfs-find and man xargs for additional details and examples.","title":"Using lfs find to change directory tree ownership or permissions"},{"location":"storage-systems/glade/lustre/#using-lfs-find-to-tar-a-directory-tree","text":"This example shows how to create a tar archive file from a specified directory tree efficiently: lfs find <dirname> -print0 | \\ tar --create --acls --no-recursion \\ --verbose --index-file=my_archive.idx --tape-length=1G \\ --file=my_archive-{0000..9999}.tar --null -T - First, lfs find will list all contents of the directory, NULL separated. Then tar will operate on the list of files and subdirectories. Its behavior is modified by the following flags: --create \u2013 Create a tar archive. --acls \u2013 Include any file/subdirectory access control lists (ACLs) encountered in the output tar files. This option is necessary to preserve ACL information when unpacking the archives later. --no-recursion \u2013 List everything in the directory: files, links, subdirectories, etc. By default tar will recurse into any directory name it encounters, so --no-recursion tells it not to do so, since the contents will be listed anyway. Combined with --acls this allows you to properly set ACLs on directories. --verbose \u2013 Print each file/subdirectory as it is processed. --index-file=my_archive.idx \u2013 Redirect the list created by --verbose into a file named my_archive.idx . --tape-length=1G --file=my_archive-{0000..9999}.tar \u2013 This instructs tar to create a series of files \u2013 my_archive-0000.tar , my_archive-0001.tar , and so on \u2013 in which each file is no larger than 1 GB. --null -T - \u2013 This tells tar that the input file list is NULL -separated and coming in on standard input. The process creates several tar files but does not modify the original source tree directory. One consequence is that storage volume increases during this process until the user removes the directory. An alternative to consider carefully is to also use the --remove-files option. It will remove each source file after it is successfully added to the tar archive, so the overall storage requirements should remain flat. This is just one example of many possibilities with this approach. See man lfs-find and man tar for more ideas.","title":"Using lfs find to tar a directory tree"},{"location":"storage-systems/glade/lustre/#using-lfs-find-lazy-to-efficiently-locate-old-large-files","text":"Determining the precise size of a file on a Lustre system is generally an expensive operation in that it requires communication with every object storage server that stores segments of the file. In some cases, knowing the approximate file size may be sufficient, and it can be obtained solely from the metadata server(s). For example, to locate all files in a directory modified seven or more days ago that are approximately 10 MB or larger, run: lfs find <dirname> --lazy --size +10M --mtime +7 -type f -print The --lazy flag requests the approximate file size instead of requiring the precise size and the associated communication overhead. That approach can be useful for quickly locating files to clean up and recover quota space. It could also be combined with xargs and rm to remove the files, similar to the chgrp example.","title":"Using lfs find --lazy to efficiently locate old, large files"},{"location":"storage-systems/glade/lustre/#more-resources","text":"Introduction to Lustre lustre.org Oak Ridge Leadership Computing Facility Lustre 101 resources","title":"More resources"},{"location":"storage-systems/glade/recovering-files-from-snapshots/","text":"Recovering files from snapshots \u00b6 CISL creates snapshots of the GLADE home file space several times each day in addition to multiple backups each week. These snapshots are records of the state of the file system at various times. Snapshots enable you to retrieve copies of deleted files yourself, quickly and easily, or recover earlier versions of files that have been revised. (To recover files or directories from backups rather than snapshots, contact the NCAR Research Computing help desk .) The number of snapshots that are available at any one time varies, and the intervals between snapshots may change at any time without prior notice. Retrieving directories and files \u00b6 If you need to retrieve directories or files, first determine if they are available in one or more snapshots by running snapls as shown below, then copy the files to your home space. The files will retain the permissions that existed when the snapshot was created. Find a directory \u00b6 To see if your current directory is present in any snapshots, just run snapls on your command line. You can also specify a directory by executing snapls -ldhtr directory_name . In this example, your current directory is /glade/u/home/username . The output from snapls identifies recent snapshots with date and time stamps in this format: YYYYMMDD-hhmmss . snapls drwxr-xr-x 41 username ncar 16384 Jul 8 11:51 /glade/u/home/.snapshots/20200208-120001/username drwxr-xr-x 41 username ncar 16384 Jul 7 10:49 /glade/u/home/.snapshots/20200207-110001/username drwxr-xr-x 40 username ncar 16384 Jul 7 09:59 /glade/u/home/.snapshots/20200207-100001/username drwxr-xr-x 40 username ncar 16384 Jul 7 13:25 /glade/u/home/.snapshots/20200207-180001/username Change to /username in the most recent snapshot directory that is identified. cd /glade/u/home/.snapshots/20200208-120001/username Copy the files that you need back to your home directory or a subdirectory. cp file1 file2 file3 /glade/u/home/username Find and copy a file \u00b6 You can find an individual file by identifying it as in this example. The output shows that filename is available in two snapshots. snapls -ldhtr filename drwxr-xr-x 40 username ncar 16384 Jul 7 09:59 /glade/u/home/.snapshots/20200207-100001/username drwxr-xr-x 40 username ncar 16384 Jul 7 13:25 /glade/u/home/.snapshots/20200207-180001/username When you identify the file you want, you can copy it back to your current directory as shown here. cp /glade/u/home/.snapshots/20200207-100001/username/filename . Comparing snapshots \u00b6 You can use the diff command to identify changes that were made between snapshots, as in this example. diff /glade/u/home/.snapshots/20200208-100001/username/filename \\ /glade/u/home/.snapshots/20200207-180001/username/filename This can be useful if you need to roll back to an earlier version of a file, but it is not a substitute for following version control best practices. The diff command is best used for comparing single files or small directory trees within snapshots.","title":"Recovering files from snapshots"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#recovering-files-from-snapshots","text":"CISL creates snapshots of the GLADE home file space several times each day in addition to multiple backups each week. These snapshots are records of the state of the file system at various times. Snapshots enable you to retrieve copies of deleted files yourself, quickly and easily, or recover earlier versions of files that have been revised. (To recover files or directories from backups rather than snapshots, contact the NCAR Research Computing help desk .) The number of snapshots that are available at any one time varies, and the intervals between snapshots may change at any time without prior notice.","title":"Recovering files from snapshots"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#retrieving-directories-and-files","text":"If you need to retrieve directories or files, first determine if they are available in one or more snapshots by running snapls as shown below, then copy the files to your home space. The files will retain the permissions that existed when the snapshot was created.","title":"Retrieving directories and files"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#find-a-directory","text":"To see if your current directory is present in any snapshots, just run snapls on your command line. You can also specify a directory by executing snapls -ldhtr directory_name . In this example, your current directory is /glade/u/home/username . The output from snapls identifies recent snapshots with date and time stamps in this format: YYYYMMDD-hhmmss . snapls drwxr-xr-x 41 username ncar 16384 Jul 8 11:51 /glade/u/home/.snapshots/20200208-120001/username drwxr-xr-x 41 username ncar 16384 Jul 7 10:49 /glade/u/home/.snapshots/20200207-110001/username drwxr-xr-x 40 username ncar 16384 Jul 7 09:59 /glade/u/home/.snapshots/20200207-100001/username drwxr-xr-x 40 username ncar 16384 Jul 7 13:25 /glade/u/home/.snapshots/20200207-180001/username Change to /username in the most recent snapshot directory that is identified. cd /glade/u/home/.snapshots/20200208-120001/username Copy the files that you need back to your home directory or a subdirectory. cp file1 file2 file3 /glade/u/home/username","title":"Find a directory"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#find-and-copy-a-file","text":"You can find an individual file by identifying it as in this example. The output shows that filename is available in two snapshots. snapls -ldhtr filename drwxr-xr-x 40 username ncar 16384 Jul 7 09:59 /glade/u/home/.snapshots/20200207-100001/username drwxr-xr-x 40 username ncar 16384 Jul 7 13:25 /glade/u/home/.snapshots/20200207-180001/username When you identify the file you want, you can copy it back to your current directory as shown here. cp /glade/u/home/.snapshots/20200207-100001/username/filename .","title":"Find and copy a file"},{"location":"storage-systems/glade/recovering-files-from-snapshots/#comparing-snapshots","text":"You can use the diff command to identify changes that were made between snapshots, as in this example. diff /glade/u/home/.snapshots/20200208-100001/username/filename \\ /glade/u/home/.snapshots/20200207-180001/username/filename This can be useful if you need to roll back to an earlier version of a file, but it is not a substitute for following version control best practices. The diff command is best used for comparing single files or small directory trees within snapshots.","title":"Comparing snapshots"},{"location":"storage-systems/glade/removing-large-number-of-files/","text":"Removing large numbers of files \u00b6 The recommended way to remove thousands or hundreds of thousands of files from a GLADE directory is by running a batch job on Casper. Removing large numbers of files can take several hours, so you will need to provide enough wall-clock time in the job to accommodate this. You can use the sample script below with your own project code, job name, and other customizations. Test your file removal process first!! Before removing large numbers of files, create a \"play\" directory inside your /glade/derecho/scratch user space and try the batch job with some fictional files and subdirectories to make sure that it does what you want. Carefully specify the files that you want removed before submitting a job like this. Create job script and submit \u00b6 Create a job script following the example just below. To submit the job when your script is ready, run the PBS Pro qsub command followed by the name of your script file, as in this example: qsub remove_files.pbs Example scripts \u00b6 Removing files or complete subdirectories Removing Files Removing a Directory Tree remove_files.pbs #!/bin/bash #PBS -N remove_files_job #PBS -A <project_code> #PBS -l select=1:ncpus=1 #PBS -l walltime=24:00:00 #PBS -j oe #PBS -q casper export TMPDIR = ${ SCRATCH } /temp && mkdir -p ${ TMPDIR } # the subdirectory and file pattern for removal subdir = \"/glade/derecho/scratch/ ${ USER } /directory_name\" file_pattern = \"files_to_remove*\" # testing mode (set to False to actually remove files) test_only = True [[ True == ${ test_only } ]] && do_echo = \"echo\" || do_echo = \"\" cd ${ subdir } || { echo \"cannot cd ${ subdir } , aborting!!\" ; exit 1 ; } # use the \"lfs find\" command to locate files in the requested # subdirectory matching the specified file pattern. # We then send this list, NULL separated, to the xargs command # to do the removal. lfs find ${ subdir } -type f -name \" ${ file_pattern } \" -print0 | \\ xargs -0 ${ do_echo } rm -f remove_subdir.pbs #!/bin/bash #PBS -N remove_files_job #PBS -A <project_code> #PBS -l select=1:ncpus=1 #PBS -l walltime=24:00:00 #PBS -j oe #PBS -q casper export TMPDIR = ${ SCRATCH } /temp && mkdir -p ${ TMPDIR } # the subdirectory and file pattern for removal subdir = \"/glade/derecho/scratch/ ${ USER } /directory_name\" # testing mode (set to False to actually remove files) test_only = True [[ True == ${ test_only } ]] && do_echo = \"echo\" || do_echo = \"\" cd ${ subdir } || { echo \"cannot cd ${ subdir } , aborting!!\" ; exit 1 ; } # (1) use the \"lfs find\" command to locate files in the requested # subdirectory. We then send this list, NULL separated, # to the xargs command to do the removal. lfs find . ! -type d -print0 | \\ xargs -0 ${ do_echo } rm -f # (2) remove the entire subdirectory tree cd - ${ do_echo } rm -rf ${ subdir }","title":"Removing large number of files"},{"location":"storage-systems/glade/removing-large-number-of-files/#removing-large-numbers-of-files","text":"The recommended way to remove thousands or hundreds of thousands of files from a GLADE directory is by running a batch job on Casper. Removing large numbers of files can take several hours, so you will need to provide enough wall-clock time in the job to accommodate this. You can use the sample script below with your own project code, job name, and other customizations. Test your file removal process first!! Before removing large numbers of files, create a \"play\" directory inside your /glade/derecho/scratch user space and try the batch job with some fictional files and subdirectories to make sure that it does what you want. Carefully specify the files that you want removed before submitting a job like this.","title":"Removing large numbers of files"},{"location":"storage-systems/glade/removing-large-number-of-files/#create-job-script-and-submit","text":"Create a job script following the example just below. To submit the job when your script is ready, run the PBS Pro qsub command followed by the name of your script file, as in this example: qsub remove_files.pbs","title":"Create job script and submit"},{"location":"storage-systems/glade/removing-large-number-of-files/#example-scripts","text":"Removing files or complete subdirectories Removing Files Removing a Directory Tree remove_files.pbs #!/bin/bash #PBS -N remove_files_job #PBS -A <project_code> #PBS -l select=1:ncpus=1 #PBS -l walltime=24:00:00 #PBS -j oe #PBS -q casper export TMPDIR = ${ SCRATCH } /temp && mkdir -p ${ TMPDIR } # the subdirectory and file pattern for removal subdir = \"/glade/derecho/scratch/ ${ USER } /directory_name\" file_pattern = \"files_to_remove*\" # testing mode (set to False to actually remove files) test_only = True [[ True == ${ test_only } ]] && do_echo = \"echo\" || do_echo = \"\" cd ${ subdir } || { echo \"cannot cd ${ subdir } , aborting!!\" ; exit 1 ; } # use the \"lfs find\" command to locate files in the requested # subdirectory matching the specified file pattern. # We then send this list, NULL separated, to the xargs command # to do the removal. lfs find ${ subdir } -type f -name \" ${ file_pattern } \" -print0 | \\ xargs -0 ${ do_echo } rm -f remove_subdir.pbs #!/bin/bash #PBS -N remove_files_job #PBS -A <project_code> #PBS -l select=1:ncpus=1 #PBS -l walltime=24:00:00 #PBS -j oe #PBS -q casper export TMPDIR = ${ SCRATCH } /temp && mkdir -p ${ TMPDIR } # the subdirectory and file pattern for removal subdir = \"/glade/derecho/scratch/ ${ USER } /directory_name\" # testing mode (set to False to actually remove files) test_only = True [[ True == ${ test_only } ]] && do_echo = \"echo\" || do_echo = \"\" cd ${ subdir } || { echo \"cannot cd ${ subdir } , aborting!!\" ; exit 1 ; } # (1) use the \"lfs find\" command to locate files in the requested # subdirectory. We then send this list, NULL separated, # to the xargs command to do the removal. lfs find . ! -type d -print0 | \\ xargs -0 ${ do_echo } rm -f # (2) remove the entire subdirectory tree cd - ${ do_echo } rm -rf ${ subdir }","title":"Example scripts"},{"location":"storage-systems/glade/setting-file-directory-permissions/","text":"Setting file and directory permissions \u00b6 This information is intended to help GLADE file system users understand common POSIX-standard commands. Note that: Experienced users may prefer to manage permissions using octal numbers rather than the methods described below. Some also find access control lists (ACLs) useful for facilitating short-term file sharing among selected users. See Using access control lists . Existing files and directories \u00b6 Should you need to change permissions for existing files or directories \u2013 to allow other users to modify or execute them, for example \u2013 follow the chmod examples below . New files and directories \u00b6 Files and directories that you create in your GLADE file spaces have certain permissions by default. To change the default settings, use the umask command described below. Don\u2019t run sudo on NCAR systems If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators. About permissions \u00b6 Files in a UNIX system have associated permissions that determine who can read ( r ), write ( w ), and execute ( x ) them. Directory permissions use those same flags to indicate who can list files in a directory ( r ), create and remove files in the directory ( w ), or cd into or traverse ( x ) the directory. Carefully consider both the file permissions and the directory permissions to get the desired end result. For example, you can give a user read permission for a file, but the user won't have access to it without also having permission to traverse the directory tree that contains the file. Three additional things to note regarding directory permissions: Users who have write permission for a directory can delete files in the directory without having write permission for those files. Subdirectories can have less restrictive permissions than their parent directories. However, if you change directory permissions recursively (see chmod below), you are changing them for all of the files and subdirectories in that directory tree. An alternative to changing permissions recursively is to set them selectively as shown in this example below . About execute flags: X vs. x When setting permissions, the execute flag can be set to upper-case X , which differs from the lower-case x setting. The X permission allows execution only if the target is a directory or if the execute permission has already been set for the user or group. It is useful in the case of handling directory trees recursively. To see who can work with your files and directories, log in and look at the output of an ls \u2011l command. Here is an example. The first column is a string of 10 permission flags. The first flag indicates, for most directory contents, that what is listed is a file (-) or a directory (d). The other nine flags, in groups of three, indicate: Permission Field Representation the user\u2019s (owner\u2019s) permissions - rwx r-xr-x group members\u2019 permissions -rwx r-x r-x others\u2019 permissions -rwxr-x r-x \"Others\" means everyone else who can log in on the machine. Changing permissions with chmod \u00b6 To modify the permission flags on existing files and directories, use the chmod command (\"change mode\"). It can be used for individual files or it can be run recursively with the -R option to change permissions for all of the subdirectories and files within a directory. The chmod command specifies which class or classes ( u : user, g : group, o : other) have access to the file or directory in various modes ( r : read, w : write, x : execute). Use the operators + and - to add or remove selected permissions for a class without changing its other permissions. Use = to specify all of the permissions for a class at once. If a class is not mentioned explicitly, the permissions are unchanged even if the = operator is used for a different class. Follow this format: chmod [classes][operator][modes] filename Examples \u00b6 Add selected permissions for a group \u00b6 Only the owner can read, write, and execute this file: -rwx------ 1 username group 57 Apr 11 12:29 filename Add group ( g ) permissions to read ( r ) and execute ( x ) like this: chmod g+rx filename The new file permissions are shown here: -rwxr-x--- 1 username group 57 May 14 09:54 filename Note that the permissions that were not specified were not changed: The user class permissions and other class permissions did not change, and the writing permission for the group class remains unchanged. Specify all permissions for a group \u00b6 To set permissions for a single class, such as group ( g ), use the = operator. chmod g=rx filename In this case, the only permissions affected were those for the specified class: group. The group can only read or execute the file, but not write. Permissions for the user class and other class were not changed because they were not specified. Specify permissions for sets of classes \u00b6 To set permissions for multiple classes with a single command, separate the class settings with a comma. chmod u=rwx,g=rwx,o+rx filename The new file permissions are shown here: -rwxrwxr-x 1 username group 57 May 14 09:54 filename Set permissions selectively \u00b6 This example shows how to give your group access to all of the files and subdirectories in a directory but limit other users' access to specified files. chmod -R u=rwx,g=rwx,o+x /glade/u/home/username/directory/ chmod u=rwx,g=rwx,o+rx /glade/u/home/username/directory/subdirectory/file1 chmod u=rwx,g=rwx,o+rx /glade/u/home/username/directory/subdirectory/file2 The result is that group members have all rights to files in the specified directories and subdirectories. Others have permission to traverse the directories as needed to read and execute two specified files. Changing default permissions with umask \u00b6 To change the default permissions that are set when you create a file or directory within a session or with a script, use the umask command. The syntax is similar to that of chmod (above), but use the = operator to set the default permissions. Examples \u00b6 The umask examples shown here will give you and group members read, write, and execute permission. Others will have only read and execute permission. bash users \u00b6 umask u=rwx,g=rwx,o=rx tcsh users \u00b6 umask 002 Managing groups \u00b6 Several additional commands are useful for managing groups to control who can access files and directories. For example, you can limit access to users who share your core-hour or storage space allocation. Say you don't want all members of the ncar group to have group permissions to read, write, and execute certain files. You should have a UNIX group that corresponds to your project code \u2013 such as group uabc0001 for project code UABC0001 . You can use the commands described below to set or change group ownership of certain files and directories so only members of that UNIX group have permission to access them. If there is no group that allows you to share as needed with other users who have NCAR user accounts: Consider using the setfacl command to set up an access control list. Request creation of a custom group. To share with colleagues who do not have NCAR user accounts , see Sharing data and making unattended transfers . Identifying current group and others - id \u00b6 Files or directories that you create or edit become associated with your current UNIX group. Usually, that is your default primary group unless you change groups after you log in. (See \"Changing current group\" below.) If you aren't sure what your current group is, or which other groups you belong to, you can find out by running the id command after you log in. It will return your user ID ( uid ) and your current group ( gid ), and it will list any other groups with which you are associated. Example: id uid=12345(jsmith) gid=1000(ncar) groups=1000(ncar),54321(cisl) Changing current group - sg \u00b6 To change from one group to another during a login session , follow this example using the sg command and the name of the new group. sg new_groupname The command will start a new shell with your new current group ID in effect. When you exit that shell, you change back to your previously used group ID. Some users prefer newgrp over sg for this, but sg has the advantage of retaining your existing user environment while changing your current group. Changing default group \u00b6 To change your default primary group \u2013 the group that will be in effect each time you subsequently log in \u2013 use the Systems Accounting Manager ( SAM ). Changes made in SAM typically take effect the next business day. Changing group ownership of a file or directory - chgrp \u00b6 Use chgrp as shown here to change ownership of a file or directory to a different group. chgrp new_groupname filename chgrp new_groupname directory To change group ownership of a directory and all of the files and subdirectories in that directory, use chgrp recursively. chgrp -R new_group directory","title":"Setting File and Directory Permissions"},{"location":"storage-systems/glade/setting-file-directory-permissions/#setting-file-and-directory-permissions","text":"This information is intended to help GLADE file system users understand common POSIX-standard commands. Note that: Experienced users may prefer to manage permissions using octal numbers rather than the methods described below. Some also find access control lists (ACLs) useful for facilitating short-term file sharing among selected users. See Using access control lists .","title":"Setting file and directory permissions"},{"location":"storage-systems/glade/setting-file-directory-permissions/#existing-files-and-directories","text":"Should you need to change permissions for existing files or directories \u2013 to allow other users to modify or execute them, for example \u2013 follow the chmod examples below .","title":"Existing files and directories"},{"location":"storage-systems/glade/setting-file-directory-permissions/#new-files-and-directories","text":"Files and directories that you create in your GLADE file spaces have certain permissions by default. To change the default settings, use the umask command described below. Don\u2019t run sudo on NCAR systems If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.","title":"New files and directories"},{"location":"storage-systems/glade/setting-file-directory-permissions/#about-permissions","text":"Files in a UNIX system have associated permissions that determine who can read ( r ), write ( w ), and execute ( x ) them. Directory permissions use those same flags to indicate who can list files in a directory ( r ), create and remove files in the directory ( w ), or cd into or traverse ( x ) the directory. Carefully consider both the file permissions and the directory permissions to get the desired end result. For example, you can give a user read permission for a file, but the user won't have access to it without also having permission to traverse the directory tree that contains the file. Three additional things to note regarding directory permissions: Users who have write permission for a directory can delete files in the directory without having write permission for those files. Subdirectories can have less restrictive permissions than their parent directories. However, if you change directory permissions recursively (see chmod below), you are changing them for all of the files and subdirectories in that directory tree. An alternative to changing permissions recursively is to set them selectively as shown in this example below . About execute flags: X vs. x When setting permissions, the execute flag can be set to upper-case X , which differs from the lower-case x setting. The X permission allows execution only if the target is a directory or if the execute permission has already been set for the user or group. It is useful in the case of handling directory trees recursively. To see who can work with your files and directories, log in and look at the output of an ls \u2011l command. Here is an example. The first column is a string of 10 permission flags. The first flag indicates, for most directory contents, that what is listed is a file (-) or a directory (d). The other nine flags, in groups of three, indicate: Permission Field Representation the user\u2019s (owner\u2019s) permissions - rwx r-xr-x group members\u2019 permissions -rwx r-x r-x others\u2019 permissions -rwxr-x r-x \"Others\" means everyone else who can log in on the machine.","title":"About permissions"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-permissions-with-chmod","text":"To modify the permission flags on existing files and directories, use the chmod command (\"change mode\"). It can be used for individual files or it can be run recursively with the -R option to change permissions for all of the subdirectories and files within a directory. The chmod command specifies which class or classes ( u : user, g : group, o : other) have access to the file or directory in various modes ( r : read, w : write, x : execute). Use the operators + and - to add or remove selected permissions for a class without changing its other permissions. Use = to specify all of the permissions for a class at once. If a class is not mentioned explicitly, the permissions are unchanged even if the = operator is used for a different class. Follow this format: chmod [classes][operator][modes] filename","title":"Changing permissions with chmod"},{"location":"storage-systems/glade/setting-file-directory-permissions/#examples","text":"","title":"Examples"},{"location":"storage-systems/glade/setting-file-directory-permissions/#add-selected-permissions-for-a-group","text":"Only the owner can read, write, and execute this file: -rwx------ 1 username group 57 Apr 11 12:29 filename Add group ( g ) permissions to read ( r ) and execute ( x ) like this: chmod g+rx filename The new file permissions are shown here: -rwxr-x--- 1 username group 57 May 14 09:54 filename Note that the permissions that were not specified were not changed: The user class permissions and other class permissions did not change, and the writing permission for the group class remains unchanged.","title":"Add selected permissions for a group"},{"location":"storage-systems/glade/setting-file-directory-permissions/#specify-all-permissions-for-a-group","text":"To set permissions for a single class, such as group ( g ), use the = operator. chmod g=rx filename In this case, the only permissions affected were those for the specified class: group. The group can only read or execute the file, but not write. Permissions for the user class and other class were not changed because they were not specified.","title":"Specify all permissions for a group"},{"location":"storage-systems/glade/setting-file-directory-permissions/#specify-permissions-for-sets-of-classes","text":"To set permissions for multiple classes with a single command, separate the class settings with a comma. chmod u=rwx,g=rwx,o+rx filename The new file permissions are shown here: -rwxrwxr-x 1 username group 57 May 14 09:54 filename","title":"Specify permissions for sets of classes"},{"location":"storage-systems/glade/setting-file-directory-permissions/#set-permissions-selectively","text":"This example shows how to give your group access to all of the files and subdirectories in a directory but limit other users' access to specified files. chmod -R u=rwx,g=rwx,o+x /glade/u/home/username/directory/ chmod u=rwx,g=rwx,o+rx /glade/u/home/username/directory/subdirectory/file1 chmod u=rwx,g=rwx,o+rx /glade/u/home/username/directory/subdirectory/file2 The result is that group members have all rights to files in the specified directories and subdirectories. Others have permission to traverse the directories as needed to read and execute two specified files.","title":"Set permissions selectively"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-default-permissions-with-umask","text":"To change the default permissions that are set when you create a file or directory within a session or with a script, use the umask command. The syntax is similar to that of chmod (above), but use the = operator to set the default permissions.","title":"Changing default permissions with umask"},{"location":"storage-systems/glade/setting-file-directory-permissions/#examples_1","text":"The umask examples shown here will give you and group members read, write, and execute permission. Others will have only read and execute permission.","title":"Examples"},{"location":"storage-systems/glade/setting-file-directory-permissions/#bash-users","text":"umask u=rwx,g=rwx,o=rx","title":"bash users"},{"location":"storage-systems/glade/setting-file-directory-permissions/#tcsh-users","text":"umask 002","title":"tcsh users"},{"location":"storage-systems/glade/setting-file-directory-permissions/#managing-groups","text":"Several additional commands are useful for managing groups to control who can access files and directories. For example, you can limit access to users who share your core-hour or storage space allocation. Say you don't want all members of the ncar group to have group permissions to read, write, and execute certain files. You should have a UNIX group that corresponds to your project code \u2013 such as group uabc0001 for project code UABC0001 . You can use the commands described below to set or change group ownership of certain files and directories so only members of that UNIX group have permission to access them. If there is no group that allows you to share as needed with other users who have NCAR user accounts: Consider using the setfacl command to set up an access control list. Request creation of a custom group. To share with colleagues who do not have NCAR user accounts , see Sharing data and making unattended transfers .","title":"Managing groups"},{"location":"storage-systems/glade/setting-file-directory-permissions/#identifying-current-group-and-others-id","text":"Files or directories that you create or edit become associated with your current UNIX group. Usually, that is your default primary group unless you change groups after you log in. (See \"Changing current group\" below.) If you aren't sure what your current group is, or which other groups you belong to, you can find out by running the id command after you log in. It will return your user ID ( uid ) and your current group ( gid ), and it will list any other groups with which you are associated. Example: id uid=12345(jsmith) gid=1000(ncar) groups=1000(ncar),54321(cisl)","title":"Identifying current group and others - id"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-current-group-sg","text":"To change from one group to another during a login session , follow this example using the sg command and the name of the new group. sg new_groupname The command will start a new shell with your new current group ID in effect. When you exit that shell, you change back to your previously used group ID. Some users prefer newgrp over sg for this, but sg has the advantage of retaining your existing user environment while changing your current group.","title":"Changing current group - sg"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-default-group","text":"To change your default primary group \u2013 the group that will be in effect each time you subsequently log in \u2013 use the Systems Accounting Manager ( SAM ). Changes made in SAM typically take effect the next business day.","title":"Changing default group"},{"location":"storage-systems/glade/setting-file-directory-permissions/#changing-group-ownership-of-a-file-or-directory-chgrp","text":"Use chgrp as shown here to change ownership of a file or directory to a different group. chgrp new_groupname filename chgrp new_groupname directory To change group ownership of a directory and all of the files and subdirectories in that directory, use chgrp recursively. chgrp -R new_group directory","title":"Changing group ownership of a file or directory - chgrp"},{"location":"storage-systems/glade/using-access-control-lists/","text":"Using access control lists \u00b6 Access control lists (ACLs) are tools for managing permissions within a file system by giving users and groups read, write, and/or execute permissions on files or directories outside of the traditional UNIX permissions. The UNIX permissions for managing files on GLADE remain in effect, but ACLs can be used to facilitate short-term file sharing among users who cannot be put in the same UNIX group. In the Cheyenne/GLADE environment, the most common use cases are: Sharing files among users in different NCAR labs or universities. Sharing files with short-term visitors, interns, students, or others during a short project period. Following are examples of how to create an ACL that allows other individuals and groups to work with your files, and how to propagate permissions to new files and directories. To create and manage ACLs on the Campaign Storage file system, log in to Casper or the data-access nodes rather than Cheyenne. Create and view an access control list \u00b6 Use the setfacl command with the --modify option to give an individual user access to a file or directory that you own. In this example, user shiquan is sharing a file with user bjsmith. Listing the file before and after creating the ACL shows how the file permissions changed. ls -l testfile.txt -rw------- 1 shiquan ncar 18 Mar 4 09 :00 testfile.txt setfacl --modify user:bjsmith:rwx testfile.txt ls -l testfile.txt -rw-rwx---+ 1 shiquan ncar 18 Mar 4 09 :00 testfile.txt The + as the last character in the permissions string indicates that an ACL exists for the directory. About execute flags: X vs. x \u00b6 When setting permissions, the execute flag can be set to upper-case X , which differs from the lower-case x setting. The X permission allows execution only if the target is a directory or if the execute permission has already been set for the user or group. It is useful in the case of handling directory trees recursively. Run getfacl as shown here to view any ACLs applied to a file or directory. getfacl testfile.txt The output will look something like this, illustrating in this case that one user's permissions ( rwx ) are different from other users' permissions ( rw- ). # file: testfile.txt # owner: shiquan # group: ncar user::rw- user:bjsmith:rwx group::--- mask::rwx other::--- Give access to a group \u00b6 To give an existing group access to a file or directory, use setfacl as shown here. In this example, the name of the group being given read/write/execute permissions is csg . setfacl --modify group:csg:rwx testfile.txt Run getfacl to see the resulting ACL. getfacl testfile.txt # file: testfile.txt # owner: shiquan # group: ncar user::rw- user:bjsmith:rwx group::--- group:csg:rwx mask::rwx other::--- Use default ACLs to propagate permissions \u00b6 ACLs can be set to propagate permissions to files and subdirectories as they are created. This is done using default ACLs. In this example, a new directory is accessible only to the user who created it. drwx------ 2 shiquan ncar 4096 Mar 6 10:12 my_shared_directory/ To enable another user to read and navigate into the directory, follow this example. setfacl --modify user:bjsmith:r-x my_shared_directory To set the directory's default ACL so that any new files or subdirectories automatically have those same permissions, use the setfacl command as shown here. setfacl --modify default:user:bjsmith:r-x my_shared_directory ls -ld my_shared_directory drwxr-x---+ 2 shiquan ncar 4096 Mar 6 10:12 my_shared_directory The + as the last character in the permissions string indicates that an ACL exists for the directory. Run getfacl to see the resulting ACL and the default ACLs. getfacl my_shared_directory # file: my_shared_directory # owner: shiquan # group: ncar user::rwx user:bjsmith:r-x group::--- mask::r-x other::--- default:user::rwx default:user:bjsmith:r-x default:group::--- default:mask::r-x default:other::--- Default permissions for a specified user \u00b6 ACLs for a specified user or group are independent of default ACLs. The next example illustrates how to modify a default ACL to set default permissions for a specified user. setfacl --default --modify user:bjsmith:rwx my_shared_directory The directory now has a default ACL that will make any new file in the directory accessible to the designated user with the specified permissions (rwx). Here is an example of a new file created in that directory. -rw-rw----+ 1 shiquan ncar 23 Mar 6 10:19 my_shared_directory/newfile Most users in the group get rw- permission. However, as a result of the default ACL behavior established above, user bjsmith 's permissions are different ( rwx ). The following getfacl output with the comment #effective:rw- shows the difference. getfacl my_shared_directory/newfile # file: my_shared_directory/newfile # owner: shiquan # group: ncar user::rw- user:bjsmith:rwx #effective:rw- group::--- mask::rw- other::--- Remove an access control list \u00b6 To remove all ACLs from a file, run setfacl --remove-all followed by the filename. setfacl --remove-all testfile.txt To remove selected permissions previously set for a user, run setfacl -x as shown here. setfacl -x user:bjsmith testfile.txt Advanced use of ACLs \u00b6 Users often have different default umask settings that can conflict with a file or directory ACL. The following example sets an ACL for a directory and all of its files and subdirectories, and it also sets the default ACL for any future files and subdirectories, ensuring the directory is protected from unknown umask settings. setfacl -R -m o:r-x -dm o:r-x /glade/scratch/bjsmith Note that there are two clauses in this example: The first ( -R -m o:r-x ) recursively sets permissions for \"others\" to read and execute ( r-x ) on all existing files and subdirectories under /glade/scratch/bjsmith . The second ( -dm o:r-x ) sets the default ACL for \"others\" to read and execute ( r-x ) on all future files and subdirectories. More information \u00b6 See the manual pages for the commands for more information. man setfacl man getfacl","title":"Using Access Control Lists"},{"location":"storage-systems/glade/using-access-control-lists/#using-access-control-lists","text":"Access control lists (ACLs) are tools for managing permissions within a file system by giving users and groups read, write, and/or execute permissions on files or directories outside of the traditional UNIX permissions. The UNIX permissions for managing files on GLADE remain in effect, but ACLs can be used to facilitate short-term file sharing among users who cannot be put in the same UNIX group. In the Cheyenne/GLADE environment, the most common use cases are: Sharing files among users in different NCAR labs or universities. Sharing files with short-term visitors, interns, students, or others during a short project period. Following are examples of how to create an ACL that allows other individuals and groups to work with your files, and how to propagate permissions to new files and directories. To create and manage ACLs on the Campaign Storage file system, log in to Casper or the data-access nodes rather than Cheyenne.","title":"Using access control lists"},{"location":"storage-systems/glade/using-access-control-lists/#create-and-view-an-access-control-list","text":"Use the setfacl command with the --modify option to give an individual user access to a file or directory that you own. In this example, user shiquan is sharing a file with user bjsmith. Listing the file before and after creating the ACL shows how the file permissions changed. ls -l testfile.txt -rw------- 1 shiquan ncar 18 Mar 4 09 :00 testfile.txt setfacl --modify user:bjsmith:rwx testfile.txt ls -l testfile.txt -rw-rwx---+ 1 shiquan ncar 18 Mar 4 09 :00 testfile.txt The + as the last character in the permissions string indicates that an ACL exists for the directory.","title":"Create and view an access control list"},{"location":"storage-systems/glade/using-access-control-lists/#about-execute-flags-x-vs-x","text":"When setting permissions, the execute flag can be set to upper-case X , which differs from the lower-case x setting. The X permission allows execution only if the target is a directory or if the execute permission has already been set for the user or group. It is useful in the case of handling directory trees recursively. Run getfacl as shown here to view any ACLs applied to a file or directory. getfacl testfile.txt The output will look something like this, illustrating in this case that one user's permissions ( rwx ) are different from other users' permissions ( rw- ). # file: testfile.txt # owner: shiquan # group: ncar user::rw- user:bjsmith:rwx group::--- mask::rwx other::---","title":"About execute flags: X vs. x"},{"location":"storage-systems/glade/using-access-control-lists/#give-access-to-a-group","text":"To give an existing group access to a file or directory, use setfacl as shown here. In this example, the name of the group being given read/write/execute permissions is csg . setfacl --modify group:csg:rwx testfile.txt Run getfacl to see the resulting ACL. getfacl testfile.txt # file: testfile.txt # owner: shiquan # group: ncar user::rw- user:bjsmith:rwx group::--- group:csg:rwx mask::rwx other::---","title":"Give access to a group"},{"location":"storage-systems/glade/using-access-control-lists/#use-default-acls-to-propagate-permissions","text":"ACLs can be set to propagate permissions to files and subdirectories as they are created. This is done using default ACLs. In this example, a new directory is accessible only to the user who created it. drwx------ 2 shiquan ncar 4096 Mar 6 10:12 my_shared_directory/ To enable another user to read and navigate into the directory, follow this example. setfacl --modify user:bjsmith:r-x my_shared_directory To set the directory's default ACL so that any new files or subdirectories automatically have those same permissions, use the setfacl command as shown here. setfacl --modify default:user:bjsmith:r-x my_shared_directory ls -ld my_shared_directory drwxr-x---+ 2 shiquan ncar 4096 Mar 6 10:12 my_shared_directory The + as the last character in the permissions string indicates that an ACL exists for the directory. Run getfacl to see the resulting ACL and the default ACLs. getfacl my_shared_directory # file: my_shared_directory # owner: shiquan # group: ncar user::rwx user:bjsmith:r-x group::--- mask::r-x other::--- default:user::rwx default:user:bjsmith:r-x default:group::--- default:mask::r-x default:other::---","title":"Use default ACLs to propagate permissions"},{"location":"storage-systems/glade/using-access-control-lists/#default-permissions-for-a-specified-user","text":"ACLs for a specified user or group are independent of default ACLs. The next example illustrates how to modify a default ACL to set default permissions for a specified user. setfacl --default --modify user:bjsmith:rwx my_shared_directory The directory now has a default ACL that will make any new file in the directory accessible to the designated user with the specified permissions (rwx). Here is an example of a new file created in that directory. -rw-rw----+ 1 shiquan ncar 23 Mar 6 10:19 my_shared_directory/newfile Most users in the group get rw- permission. However, as a result of the default ACL behavior established above, user bjsmith 's permissions are different ( rwx ). The following getfacl output with the comment #effective:rw- shows the difference. getfacl my_shared_directory/newfile # file: my_shared_directory/newfile # owner: shiquan # group: ncar user::rw- user:bjsmith:rwx #effective:rw- group::--- mask::rw- other::---","title":"Default permissions for a specified user"},{"location":"storage-systems/glade/using-access-control-lists/#remove-an-access-control-list","text":"To remove all ACLs from a file, run setfacl --remove-all followed by the filename. setfacl --remove-all testfile.txt To remove selected permissions previously set for a user, run setfacl -x as shown here. setfacl -x user:bjsmith testfile.txt","title":"Remove an access control list"},{"location":"storage-systems/glade/using-access-control-lists/#advanced-use-of-acls","text":"Users often have different default umask settings that can conflict with a file or directory ACL. The following example sets an ACL for a directory and all of its files and subdirectories, and it also sets the default ACL for any future files and subdirectories, ensuring the directory is protected from unknown umask settings. setfacl -R -m o:r-x -dm o:r-x /glade/scratch/bjsmith Note that there are two clauses in this example: The first ( -R -m o:r-x ) recursively sets permissions for \"others\" to read and execute ( r-x ) on all existing files and subdirectories under /glade/scratch/bjsmith . The second ( -dm o:r-x ) sets the default ACL for \"others\" to read and execute ( r-x ) on all future files and subdirectories.","title":"Advanced use of ACLs"},{"location":"storage-systems/glade/using-access-control-lists/#more-information","text":"See the manual pages for the commands for more information. man setfacl man getfacl","title":"More information"},{"location":"storage-systems/quasar/","text":"Quasar archive for data collections \u00b6 The Quasar archive is a cold, tape-based archive for storing curated data collections that have an indefinite lifetime. It is not designed to serve data or to store data that will be frequently accessed, overwritten, or deleted. (Active data should be on GLADE or Campaign Storage rather than on Quasar.) Before requesting access, please review the following information regarding how to archive files, the minimum and maximum file sizes, and related use policies. Storing data \u00b6 Users store data on Quasar by transferring files via the Globus mapped collection named NCAR Quasar . For documentation about how to use Globus, see Globus file transfers . A note about verifying Globus transfers Using the Globus checksum sync option when transferring files can result in \"operation timed out\" error messages when it causes file recalls from tape, which can be slow. To avoid such errors when doing an incremental backup, use a different sync level \u2013 exists , mtime or size , for example \u2013 when making the transfer. To verify that a just-completed transfer did not encounter any corruption, do the checksum immediately to complete it before files are transferred to tape and purged from the disk cache. File size requirements \u00b6 The following requirements apply to files stored in a project's high-level allocation (RDA or EOL, for example). The maximum file size is 5 TB. The target file size for Quasar is 1 GB or larger, so use a tool like tar to combine multiple smaller files into a larger file or files before storing them. At least 90% of a project's files must be at least 100 MB. Up to 10% of a project's files may be smaller than 100 MB. The length of a file name and its full path name cannot exceed 1022 characters. File reads and data/metadata change frequency \u00b6 The system is designed to support large file writes effectively. As a tape-based archive, however, it is not designed to support frequent read activity. File reads should be infrequent, and data and metadata changes should also be rare. Under normal operational use, no more than 10% of your files should be read, rewritten, renamed, or deleted during any 12-month period. If a special case arises \u2013 a recovery operation, for example \u2013 and you anticipate more activity, please contact the NCAR Research Computing help desk . Disaster recovery \u00b6 Disaster recovery storage is available to approved projects. When a disaster recovery account is approved, a secondary directory tree is made available for users\u2019 data The data are written to a separate pool of tapes from the primary data copies. The disaster recovery tapes are moved from the TS4500 library as they fill up and are stored in a fireproof vault in Cheyenne, Wyoming. See Quasar system specifications for details. Policies \u00b6 The system is not backed up. Vendor support for the system is 9 a.m. to 5 p.m. next business day, so problems that occur outside of those hours may need to wait to be resolved. CISL does not enforce file size at writing time, but when files smaller than the minimum size are found on the system, you may be asked to relocate the holdings to more appropriate storage such as the NCAR Campaign Storage file system or Stratus object storage system. If excessive read, rewrite, or metadata change activity is detected, you may be asked to relocate the holdings to more appropriate storage such as the NCAR Campaign Storage file system or Stratus object storage system.","title":"Quasar archive for data collections"},{"location":"storage-systems/quasar/#quasar-archive-for-data-collections","text":"The Quasar archive is a cold, tape-based archive for storing curated data collections that have an indefinite lifetime. It is not designed to serve data or to store data that will be frequently accessed, overwritten, or deleted. (Active data should be on GLADE or Campaign Storage rather than on Quasar.) Before requesting access, please review the following information regarding how to archive files, the minimum and maximum file sizes, and related use policies.","title":"Quasar archive for data collections"},{"location":"storage-systems/quasar/#storing-data","text":"Users store data on Quasar by transferring files via the Globus mapped collection named NCAR Quasar . For documentation about how to use Globus, see Globus file transfers . A note about verifying Globus transfers Using the Globus checksum sync option when transferring files can result in \"operation timed out\" error messages when it causes file recalls from tape, which can be slow. To avoid such errors when doing an incremental backup, use a different sync level \u2013 exists , mtime or size , for example \u2013 when making the transfer. To verify that a just-completed transfer did not encounter any corruption, do the checksum immediately to complete it before files are transferred to tape and purged from the disk cache.","title":"Storing data"},{"location":"storage-systems/quasar/#file-size-requirements","text":"The following requirements apply to files stored in a project's high-level allocation (RDA or EOL, for example). The maximum file size is 5 TB. The target file size for Quasar is 1 GB or larger, so use a tool like tar to combine multiple smaller files into a larger file or files before storing them. At least 90% of a project's files must be at least 100 MB. Up to 10% of a project's files may be smaller than 100 MB. The length of a file name and its full path name cannot exceed 1022 characters.","title":"File size requirements"},{"location":"storage-systems/quasar/#file-reads-and-datametadata-change-frequency","text":"The system is designed to support large file writes effectively. As a tape-based archive, however, it is not designed to support frequent read activity. File reads should be infrequent, and data and metadata changes should also be rare. Under normal operational use, no more than 10% of your files should be read, rewritten, renamed, or deleted during any 12-month period. If a special case arises \u2013 a recovery operation, for example \u2013 and you anticipate more activity, please contact the NCAR Research Computing help desk .","title":"File reads and data/metadata change frequency"},{"location":"storage-systems/quasar/#disaster-recovery","text":"Disaster recovery storage is available to approved projects. When a disaster recovery account is approved, a secondary directory tree is made available for users\u2019 data The data are written to a separate pool of tapes from the primary data copies. The disaster recovery tapes are moved from the TS4500 library as they fill up and are stored in a fireproof vault in Cheyenne, Wyoming. See Quasar system specifications for details.","title":"Disaster recovery"},{"location":"storage-systems/quasar/#policies","text":"The system is not backed up. Vendor support for the system is 9 a.m. to 5 p.m. next business day, so problems that occur outside of those hours may need to wait to be resolved. CISL does not enforce file size at writing time, but when files smaller than the minimum size are found on the system, you may be asked to relocate the holdings to more appropriate storage such as the NCAR Campaign Storage file system or Stratus object storage system. If excessive read, rewrite, or metadata change activity is detected, you may be asked to relocate the holdings to more appropriate storage such as the NCAR Campaign Storage file system or Stratus object storage system.","title":"Policies"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/","text":"Quasar system specifications \u00b6 The following information applies to the Quasar system hardware and disaster recovery storage maintained at the NCAR-Wyoming Supercomputing Center in Cheyenne, Wyoming. Quasar system \u00b6 Configuration IBM TS4500 robotic library with 2,198 slots and dual accessors; media verification enabled. 22 IBM TS1160 tape drives 2 PB IBM ESS disk cache 5 Dell PowerEdge R540 data movers 1620 JE cartridges, each with an uncompressed capacity of 20 TB IBM Spectrum Archive and IBM Spectrum Scale software User interface: Globus High-availability capabilities include: Dual accessors in library Spectrum Archive software can failover to any of the movers Disk cache uses declustered RAID to be able to handle multiple disk failures Disk cache comprises two servers, which are redundant Fireproof safe for disaster recovery tapes \u00b6 Door \u201cB\u201d rated heavy duty construction. 2-Hr. 350\u00b0F fire protection. Tested at temperatures up to 1700\u00b0F. U.L. Listed Residential Security Container burglary rating Body Overall thickness of 4-5/8,\" constructed with 2\" defense barrier of outer and inner steel plates. Heat-expandable intumescent door seal guards contents against severe fires. Heavy-duty steel hinges provide easy, smooth door operation. Locking mechanism Two 1-1/2\"-diameter solid steel chrome-plated locking bolts. Two 1-1/2\" diameter solid steel deadbolts lock deep into the body, preventing door removal during a forced entry attempt. Equipped with the AMSEC ESL10XL U.L. Listed Type 1 electronic lock. Lock protected by a tempered glass relock device. Additional information Weight: 318 lbs Interior dimensions: 19\"H x 12.5\"W x 12.2\"D Outside dimensions: 24.5\"H x 18\"W x 19.875\"D Cubic inches: 2,898 Shelves: 2 Steel door thickness: 2\" inches Body thickness: 2.875\" Inner wall steel thickness: 2.875\" Burglary protection (Safe Rating): RSC I Fill type: Concrete mix Fire protection: 120-minute Customizable: Yes Safe type: Commercial Security Safe | Composite Safe | Retail Safe | Back Office Safe USA-made or foreign-made: Foreign-made","title":"Quasar system specifications"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#quasar-system-specifications","text":"The following information applies to the Quasar system hardware and disaster recovery storage maintained at the NCAR-Wyoming Supercomputing Center in Cheyenne, Wyoming.","title":"Quasar system specifications"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#quasar-system","text":"","title":"Quasar system"},{"location":"storage-systems/quasar/quasar%2Bsystem%2Bspecifications/#fireproof-safe-for-disaster-recovery-tapes","text":"","title":"Fireproof safe for disaster recovery tapes"},{"location":"storage-systems/stratus/","text":"Stratus object storage system \u00b6 Stratus, the CISL object storage disk system described here, is for long-term data storage. Some documents attached below include the name of the vendor \u2013 Active Scale, a division of Western Digital \u2013 and some refer to the system with the name \"Data Commons S3.\" System overview \u00b6 Stratus does NOT have POSIX file system access. In fact, it differs from other file systems in many ways: There is no directory structure, only a flat hierarchy with a single level (bucket and content of the bucket). The data and metadata are accessed programmatically (rather than at the command line) with get/put commands, via an HTTP REST API. Data and metadata can be accessed either via a library (such as Python's boto3 ) or a web browser (either directly for the HTTP calls or via web interface). The system uses an API that is similar but not identical to the Amazon Web Services S3. Accounts are identified by a key pair: access key and secret key, as in these examples: Access key: AK0IYXKCCIA63BMNCOUN Secret key: Joeke2uHHebQdKJBgTVUzp+j7uRDthPdIBl5YaLE Accounts are associated with email, and each email address can have only a single account with a single role. A person who needs two roles must use two separate emails. Two roles exist: Admin \u2013 An admin can create buckets and users, set up read/write access control for users, and do everything a user can do; owns data created by users. Users \u2013 Users may access buckets and read or write data inside buckets if the admin granted access. Users cannot create buckets. Policies \u00b6 The system is not backed up. Support will be provided during business hours on business days. CISL will create only one admin account per lab. The admin will be able to create accounts for other users. Because the secret key-based logins do not expire, the admin will also delete accounts as appropriate \u2013 for example, when a user leaves NCAR. Requesting account \u00b6 Contact CISL to request an account. You will be asked to: - Specify how much disk space you need. - Give a brief description (one sentence) of your intended use case. - Acknowledge that you will be the admin and will manage buckets and users. Documentation and additional information \u00b6 This related page will help you get started as an object storage admin: Getting started with object storage admin account . Additional documentation is attached below. The system is accessible only via the NCAR VPN. This is important mostly for the browser-based access, since CISL anticipates that the server-based access will be from an internal server anyway. The access and secret credentials will be sent via email. They are all it takes a user to login (there is no UCAS, CIT, or Duo login). The NCAR username is irrelevant for this system. The way that these credentials are (unlike username/password) seems to nudge users towards nonoptimal patterns, such as hardcoding them into the source code. Users are strongly advised to NOT do that. Instead, use a separate file (outside of version control) similar to the following and source that file before running your code. This applies to both admin and user accounts. export AWS_ACCESS_KEY_ID = 'xxx' export AWS_SECRET_ACCESS_KEY = 'yyy' Admins might want to create a separate user account for themselves with just reading (and perhaps writing) capabilities and not admin capabilities. This would require use of a different email address, since the system does not allow reuse of existing emails. Admins might use a personal email, or a (group) alias setup in PeopleDB. Click to download \u00b6 Object_Storage_S3_API_Reference.pdf Object_Storage_View_User_Guide.pdf","title":"Stratus object storage system"},{"location":"storage-systems/stratus/#stratus-object-storage-system","text":"Stratus, the CISL object storage disk system described here, is for long-term data storage. Some documents attached below include the name of the vendor \u2013 Active Scale, a division of Western Digital \u2013 and some refer to the system with the name \"Data Commons S3.\"","title":"Stratus object storage system"},{"location":"storage-systems/stratus/#system-overview","text":"Stratus does NOT have POSIX file system access. In fact, it differs from other file systems in many ways: There is no directory structure, only a flat hierarchy with a single level (bucket and content of the bucket). The data and metadata are accessed programmatically (rather than at the command line) with get/put commands, via an HTTP REST API. Data and metadata can be accessed either via a library (such as Python's boto3 ) or a web browser (either directly for the HTTP calls or via web interface). The system uses an API that is similar but not identical to the Amazon Web Services S3. Accounts are identified by a key pair: access key and secret key, as in these examples: Access key: AK0IYXKCCIA63BMNCOUN Secret key: Joeke2uHHebQdKJBgTVUzp+j7uRDthPdIBl5YaLE Accounts are associated with email, and each email address can have only a single account with a single role. A person who needs two roles must use two separate emails. Two roles exist: Admin \u2013 An admin can create buckets and users, set up read/write access control for users, and do everything a user can do; owns data created by users. Users \u2013 Users may access buckets and read or write data inside buckets if the admin granted access. Users cannot create buckets.","title":"System overview"},{"location":"storage-systems/stratus/#policies","text":"The system is not backed up. Support will be provided during business hours on business days. CISL will create only one admin account per lab. The admin will be able to create accounts for other users. Because the secret key-based logins do not expire, the admin will also delete accounts as appropriate \u2013 for example, when a user leaves NCAR.","title":"Policies"},{"location":"storage-systems/stratus/#requesting-account","text":"Contact CISL to request an account. You will be asked to: - Specify how much disk space you need. - Give a brief description (one sentence) of your intended use case. - Acknowledge that you will be the admin and will manage buckets and users.","title":"Requesting account"},{"location":"storage-systems/stratus/#documentation-and-additional-information","text":"This related page will help you get started as an object storage admin: Getting started with object storage admin account . Additional documentation is attached below. The system is accessible only via the NCAR VPN. This is important mostly for the browser-based access, since CISL anticipates that the server-based access will be from an internal server anyway. The access and secret credentials will be sent via email. They are all it takes a user to login (there is no UCAS, CIT, or Duo login). The NCAR username is irrelevant for this system. The way that these credentials are (unlike username/password) seems to nudge users towards nonoptimal patterns, such as hardcoding them into the source code. Users are strongly advised to NOT do that. Instead, use a separate file (outside of version control) similar to the following and source that file before running your code. This applies to both admin and user accounts. export AWS_ACCESS_KEY_ID = 'xxx' export AWS_SECRET_ACCESS_KEY = 'yyy' Admins might want to create a separate user account for themselves with just reading (and perhaps writing) capabilities and not admin capabilities. This would require use of a different email address, since the system does not allow reuse of existing emails. Admins might use a personal email, or a (group) alias setup in PeopleDB.","title":"Documentation and additional information"},{"location":"storage-systems/stratus/#click-to-download","text":"Object_Storage_S3_API_Reference.pdf Object_Storage_View_User_Guide.pdf","title":"Click to download"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/","text":"Getting started with an object storage admin account \u00b6 This page describes and shows how to get started as an admin for the Stratus object storage system. Using the web GUI to log into your S3 admin account \u00b6 After connecting to the UCAR internal network or the VPN: Set your browser to point to this URL: https://stratus-admin.ucar.edu:10443/asview Enter your access ID and secret key Figure 1. Creating buckets \u00b6 Figure 2 shows the screen where you'll create buckets. To create a bucket, press the \"Create Bucket\" button and you'll be prompted for a bucket name. Note that the bucket name must be globally unique for the entire system. If a different account holder on the system already has a bucket with that name, you'll get an error. This behavior conforms with the AWS S3 API. (See also https://stackoverflow.com/a/59656742/25891 ) Figure 2. Figure 3 shows your new bucket and will show all your buckets as you create them. Figure 3. Now that a bucket has been created, you can write objects to the bucket and read from it using the access and secret keys that you used to log onto the web interface. There are many clients that can be used to take S3 actions, such as transferring objects to/from the system, listing buckets, etc. For example, the cyberduck desktop client can be utilized. Or python scripts (using the boto3 library) can be used to perform S3 operations. CISL has tried alternative access libraries bucketstore and apache-libcloud, but they lack the ability to select the URL where to connect, and therefore cannot be used with CISL's hardware (only with AWS). These other clients can also be used to create buckets and manage your account (not everything has to be done through this web interface). When connecting to the system with a client to perform S3 operations (e.g., transfer data, etc.), use the host name stratus.ucar.edu rather than stratus-admin.ucar.edu . The host name stratus-admin.ucar.edu should only be used to connect to the administrative web GUI. Granting permissions to other users \u00b6 By default, only the account owner has the privileges to write to and read from buckets in the account. It's possible to grant other users on the system and even anonymous users (those without any keys) write and read privileges. The account owner is responsible for informing additional users of CISL communications regarding this system, such as announcements of planned downtime, or ensuring that they subscribe to the \"Stratus Object Storage\" Notifier list. To grant other users access, you may need to first add the user. Press the \u201cUsers\u201d link on the left side of the screen and add the user. Once the user has been added click on a specific bucket name shown in the list of buckets in Figure 3. Figure 4 shows the options you will see for granting access. Selecting \u201cUser Permission\u201d will allow you to grant access to a specific user. Figure 4. Figure 5. In the section titled \"Add Other Account or User\" as shown in Figure 5, enter the email address of the person you would like to grant access to. Next, select the permissions you want to grant: Permission Meaning Read objects Allows the user to read objects from the bucket Write objects Allows the user to write objects to the bucket Read bucket permissions Allows the user to read the permissions on the bucket and, for example, see what other users have permissions on the bucket Write bucket permissions Allows the user to change the permissions on a bucket. For example, a user could grant another user permissions on the bucket Once granted, the specific user will have the permissions that you've configured for that bucket. Since each bucket has its own permissions, you'll need to set permissions for each bucket if you want other users to be able to access it. It's also possible to grant permissions to what are termed public users. A public user is defined as either an anonymous user or an authenticated user. An anonymous user means a user who does not need any keys to access the bucket; anonymous users can only read objects from buckets. An authenticated user means any user on the system. To grant permissions to public users, click on \"Public Permissions\" in the screen that was visited earlier and shown in Figure 6. Figure 6. Figure 7. Select the permissions you want to grant, as shown in Figure 7, to enable access for authenticated users or anonymous users. Note that the Data Commons System is currently only reachable from devices on the internal UCAR network. Devices outside of the UCAR network are not able to access the storage system, even for anonymous access. Vendor documentation \u00b6 Here is a link to vendor documentation that may help you: S3 API Reference Glossary \u00b6 AWS SDK - Amazon Web Services Software Development Kit for Python (Boto3) ( http://aws.amazon.com/sdk-for-python/ ) Bucket - Container to store objects, similar to a directory Cyberduck - Cloud storage browser ( http://cyberduck.io ) Object - data structure that stores both file metadata and data","title":"Getting started with an object storage admin account"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#getting-started-with-an-object-storage-admin-account","text":"This page describes and shows how to get started as an admin for the Stratus object storage system.","title":"Getting started with an object storage admin account"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#using-the-web-gui-to-log-into-your-s3-admin-account","text":"After connecting to the UCAR internal network or the VPN: Set your browser to point to this URL: https://stratus-admin.ucar.edu:10443/asview Enter your access ID and secret key Figure 1.","title":"Using the web GUI to log into your S3 admin account"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#creating-buckets","text":"Figure 2 shows the screen where you'll create buckets. To create a bucket, press the \"Create Bucket\" button and you'll be prompted for a bucket name. Note that the bucket name must be globally unique for the entire system. If a different account holder on the system already has a bucket with that name, you'll get an error. This behavior conforms with the AWS S3 API. (See also https://stackoverflow.com/a/59656742/25891 ) Figure 2. Figure 3 shows your new bucket and will show all your buckets as you create them. Figure 3. Now that a bucket has been created, you can write objects to the bucket and read from it using the access and secret keys that you used to log onto the web interface. There are many clients that can be used to take S3 actions, such as transferring objects to/from the system, listing buckets, etc. For example, the cyberduck desktop client can be utilized. Or python scripts (using the boto3 library) can be used to perform S3 operations. CISL has tried alternative access libraries bucketstore and apache-libcloud, but they lack the ability to select the URL where to connect, and therefore cannot be used with CISL's hardware (only with AWS). These other clients can also be used to create buckets and manage your account (not everything has to be done through this web interface). When connecting to the system with a client to perform S3 operations (e.g., transfer data, etc.), use the host name stratus.ucar.edu rather than stratus-admin.ucar.edu . The host name stratus-admin.ucar.edu should only be used to connect to the administrative web GUI.","title":"Creating buckets"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#granting-permissions-to-other-users","text":"By default, only the account owner has the privileges to write to and read from buckets in the account. It's possible to grant other users on the system and even anonymous users (those without any keys) write and read privileges. The account owner is responsible for informing additional users of CISL communications regarding this system, such as announcements of planned downtime, or ensuring that they subscribe to the \"Stratus Object Storage\" Notifier list. To grant other users access, you may need to first add the user. Press the \u201cUsers\u201d link on the left side of the screen and add the user. Once the user has been added click on a specific bucket name shown in the list of buckets in Figure 3. Figure 4 shows the options you will see for granting access. Selecting \u201cUser Permission\u201d will allow you to grant access to a specific user. Figure 4. Figure 5. In the section titled \"Add Other Account or User\" as shown in Figure 5, enter the email address of the person you would like to grant access to. Next, select the permissions you want to grant: Permission Meaning Read objects Allows the user to read objects from the bucket Write objects Allows the user to write objects to the bucket Read bucket permissions Allows the user to read the permissions on the bucket and, for example, see what other users have permissions on the bucket Write bucket permissions Allows the user to change the permissions on a bucket. For example, a user could grant another user permissions on the bucket Once granted, the specific user will have the permissions that you've configured for that bucket. Since each bucket has its own permissions, you'll need to set permissions for each bucket if you want other users to be able to access it. It's also possible to grant permissions to what are termed public users. A public user is defined as either an anonymous user or an authenticated user. An anonymous user means a user who does not need any keys to access the bucket; anonymous users can only read objects from buckets. An authenticated user means any user on the system. To grant permissions to public users, click on \"Public Permissions\" in the screen that was visited earlier and shown in Figure 6. Figure 6. Figure 7. Select the permissions you want to grant, as shown in Figure 7, to enable access for authenticated users or anonymous users. Note that the Data Commons System is currently only reachable from devices on the internal UCAR network. Devices outside of the UCAR network are not able to access the storage system, even for anonymous access.","title":"Granting permissions to other users"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#vendor-documentation","text":"Here is a link to vendor documentation that may help you: S3 API Reference","title":"Vendor documentation"},{"location":"storage-systems/stratus/getting-started-with-an-object-storage-admin-account/#glossary","text":"AWS SDK - Amazon Web Services Software Development Kit for Python (Boto3) ( http://aws.amazon.com/sdk-for-python/ ) Bucket - Container to store objects, similar to a directory Cyberduck - Cloud storage browser ( http://cyberduck.io ) Object - data structure that stores both file metadata and data","title":"Glossary"},{"location":"tutorials/","text":"","title":"Index"},{"location":"tutorials/advanced-pbs/","text":"","title":"Advanced PBS"},{"location":"tutorials/containers/","text":"","title":"Containers"},{"location":"tutorials/dask-tutorial/","text":"","title":"Dask Tutorial"},{"location":"tutorials/gpu-workshop/","text":"","title":"GPU Workshop"},{"location":"tutorials/jupyterhub/","text":"","title":"JupyterHub"},{"location":"tutorials/new-user-training/","text":"","title":"New User Training"},{"location":"tutorials/profiling-and-debugging/","text":"","title":"Profiling and Debugging"},{"location":"user-support/","text":"Consulting \u00b6 Consultant on Duty \u00b6 Virtual Consulting \u00b6 Documentation \u00b6 Search for relevant documentation with the \"Search\" field at upper-right or use one of the service desk links below to submit a help request (login required). Where a login is required, use your NCAR/UCAR username and CIT password. Users who do not have CIT passwords (non-NCAR/UCAR staff members, for example) can reset their passwords here or call 303-497-2400 for assistance. NCAR Help Desk Resources \u00b6 Research Computing help desk \u00b6 Contact the help desk for assistance with Cheyenne, Casper, and related data-storage systems. Enterprise Service Desk staff support \u00b6 The Enterprise Service Desk portal is for NCAR/UCAR staff who need support for internal IT services. CISL staff ask that you provide the information listed below when you report the failure of a Cheyenne or Casper job. This information will help us find a solution as quickly as possible. Best Practices for Support Tickets \u00b6 When submitting a support ticket please include as much detail as possible to enable quicker resolution. It is also important to preserve any input files, binaries and executables in the directories as they were when the problem occurred. Resource name (Derecho, Casper, JupyterHub,...), Exact error messages and/or paths to error output, Batch script location, PBS JobID(s) of failed effort, Run & source directory paths (ideally UNIX-readable by \u2018others\u2019), Any other pertinent information: Last time this exact workflow was successful, if any (or changes since last success), Troubleshooting steps already attempted, etc. And please remember to let us know when your issue is resolved! https://rchelp.ucar.edu Getting Connected \u00b6 NCAR HPC Users' Group \u00b6 The NCAR HPC Users\u2019 Group, NHUG, is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community. NHUG is open to all HPC users and holds monthly meetings featuring different HPC-related topics. See the NHUG page for additional details. Daily Bulletin \u00b6 If you are not already receiving the Daily Bulletin, use this link to subscribe: https://ncar.pub/CISL-Daily-Bulletin-subscribe New users of our HPC systems are subscribed automatically. Sundog \u00b6 The CISL on Sundog intranet space is open to all NCAR and UCAR staff, who log in using a CIT password. What to do: Use this link to go to the CISL Sundog space . Log in if you haven't already done so. Click the Submit button under \"Joining Instructions.\" (Required only once.)","title":"Index"},{"location":"user-support/#consulting","text":"","title":"Consulting"},{"location":"user-support/#consultant-on-duty","text":"","title":"Consultant on Duty"},{"location":"user-support/#virtual-consulting","text":"","title":"Virtual Consulting"},{"location":"user-support/#documentation","text":"Search for relevant documentation with the \"Search\" field at upper-right or use one of the service desk links below to submit a help request (login required). Where a login is required, use your NCAR/UCAR username and CIT password. Users who do not have CIT passwords (non-NCAR/UCAR staff members, for example) can reset their passwords here or call 303-497-2400 for assistance.","title":"Documentation"},{"location":"user-support/#ncar-help-desk-resources","text":"","title":"NCAR Help Desk Resources"},{"location":"user-support/#research-computing-help-desk","text":"Contact the help desk for assistance with Cheyenne, Casper, and related data-storage systems.","title":"Research Computing help desk"},{"location":"user-support/#enterprise-service-desk-staff-support","text":"The Enterprise Service Desk portal is for NCAR/UCAR staff who need support for internal IT services. CISL staff ask that you provide the information listed below when you report the failure of a Cheyenne or Casper job. This information will help us find a solution as quickly as possible.","title":"Enterprise Service Desk staff support"},{"location":"user-support/#best-practices-for-support-tickets","text":"When submitting a support ticket please include as much detail as possible to enable quicker resolution. It is also important to preserve any input files, binaries and executables in the directories as they were when the problem occurred. Resource name (Derecho, Casper, JupyterHub,...), Exact error messages and/or paths to error output, Batch script location, PBS JobID(s) of failed effort, Run & source directory paths (ideally UNIX-readable by \u2018others\u2019), Any other pertinent information: Last time this exact workflow was successful, if any (or changes since last success), Troubleshooting steps already attempted, etc. And please remember to let us know when your issue is resolved! https://rchelp.ucar.edu","title":"Best Practices for Support Tickets"},{"location":"user-support/#getting-connected","text":"","title":"Getting Connected"},{"location":"user-support/#ncar-hpc-users-group","text":"The NCAR HPC Users\u2019 Group, NHUG, is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community. NHUG is open to all HPC users and holds monthly meetings featuring different HPC-related topics. See the NHUG page for additional details.","title":"NCAR HPC Users' Group"},{"location":"user-support/#daily-bulletin","text":"If you are not already receiving the Daily Bulletin, use this link to subscribe: https://ncar.pub/CISL-Daily-Bulletin-subscribe New users of our HPC systems are subscribed automatically.","title":"Daily Bulletin"},{"location":"user-support/#sundog","text":"The CISL on Sundog intranet space is open to all NCAR and UCAR staff, who log in using a CIT password. What to do: Use this link to go to the CISL Sundog space . Log in if you haven't already done so. Click the Submit button under \"Joining Instructions.\" (Required only once.)","title":"Sundog"},{"location":"user-support/how-to-join-cisl-on-sundog/","text":"The CISL on Sundog intranet space is open to all NCAR and UCAR staff, who log in using a CIT password. What to do: Use this link to go to the CISLspace . Log in if you haven't already done so. Click the Submit button under \"Joining Instructions.\" (Required only once.)","title":"How to join cisl on sundog"},{"location":"user-support/how-to-subscribe-to-cisl-daily-bulletin/","text":"If you are not already receiving the Daily Bulletin, use this link to subscribe: https://ncar.pub/CISL-Daily-Bulletin-subscribe New users of our HPC systems are subscribed automatically.","title":"How to subscribe to cisl daily bulletin"},{"location":"user-support/troubleshooting-tips/batch-job-crashing/","text":"","title":"Batch jobs crashing"},{"location":"user-support/troubleshooting-tips/common-causes-of-job-failures/","text":"","title":"Common causes of job failures"},{"location":"user-support/troubleshooting-tips/killed-processes-on-login-nodes/","text":"","title":"Killed processes on login nodes"},{"location":"user-support/troubleshooting-tips/peak_memusage-output-fails/","text":"","title":"Peak memusage output fails"}]}